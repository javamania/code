 Music hello and welcome to our very firstspotlight i hope you enjoyed the openingkeynote for the next hour we will bediving into all things data i mstephanie wong a developer advocate andi m here with garrett kasmeier who leadsdatabase analytics and bi solutions atgoogle cloudthanks stephanie i m very excited to behere and to share with everyone what wehave been working onthe team thought we start on the lightside of things here and start with ajoke one of my favorite jokes actuallysohere it goes a secret query walks into abarand approaches two tables and askscan i join you Music okay i guess we wrote a video insteadinnovation is a huge challenge inbaseball the current mlb fan expects usto be relevant customers call uspatients call us members call us and youcan imagine a team that s just buriedunder the weight of thousands of callrecordingswe look at so much data everything youcan imagine we analyze frankly we spend70 percent of our time just trying to tocapture it and trying to sift through itwe re expecting totransfer around 300 to 400 petabytes tothe cloudit s hard to even describe how big thatisgoogle cloud was very impressive to usit allowed us flexibility in storage andcompute scaling google cloud provides uswith technology that will drive us to bebetternow our team can look at hundreds ofcalls a day in minutes instead of weekswith google cloud we re going to be ableto spend our time analyzing real timeand using the data that we ve beencollectingwe thought google cloud was the bestpartner for us i think this is very muchgoing to be an open and evolvinglong term relationship between googleand twitter it s a perfect fit withgoogle cloud as we go forward google smade it possible for us to reachexceptional customer experience in a waythat hasn t been done before Music and welcome back so what are we solvingfor today this spotlight session isabout how to leverage data as your moststrategic business asset especially asyou adapt to and anticipate changedata has the power to transform stagnantbusinesses into profitable onesstreamline operations to improvepeople s lives and identify areas ofwaste to radically reduce unnecessaryspendingit can be used to solve businesses andworld problems both the simple ones andthe ones that keep people up at nightit has the potential to accelerateprogress faster than a think tank orworld summitand it s largely untappedas you learn more about google cloudcapabilities and make progress on yourdata cloud journey you can unlock yourown 10x mindset as external factorsaccelerate the pace of changenever before have we faced this muchuncertainty and now is the time whenevery company can turn data into anabilitythe reality today is that too manycompanies struggle to remove thebarriers to turn data into intelligencethis is exactly what we are seeing withour customers turning data intointelligence and delivering value is nolonger a nice to have it is essential arecent study shows that only 32 percentof the companies surveyed gain valuefrom their data investments and this isthe result of difficult to access slowunreliable complex and fragmentedsystemsmany companies just feel overwhelmedwith the demand for the various types ofworking with data and of doing it atscale they feel walled in with a lack ofintegration a lack of openness a lack ofportability and very poor apisit is time to reimagine our data cloudstrategy that is unlocking your dataadvantagebuild great applications that understandand interact with your customer onoperational databases that scale tobillions of queries per secondreact on any event in real time andprocess millions of data streamsanalyze petabytes of data without sizingor tuning your data warehouse anymoreand ultimately optimize your processeswith hundreds of machine learning modelsin productionthis is how you unlock your data cloudand unlock your data advantageto build your data cloud you need yoursystems to be simpleand this requires a partner that meetsyou where you are today and helps you tomature where you want to bebut at your own pacemaking the complex simple is ourbusinessit has been for decades building systemsthat mask the complexity of the entireinternet behind a simple singleinterface and building applications withbillions of usersit all runs on data and analytics so youcan say the data is our dnaand we can t predict the future but wecan plan for it and we are deeplygrateful to work with tens of thousandsof organizations who build their dataclouds on google cloudand unlock ultimately their dataadvantage from databases to analytics tostreams to lakes to aiand build a unified data cloud togetherwith usit s leaders like you who areaccelerating their use of data totransform complexity into simplicitythis week at next you ll hear fromleading organizations that have chosengoogle cloud for their data cloudstrategy over market alternativesverizon media is one of these customersverizon uses bigquery and looker toscale their massive enterprise mediawarehouse supporting monthly scans ofmore than one exabyte of datathe performance improvements include90 percent plus productivityimprovements across the board and loweroverall tco compared to other cloudprovidersalmost overnight traffic to chess comtripled the pandemic and successfulseries the queen s gambit created aperfect stormten years worth of growth occurred injust six months and daily active usersrose from 280 000 to more than 1 millionthere s no way to handle that searchwithout the move to google cloud usingbigquery s near infinite ability toscale in response to demand this meantthat chess com didn t have to forecastsite traffic and worry about unexpectedsurges or waste money by overprovisioning servers that go unused takeautotrader it s uk s largest onlineautomotive marketplace and they wereable to migrate more than 60 of theircurrent oracle footprint to cloud sqlautotrader builds their data cloud byleveraging cloud sql bigquery and lookerto deliver access to their data to allof their users and with cloud sql as afully managed service auto greaterrelease cadence has improved by over 130percent year over year enabling animpressive peak of more than450 releases to productionon a single day and this is how youbuild your data advantagewhen cardinal health migrated to googlecloud they enabled disaster recoveryincreased their ability to respond tobusiness challenges and leverage machinelearning to drive business efficienciesall while reducing their cost by 25we would like to welcome ray bajaj ctoof cardinal health to share their datatransformation storyray thank you so much for joining ushere todaythanks stephanie and i m really happy tobe here today cardinal health is apioneer in healthcare industry forleveraging the cloud as a primary sourceof infrastructureachieving our cloud transformationjourney goalwith more than 80 percent of ourworkload fully migrated onto the cloudour journey to google cloud started in2018 leveraging the security andflexibility of google cloudallows us to scale and innovate at lowercostwhich empowers our organization to focuson building the future of healthcareand driving better outcomes across thehealthcare ecosystem sowhy google cloudwith google we can deliver modern datainsights capabilitiesto our engineering teams so that theycan transform our core businesswhile also improving the customerexperiencehelping the organization evolvein the adjacent areas of healthcarewe also selected google cloud because ofits ability to partner with sapand run massive erp workloads reliablynow what outcomes have you been able torealize as a result of thiswe leveraged google s local and bigqueryto develop episode analytics a datadriven predictive cost tracking toolthat enables community oncologypractices to accurately measurethe cost of care and provides clearerinsightsinto value based performancewell ray cardinal is clearly at theforefront of these shifts and we can twait to see what s in store for you sothank you so much for being here with ustodaythank youorganizations like cardinal health aresolving complex challenges with googlecloudbutwhat is holding others back from usingdata to accelerate innovationcompanies are looking for solutions thatcan simplify their data landscape tomove from a state where every scenariorequires a new tool to an agile platformthat combines data analytics databasesdata engineering and science securityand governance into a single platform tomove from data at rest to a platformwhere data is accessible in real timeour vision is to provide a platform thatallows data to unfold its full potentialultimately enabling us to reach newhorizons ranging from newly builtdigital experiences products andbusiness models to advanced machinelearning and ai to ultimately empoweringhumans to reason beyond their ownbiological capacityand we have built the industry s mostsimple intelligent and best integratedcloud data platform that enablesbusinesses to fuel their own digitaltransformationand this is how you unlock your dataadvantageto seize new opportunities improveoperational efficiencies and deliver newuser experiences that delight customerspartners and your employeesthanks to you our customers and partnersgoogle s data platform is one of thefastest growing clouds fueled by thedemand for products like big query andhere comes the most exciting part todaywe are excited to announce a number ofnew product innovationsand the first announcement goes out toall of the data engineers in the worldwho are using spark on google cloud openstandards are at the heart of our datacloud strategy and at google we followan open approach that enablesflexibility and choice and today i amexcited to announce project big spark itis the world s first auto scalingserverless spark service in existenceand regardless of whether you are a bigquery user a data proc user our cloud aiuser we have created an integratedecosystem around spark the de factostandard for data engineering workloadswe want to make it easy for you toonboard on our platform with spark andnever limit your ambition with datascience and data engineeringour long history in the open sourcecommunity has shown us that betting onthe community first is always the rightstrategy to give our customersportability and optionality we reexcited to make spark a first classcitizen on our platform and to automatethe productionalization of your dataengineering and data science workloadsthe next announcement goes to all of thedata scientistsas enterprises consolidate their dataplatforms the distinction betweenadvanced analytics and data science andmachine learning is really diminishingtoday more than ever data scientists arelooking for a unified interface to beable to seamlessly connect their entiredata estate and also run allexplorations with sql spark and othermachine learning libraries with vertexai we have simplified the experience fordata scientists with 80 percent lesscode required to drain a model comparedto any other platformas a part of google s vertex ai strategyi am happy to announce the brief view ofai notebook for vertex ci that offers afully managed scalable and enterprisegreat ide combined with easilyenforceable policies and user managementthese ai notebook innovations are greatexamples of google bringing together thepower of data and aigoogle truly aims to fast trackinnovation by eliminating the need fordata scientists to shift context betweenaspects like data analytics data scienceand machine learningthat is right on and here is an excitingannouncement to all of the developerswe are adding a postgres interfacefor spanner which continues ourcommitment to openness and reinforcesour efforts to democratize the access tospanner with this new postgres interfaceenterprises can take advantage ofspanner s unmatched global scalefive nines of availability and strongconsistency using skills and tools fromthe popular postgres ecosystem andstarting today you can create a spannerdatabase that uses postgres sql dialectand connect with its open wire protocolthis interface supports banner s richfeature set using the most popularpostgres data types and sql features andthus democratizing access to spanner formillions of developers out there ourcommitment to the ecosystem has beenlong standing with ongoing support forcloud sql for mysql postgres and sqlserver so you can lift and shift yourdatabasesour mission with cloud sql iscompatibility and we were the firstcloud provider to support postgres 13 these announcements are so excitingbecause postgres has emerged as apopular choice for operational databasespostgres is known and love for itsstability and feature set and has builtan active and growing ecosystem ofdevelopers and toolsour investments in postgres are a stepin the right direction to continuemeeting developers where they are withthe tools they love we are excited toannounce new ways to help organizationsgoing beyond traditional businessintelligence to analyze access and acton data looker continues to integratewith the other offerings in ourportfolio as well as continues toinnovate with our core product and toaddress the needs we are continuing tosimplify the experience across bigquerylooker and data studio together withconnected sheets today we are announcingnew enhancements which bring the realityof augmented analytics to more peoplewith looker s solution for contactcenter ai to help businesses leverage aiand converse naturally with customersand deliver outstanding experienceswe are also announcing a new lookerblock for healthcare nlp ai whichcreates a user interface to advanced aiand natural language functionality andthis helps healthcare providers to gaindeeper understandings of their patientdata part of the secret sauce lies inlookers in database architecture and thesemantic layer which makes it possiblefor business users to trust data nomatter how they experience it via adashboard email or text alert or even acustom applicationwe are excited for these recentinnovations but we couldn t do thisalone together with our community ofcustomers partners and developers we aredelivering innovation across a varietyof use cases we are able to create newsolutions and accelerate your journey totransform data into valuenext i want to announce an importantpartnership with informatica we havepartnered with informatica to helphundreds of customers succeed with theirjourney to the cloudtogether we are helping customers movemission critical workloads to the cloudand integrate with google cloudtechnologies such as bigquerytoday we are excited to announce that weare deepening our partnership withinformatica even more first for existinginformatica power center customerswe are so pleased to announce ourmigration factory which will help toaccelerate migration of data drivenworkloads to google cloud withinformatica cloudsecondwe are pleased to announce our newservices on google cloud s marketplacewhich will include informatica sintegration and enterprise data catalogofferingsand we are continuing to invest in keyproduct integrations with data bricksour engineering and product teamscontinue to work together to deliver thejoint roadmap and we have multiplecustomers using our latest enterprisesecurity and performance enhancements ofdatabricks on gke in q4 we will alsodeliver data brick sql databricks photonon google cloud as well as we arelaunching new data center regionstogether in emea and apecour ai teams are engaged to allowcustomers to experiment and developmodels on data bricks and vertex aitogether and deploy models at scale intoproduction with vertex ml ops suite somuch goodness okay before we get intothe live demo i recommend tuning in tomore sessions to hear from all ourcustomers this week join the chief dataofficer panel discussion and listen toour product strategy sessions fordatabase data analytics bi and aiwe can t wait to hear from you connectwith you join us in our community andshare your expertise and ask youquestions but most importantly start tobuild your data cloud and unlock yourdata advantageokaytime now to get hands on and check outour products in real actionjoin us for a live demo and interactwith our experts and one another andthen stick around for the live q afollowing the demoi see you thereAMINA MANSOUR  Hello  everyone My name is Amina Mansour and I m a Solutions Architectat Google Cloud Scott Surovich  who is theGlobal Container EngineeringLead at HSBC  will bejoining me later for a Q A With the growinginterest and adoptionof hybrid and multicloudarchitectures I find myself workingwith a lot of customersthat are looking for guidanceon architecting their platformsand choosing wheretheir applicationsand services reside Today we will briefly reviewthe difference between hybridand multicloud then I will presentsome of the more common patternswe ve seen with our customers And for the interestingpart of the talk we will have a Q A withScott to share his experienceand insights withhybrid and multicloud So what is the differencebetween hybrid and multicloud If we take hybridfirst  this is whereyou have workloadsthat are deployedacross multipleenvironments  onebeing a privateon prem data centerand one in the public Cloud Multicloud  on the otherhand  combines at least twopublic Cloud environments but you can alsohave a private computingenvironment as partof your multicloud setup So really  multicloud is asuperset of hybrid Cloud We can put these patternsinto two buckets The first isdistributed deployment where the aim is to run theapplication in the environmentthat suits it best So each application runsin a specific environment The second isredundant deployment where you deploythe same applicationin multiple environments Let s now look at thedifferent patterns The first patternwe will talk aboutis the tiered orlayered pattern The idea of it is tofocus first on deployingfront end applicationsto the public Cloud In this pattern  you keep yourback end applications and datain the privatecomputing environment Some advantages arethat this allowsyou to start with theless complex migrations You also benefit fromwhat Google Cloud knowswhat to do well  so things likeload balancing and autoscaling And you keep your dataand back end on premfor regulatory reasons Because you are communicatingacross environments choose the Google Cloud Regionsand Interconnect locationclosest to your data center Make sure to secure thecommunication with VPN tunnels TLS  or both  andestablish a common identitybetween environmentsto authenticate across The second pattern isthe partition pattern It combines multiplepublic cloud environmentsthat gives you theflexibility to deployan application in the bestcomputing environment for it So application A is on Cloud 1and application B is on Cloud 2based on its dependenciesand Cloud services it uses Because of your presencein multiple public Cloudenvironments  you relowering your riskand you have the flexibility tochange plans or partnerships You can also choosewhich providerto run an application or serviceon depending on its needs Each additional environmentcomes with additional overhead so weigh the advantagesagainst the overhead You should also minimizedependencies across Cloudsso it doesn taffect performance And focus less on portabilityof your applicationsand more on the portabilityof your workflowsand having a unifiedplatform across providers Our third pattern is edge Running workloads inthe Cloud requiresthat clients have fast andreliable internet connectivity There are scenarios when youcannot rely on continuousconnectivity  such as stores orsupermarkets that might be onlyconnected occasionally or uselinks that are not reliableenough for businesscritical transactions With this pattern  you run time and business critical workloadslocally at theedge of the networkwhile using the Cloud forall other kinds of workloads In an edge setup the internet linkis a non criticalcomponent that isused for management purposesand to synchronize your uploaddata This helps you ensure lowlatency and self sufficiencyin cases when you cannot do thatdue to network restrictions You also reuse existinginvestments and equipmentat those edge locations And remember that increasedtraffic to Google Cloudis free  so that makesit easier to communicateback to the Cloud withstatus or data syncs Two things to keep inmind with this pattern  you don t want to increasethe burden of managementof these edge locationslinearly with how many they are so you should have a centralizedcontrol plane in the Cloudto manage them all Also  the less dependencieson the Cloud environment the more reliable andfaster your edge setup is Our fourth pattern is analytics In this pattern transactional workloadsstay on prem andanalytics workloadsare in the public Cloud toleverage best of read services Of course  the first thing thatcomes to mind with this patternis BigQuery  which is a verycommon reason why our customerschoose this pattern Analytics workloads often needto process huge amounts of dataand can be bursty  sothey lend themselveswell to the publicCloud and you don tneed to overprovisionon prem anymore Google Cloud alsoprovides a rich setof services to managedata throughout its entirelifecycle  ranging frominitial acquisition to processing and analyzing to final visualization Also you get to take advantageof Cloud capabilities  again  like I mentioned increased traffic and movingdata from the private computingenvironment to Google Cloudis free of charge And when you have existingHadoop or Spark workloads consider migratingjobs to data PROCand migrating existingHDFS data to Cloud Storage Use queues to hand over data So things like Pub Sub or maybeeven Cloud Storage bucketsto hand over thedata to Google Cloud Choose the transferapproach that is best suitedfor your data set sizeand available bandwidthfor the initial datatransfer  and youshould use consistenttooling and processesacross environmentsto help increaseyour operational efficiency This one is very common fora majority of the patterns Last but not least isthe bursting pattern and it s the onlypattern discussed todaythat fits in the redundantdeployment pattern bucket The idea of the Cloudbursting patternis to use a privatecomputing environmentfor the baseline loadand burst to the Cloudtemporarily when youneed that extra capacity While you can accommodate burstyworkloads in a classic datacenter basedcomputing environment by overprovisioningyour resources this approach is notreally cost effective So this patternallows you to apply itto interactive andbatch workloads But when you re dealingwith interactive workloads you must determine howto distribute requestsacross environments These three advantagesare all related It allows you toreuse  or in this case use your investments and there is no longera need to overprovision yourcompute resources on prem So that enables increasedutilization and costeffectiveness in the privatecomputing environment As for bestpractices  again  youshould choose the regionsthat are close together That applies to the Cloud regionand the Interconnect location You should make Cloud resourcesprivate for batch workloads Those are not public facing and so in this case you should secure andhave them be private And you should securethe communicationbecause the data that isexchanged between environmentsmight be sensitive And finally  establishcommon identityso that you cansecurely authenticateacross environment boundaries Now it s time for themore interesting partof our session Scott is the GlobalContainer EngineeringLead at HSBC and the GlobalAnthos Product Owner He s also a GoogleCloud CertifiedFellow and Co Chair of theCNCF Financial Services WorkingGroup Great to have you withus here today  Scott SCOTT SUROVICH  Hi  Amina Thanks for having me AMINA MANSOUR  Before westart with our questions why don t you tell us a littlebit about the scale of HSBC SCOTT SUROVICH  Sure So before we getinto the patterns I ll explain about who HSBC is We are one of the world slargest global banks and the sheer scale of whatwe deal with on a daily basisis presented in this slide As you can see  we rein 66 countries We process about  1 5trillion a day in payments We have 110 000 servers roughly  globallyright now   so that san out of date portion We have data centers in21 different countries  that does not count where wehave large server rooms  aswell We have a large amount ofemployees  around 235 000 We ve got 39 millioncustomers globally We process about revenueof  53 8 billion a year and we have offices all overthe world   roughly about3 900 offices So it s a large organization And even in the IT section  wehave 40 000 IT professionalsworking for us AMINA MANSOUR  Thank youfor that intro  Scott Let me start by askingyou why you would considera hybrid or multicloud solutionin a highly regulated industry SCOTT SUROVICH Definitely  great idea The first thingI ll talk about isgoing to be the hybrid Cloud So in the hybrid Cloud as we have on the slide we have data centers globally And again  we have serverrooms globally  as well So we have a large investmentin data centers already So we want to utilizethat where we can But of course going to the Cloudmakes sense inmultiple scenarios So to use these data centers we have a hybrid Cloud solution It helps us with a backup plan So if we go to a Cloudwith an application we have to have a backupplan for the regulators So we have to showthat if somethingwere to go wrong in thelinks  the relationship  justwith the vendor in general where do we go from there And on prem is aviable solution It also could go toanother CSV  but since wehave those data centers  wecan leverage that internally Finally  it takes time to getapproval to go to the Cloudfor an application It can take three to six months Now  in that time  youdon t want to sit idle You might want torefactor your app You might want to testout some new services So with the introductionof containers  Kubernetes andAnthos   for on prem if a workload fits intothat  my developerscan actually startdeveloping that todayand they can movethat to the Cloudonce they have theapproval to do so In the multicloud side  weselect the best of breedfor features for each CSP As you said  BigQuery is abig one for the Google side and it s just one ofmany different featureson the Google side There s also regionalavailability Certain times Ihave an applicationthat has to go intomultiple regions But somebody like Google maynot have a presence in China or you may not havea point of presencesomewhere close to acertain other region So we have to go multicloudwhere it makes senseto go multicloud AMINA MANSOUR  Those areall great reasons  Scott Of course  werecognize there aremany other reasons for otherindustries and verticals But of those patterns that Idescribed  which patterns areyou most likely to implement SCOTT SUROVICH  Sure All the models are great All the patterns are great So the first one thatI like to talk aboutis going to be the tieredmodel  or the tiered pattern So in my previous days  wewould have our websites on site And we would be under attack  get denial of service attacks We d have to engagemultiple teams  operating systems  networking obviously our serviceproviders  and we wouldspend hours and hours tryingto remediate that situation We want to restore theservices to our customersand our employees So rather than have us dealwith that   the whole time we refighting that fire we are not innovatingand we re not goingforward for the business So let s letsomebody like Google who was born in the Cloud raised in the Cloud knows the Cloud  andcan protect the Cloud  we will put ourpresentation layer in GCPand products like theWeb Application Firewall available on CloudArmor  would helpus to remediate thatattack a lot fasterand not drain our resources So put the front endin the CSP  but keepingyour data safe  and evenbetter  regulatory complianceto have your data on prem The other model wouldbe the partition model Now  we ve already mentionedthat HSBC has multiple datacenters and multipleserver roomsglobally  so we have a heavyinvestment in machines AMINA MANSOUR  But whatabout the bursting pattern From your experience  whatare some use cases for that SCOTT SUROVICH  Yeah the bursting patternis a great use case for Cloud However  it s also one of themore difficult ones to utilize And I say thatbecause the challengeis the regulatory issues thatwe deal with in this industry So because of that  I can tjust select any workloadand say  OK  I m going to burstwhen I need to in the Cloud It has to be an applicationthat is approved the data has to be approved So we have to be careful So with the right choice it s the perfect scenariofor the challengesthat it addresses And 2020 was one ofthose challenges So  of course  during2020  a lot of peoplestarted working from home Basically everybodystarted working from home If you had to expand yourdata center for certain usecases because of thisdifferent working pattern you might have toexpand out and scale outyour server infrastructure That was a bigchallenge in 2020 Trying to orderequipment was delayedfor months due tomanufacturing  due to shipping or due to silicone shortages So we can t waitmonths for that So to address this  firstthing is the perfect scenario It s a little bit longerthan normal temporary kind of like whatyou were mentioningduring your introduction but at some pointwe do hope that all ofthis is going to die down everything s goingto be good  and thenwe all get back tothe office   we don tneed all that extra capacity So by scaling up andbursting into the Cloud we do it faster we do it cheaper and we don t have any ofthat hardware left overafter we re done with allof the nightmare of 2020 AMINA MANSOUR  100  You hit the nail on thehead with this example But let s pivot tosomething happier What s a fun use casefor the edge pattern SCOTT SUROVICH  Yeah the edge patternis one I ve always liked So in the banking industry a lot of processes a lot of hardware started to look at that So ATMs   we re notquite there yet but that s a greatuse case for an edge From an HSBC view  though  theedge case I like to bring upis something called Pepper Pepper is actually arobot  originally developedby SoftBank  that wehave in 22 branches And what Pepper does isit will greet the customeras they come in So banking isn t exciting We all kind of know that Pepper makes it exciting So when you walkinto this branchand you see this robot witha really cute face lookingat you with an LCD and touchscreen  you can walk up to herand you can choose what youwant to talk about to CSRor what products youmight be interested in Not only is it funand interactive but it cuts the wait timedown for the customer This all goes into a systemthat the CSR will seeand then they can walkout to get you and knowexactly what you needbefore four other people askyou the same questions You go right to the person they know what you need and they can process yourrequest really quick Now aside from the cool factor  it does bring peopleinto the branch So people want to see Pepper  taking pictures  posting them We get our brand out there But it can go further thanjust this basic stuff So internally  Pepper is limitedto about four core M processorwith 4 gigs of RAM It can do a lot of processing It can talk to systemslocally so we can figure outwhat we need for you But to make it morefun and to expand it we can connect thatinto the Cloud So of course  usingsomething like Google we can use translationservices  text to speech  AIinteractions And one of my favoriteones that we vebeen debating  with consent would be facial recognition So Pepper can greet you whenyou walk into the branchand it can say  Welcome back  Amina Would you like tolook at this service  Or just having aconversation with youusing the AI interaction AMINA MANSOUR  That s amazing I personally hope tomeet Pepper someday I know that HSBC leveragesBigQuery for some analyticsworkloads  so tellus a little bit moreabout the analyticspattern there SCOTT SUROVICH  Yeah We re a bank  weget a lot of data We have a lot of things thatwe have to sift through look at  do analysis on So BigQuery is very popularamong our developers and that s at a global scale So with BigQuery  as you saidduring the introductions it can process your data handle millions of queriesper second  and withoutus having to expandand install hardware for this Now  in the earlierdays  we used to do this We would run things thatwould take two weeks It would run at a bunch ofkit that we also had DR for because if we didn thave a problem of fraud we d have to put it toDR  It s a lot of moneythat when you re not processinganything  it just sits there So aside from just beingslower and costing more we wanted to go tothe Cloud for that So one of ourapplications that we didhappen to move used to takeabout 10 days to processthe information It sat on a lot of kit  andfor the rest of that monthit just sat So it takes up power it takes up cooling it just takes upgeneral maintenance We have to pay for a lotof stuff that isn t in use By moving it over toBigQuery  we only utilizeor we only get chargedwhen we re utilizing it It also cut our processing timeon that operation from those10 days down to less than aday  and on certain operationsdown to a couple of hours So we get theinformation analyzedmuch quicker and cheaper And there really isnothing better  and faster and cheaper  and it s done well AMINA MANSOUR  That s agreat use case for BigQueryand the analytics pattern Thank you so much forjoining us today  Scott It is great to learnabout all the innovationthat Google Cloud productsenable for our customers SCOTT SUROVICH  Great Thanks  Amina It s my pleasure to be here AMINA MANSOUR  If you d liketo continue learning moreabout the differentmulticloud and hybrid patternsand how to implement thoseusing Google Cloud products please check outthese sessions herethat will talk about Anthos for example  as the controlplane supporting thatmulticloud or hybrid patterns as well as differentexamples with Cloud Runrunning in hybrid ormulticloud environments all the way toextending your Anthosor multicloud environment to VMworkloads on premises  as well Thank you so much forjoining the session today and hopefully it was useful STEPHANIE WONG  Hi everyone  and welcome I hope you re enjoyingGoogle Cloud Next  21 I know this isanother virtual year But I m hoping that today  I getto bring some of my experiencesto life for you I m going to betalking about whatit was like to explore the sixlayers of Google Cloud datacenter security Now  you might be thinking oflasers  night vision cameras and military grade fences You re not too faroff because securityis one of the most criticalelements of our data center sDNA and Google Cloud workloads With dozens of datacenters globally security operationsmeans managinga massively complex network I was one of the rareGooglers to visit one So I m going to takeyou on a journeyto the core of a datacenter to show youthe six layers ofphysical security designedto thwart unauthorized access But first  I ll touch on GoogleCloud security philosophybefore I take you on thatjourney to the core of the datacenter And then we lltouch on how you canleverage that samephilosophy on Google Cloud And it s great to be here today My name is Stephanie Wong I m a Cloud developer advocate And I ve been creatingdeveloper focused contentfor the last fouryears or so here Of course  when I got thechance to visit a data centerand make a film about it  Ijumped at the opportunity So let s go ahead anddive right into it So in terms oftransparency  we revery clear about our commitmentsto customers about your data Rule number one   your databelongs to you  no one else We do not sell customerdata to third parties nor do we use itfor advertising Your data is your data Your data is also encrypted intransit and at rest all times And you do not haveto ask for that And if you want  wecan give you waysto add additionalencryption on top of that We also have explicit rules toguard against insider access There is no back door to Google There s no secretgovernment access We take pride in ourinternational audits And we deliverresults on request These are the mostimportant rolesin our mission ofdigital transformation It s giving you the trust thatyou need to build great things So how can Google Cloudhelp organizationswith their security Well  first  we provideprotection from threatsthrough a secure foundationcore infrastructurethat is designed  built  andoperated with security in mind Second  we provide customerswith a constantly expandingarray of controlsthat they can useto help meet policy  regulatory and business objectives And third  we work to meet ourcompliance responsibilitiesand provide capabilitiesto make compliance easierfor customers But first  let s talkabout the protectionthat you get from building onour infrastructure foundation Our infrastructure doesn trely on any single technologyto make it secure Rather  we build securitythrough progressive layers thatdeliver true defense in depth So our hardware isGoogle controlled  built and hardened Any application binary thatruns on our infrastructureis deployed securely We do not assume anytrust between services And we use multiple mechanismsto establish and maintaintrust Our infrastructurewas designed to bemulti tenant from the start All identities users  and servicesare strongly authenticated Data stored onour infrastructureis automaticallyencrypted at restand distributed foravailability and reliability Any communications over theinternet to our Cloud Servicesare also encrypted And the scale ofour infrastructureallows us to absorb manydenial of service attacks We have multiplelayers of protectionthat further reduce therisk of any DDoS impact And finally  our operationsteams detect threatsand respond to incidents24 7  365 days a year We actually have a white paperwritten on our infrastructuredesign that goes into allof these areas in muchmore detail Other cloud providers mightdescribe a similar stackof capabilities But the way that Google Cloudapproaches many of these in my opinion  is very unique So I had the chance tovisit a data center in 2019 And I made a videoabout my experiencethat you should all check out I ll be sure to share the link But you can find itat bit ly 6layersdeep This was a huge project And it wasn t easy toget approvals or evenstep foot inside the premises But in my job atGoogle  I spend my daysworking with developers Our data centers are crucialto the work that they do But most never actually setfoot inside a data center So as you can see here  thereis a huge amount of spacethat our data centers span 352 000 square feet  or 33 000square meters And all in all  lessthan 1  of Googlersever get to set foot inside So I was determined to make avideo because for close to twodecades  Googlehas designed datacenters from the ground up Security has surfaced as oneof the most critical elementsof our DNA And it s part of our commitmentto keep our customers data safe  and not tomention Google Cloudnow meets more than 45regulatory requirements and growing Our products regularly undergoindependent verificationof their security  privacy and compliance controls achieving certifications attestations of compliance or audit reports againstthese global standards This means that anindependent auditorhas examined the controlspresent in our data centers infrastructure  and operations These certifications includesome of the most widelyrecognized  internationallyaccepted independent securitystandards  including ISO IEC27001 for security controls ISO IEC 27017 forCloud Security and ISO IEC 27018for cloud privacyas well as AICP SOC 1  2  and 3 These certificationsare helping usmeet the demands ofthe industry standards like CSA STAR andPCI DSS as wellas many other regionalstandards as well We continue to expand ourlist of compliance offeringsglobally to assist ourcustomers with their complianceobligations So while auditors can sometimesenter our data centersto perform these audits  mostpeople  including Googlers cannot Under specialauthorization  I found outthat our physicaldata center securityis built using that defensein depth foundation It s made up of sixlayers  from the exteriorall the way to the center each increasing in controlsas you get to layer 6 So I m going to walkthrough each layer and someof the most notablefeatures and experiencesthat I went through So let s start with layer 1 This is the secure perimeter Basically  it means theproperty boundaries And that includes allthe signage and fencingaround the property These are the furthest  mostexterior edges of the property So as you driveinto the property you ll see signs that mighthave a Google logo or a signage But otherwise  you reallycan t tell immediatelythat s   it s a data center It s  more or less  looks like an industrialarea or a business park But things really startto get interestingwhen it comes to layer 2  alsoknown as the secure perimeter And that includes themain entrance gate So I started myjourney by driving upto this gate  atwhich point I neededmy credentials andmy identity checked Even before stepping footinto the property boundaries I was already on anaccess approval list The least privilegedprotocol is the ideathat someone should have onlythe bare minimum privilegesnecessary to perform their job So if your least privilegeis to enter layer 2 then you re not going tohave luck moving to layer 3 Each person s accessand permissionsare checked at badge readers starting with layer 1 and progresses morestringently at layer 6 These exist at every accesspoint in the data centerfacility And authorizationmeasures happen everywhereusing this protocol This is something that we don tjust employ in a data center but it s also a bestpractice we alwaysrecommend for how you set upyour organization resourcesand data on Google Cloud So as you know  applyingleast privilege to IAM roleson Google Cloud is recommended So any user has the bare minimumroles and their permissionsto perform their functions So layer 2 has a lotof security features ranging from smart fencingto overlapping camerasto 24 7 guard patrols and more And there s actuallya lot more technologyand operations going on behindthe scenes than meets the eye So from the timethat you re on site they already knowthat you re there But what I foundalso interestingis that they re able to docorrelation analysis of whereyou ve been on site throughcameras  badge accesstracking  et cetera And there are alsoguards in vehiclesand some guards on foot As you can see here  there salso the vehicle crash barrier This is designed to stop afully loaded truck from crashingthrough the front entrance This particular black fencealso that you see hereis an anti climb fence So there are spikespointed outward It d be pretty hard to getover that in one piece It s also equipped with fiber The technology tells datacenter security operationsif someone s near thefence or touches the fence They also use thermalcameras and standard camerasso that they re able to seevideo footage at night justas clearly as theycan during the day Each camera overlaps thearea that it can cover So there s no blindspots anywherearound the data center Now  you might bewondering  well what if somebody skydivesinto the propertyor digs under the ground Trust me  they have enoughcoverage of the areathat they will detectsomething cominginto the propertyor motion that soccurring near the fence These are not onlyphysical safeguards but act as a bigdeterrent to anyoneeven thinking about tryingto forcibly come inside And more obstaclesawait them inside And we have additionaloperational monitoringhappening in any case thatwill alert more guardsto take action ifthey detect something Layer 3 is building access In this case  I wasable to enter the lobby But just so youknow  this is stillnowhere near thedata center floorbecause that s still afew more layers deep So let s say that you vegotten through the gate you ve come inside  you vecome into the secured lobby and you have your card or aform of identification  whichis checked by a receptionistto make sure that you reon the access control list But what happens ifyou lose your card Then what For that reason  biometricscanning is also employed So  yes  I did have toget my irises scanned Similar to concepts insoftware development biometric scanning is forauthentication of identity Meanwhile  my badge allowingaccess to a certain areais authorization Yes  this also feltlike I was on some sortof secret top mission  maybe Mission Impossible  or maybe Ocean s Eleven or  more recently I ve been hearing it sbeen compared to Professorfrom  Money Heist  But once that was takencare of  another ruleexists at the datacenter that preventsa vehicle or anindividual closelyfollowing another to gainentry into a restricted areawithout a badge swipe So if the system detectsa door open for too long it immediately alertssecurity personnel Any gate or door must closebefore the next vehicleor the person can badgein and then gain access You ve probably seendual authenticationwhen you try tosign into an accountand you get a one timepassword sent to your phone We take a similarapproach at the datacenters to verify a person sidentity and access At some layers inthe data center you re required toswipe your badgeand then enter a circlelock or a tubular doorway You walk into thisspecial half portalthat checks your badge andscans your eyes to gain accessto the next layerof the data center It prevents anyone fromclosely following youthrough because only one personis allowed into the circle lockat a time So you can see here that Ihave to first badge  thenstep inside  getmy irises scanned and then it verifies my identityand the door on the other sideunlocks And something not shownin the video that I madeis the facility loadingdocks  which area special section of layer 3 They re used to receive andsend shipments of materials like new hardware Truck deliveries from vendorsmust be approved for accessto layer 3 to enter the dock For security  theloading dock roomis physically isolated fromthe rest of the data center And a guard is present  a guard presenceis required whena shipment is received or sent I actually got super luckywhen I was there and gotto witness shipments being madeof new servers being delivered And in order for themto even get to layer 3 they had to have been onthe allowed access listthrough layers 1 and 2before arriving at layer 3 Guards checked theiridentification and the accesslist before letting them intolayer 2 and  subsequently opening the garage door to letthem back up into the loadingdock room Then out came fresh newserver racks  one by one And once unloading wascomplete  the vendor leftand the hardwareoperations team beganto move the racks into layer4 by badging themselves in one at a time  with the actualhardware going into the datacenter Badge checks occurat every entry pointthroughout thefacility to ensureleast privilege is adhered to Even timedauthorizations happen So you can do your job for onlyan allotted amount of time So this could apply to loadingdocks or even performinghardware maintenance onthe data center floor You re probably thinkingof your favorite movie wondering  what if supplychain gets intercepted Well  additional checks happenfor this potential risk And we have tested formany potential riskscenarios  which I ll talkabout in a little bit Layer 4 is once you ve actuallyentered the data center itself And this includes an extremelyimportant part of data centersecurity  the SecurityOperations Center  or SOC a hive of activity that smonitoring the datacenter 24 7  365 days a year The doors  the cameras  thebadge readers  iris scan  everything ismonitored from here Anyone on site atthe facility willhave their movement patternstracked throughout the propertyvia their badgeand biometric scansat entry points and doors If a door is kept open fortoo long or correlationanalysis finds thattheir presence isin an unauthorized location alerts will be sent to the SOCinstantly This is the brains ofthe security system So if there s anything outof the ordinary happening they have to be ableto pick that up The SOCs of allGoogle data centersare also designed toroll up to a regional SOCfor multi regionalhigh availability purposes If an on site SOCwere to be disrupteddue to an event  such as anatural disaster or emergencybuilding evacuation monitoring instantlyrolls over to the regional SOCfor continuous  uninterruptedmonitoring SOCs act as a centerfor emergency and safetyresponse in addition tosecurity responsibilities These SOCs monitorand respond to alarmsat all of our facilitiesand are constantlytracking local and global eventsthat could impact operationsat our data centers Think of this likehigh availabilityfor physical security And really  that also meansit s high availabilityfor logical security anddata protection as well Layer 5 is thedata center floor Servers and network equipmentthat power Google live here Security in this layer is oneof the most highly restrictive as it s where the data resides And this meant I neededto enter a circle lock get my irises scannedand badge checked And then once in there  Iwas accompanied by our guardsin every space And the VP of globaldata centers  Joe Kava took me around So only hardware operationspersonnel  securityprofessionals  andkey people grantedaccess on an as neededbasis can enter Access is limited to thetime required to completethe necessary job function So this is truly anas needed only access area meaning that only thetechnicians and engineers thathave to be there tomaintain  upgrade  or repairthe equipment areever allowed there This layer is protected by acircle lock  as I mentioned security staff  andbadge protected doors And the data center floorfeatures laser beam intrusiondetection Additional cameras are alsoplaced throughout the areawhile security staff monitor thedata center floor at all times And the technicians whohave access to the devicesare there But the data atrest is encrypted And you can also issue andkeep your own encryption keys And if you rewondering  Googlersdo not have access to the dataunless you ve authorized itfor a technical issue And this is calledaccess transparency which is a technical control It produces immutable logs anytime a Google administratorneeds to enter a customer stenant to perform something whether it s a supportrequest or maintenance And you control what types ofaccess that you want to permit For example  maintenance is OK Support s not  or it s OK for agiven set of your environment So layer 6 is disk destroyalor  as I like to call it hard drive graveyard So Google employs astrict equipment chainof custody monitoring meaning that we meticulouslytrack the location and thestatus of all equipmentwithin our data center This means that we trackthe location and statusof every hard drive  fromacquisition to destruction using bar codes and asset tags These asset tags are scannedthroughout a hard drive slifecycle in a datacenter from the timethat it s installedto the time that it sremoved from circulation And tracking thesehard drives closelyensures that they don tgo missing or end upin the wrong hands We also make sure harddrives are properlyfunctioning by doingfrequent performance tests If a component fails topass a performance test then it s deemedno longer usable So to prevent anysensitive informationfrom living on that disk  thenwe remove it from inventoryto be erased and destroyedin layer 6  disk erase Disks enter layer 6 through asecure two way locker system allowing onlyauthorized personnelto retrieve it in layer 6 which you re seeing here When a hard drive iserased and retired the disk erase formatteruses a zero filling methodthat wipes the disk data andreplaces each bit of datawith zeros And we try to recycledrives and their components But if a drive can tbe erased  then it sstored securely until itcan be physically destroyed as you can see here And it d be prettyimpossible to recoverany data from these drives And even if anyone tried to takeanything out of the data centerfloor with them  we alsohave metal detectorsand video surveillance toensure all personnel exitingthe data centerhalls are properlychecked to prevent unauthorizedequipment from leavingthe floor If you re wondering whathappens to this waste we actually have researchersin Google s Oklahoma facilitywho are looking into methodsof collecting and recyclingrare earth metalsin hard drives Data centers are the largestconsumer of hard drivesthat contain elements likeneodymium and dysprosium These are the sameelements that havea role in the componentsof electric cars And there s a greatarticle to check outif you want to learn more Leaving layers 5 and 6 isarguably more difficultthan entering  aseach person mustexit through fullmetal detectionand detailed examination And fun fact   Google Cloudalso employs a security testingprogram where undercoveractors are  in our employ attempting to bring hardwareout of the data center This helps us constantly testand improve our physical datacenter security techniques We actually run dozensof these drills a year paying unannounced skilled adversariesto try to get pastcontrols by any meansnecessary except for violence And this includes fake badges social engineering  fake UPStrucks  ladder trucks toget over the fence  peoplehidden in trucks  attemptsin thick fog  everything And we usually win After every attempt  we actuallyrun a blameless post mortemto improve and make thenext attempt even harder And we ensure the strengthof our security controlsand iterate controls as needed This is all about buildinga culture of security So these layers don t stopat the physical layer They extend into ourtechnology stack that supportsGoogle Cloud products  too So at Google Cloud  we providecapabilities and assistanceacross Google  Cloud Workspace  and connected devicesthat cover the wholescope of responsibility We have a number ofnative security featuresthat help protect youfrom vulnerabilities like encryption intransit and at rest And our hardware infrastructureis custom designed Our servers don t includeany unnecessary components like video cards orperipheral interconnects that can introduceany vulnerabilities And the same goes for software including low level softwareand our OS  which is astripped down  hardenedversion of Linux Further  we also design andinclude hardware specificallyfor security  like Titan our custom securitychip that we use to establisha hardware root of trustin our servers and peripherals We build our networkhardware and softwareto improve performanceas well as security So on Google Cloud  what s thebest way to approach security Well  cloud security isa shared responsibility Depending on what cloudservice you re using Google Cloud will managevarious parts of securityabove what we ensure throughour secure infrastructure So customers are responsiblefor securing their own workloadsto meet their requirementsand compliance When it comes to cloudsecurity best practices there are two sides to it preventative and architecturalcontrols  which are implementedthrough architecture and policydecisions And first  we re going to coverarchitectural controls  whichinclude the communicationbetween services how you structure yourresources on Google Cloud and permissions And then I ll quickly coverdetective controls  whichuse monitoring capabilitiesto look for drift  anomalous or malicious behavioras it happens Let s talk about ourapproach to security toolingon Google Cloud We have layers of securityproducts and blueprintsfor you to quicklyget started here So from the bottom up we have a global scaletechnical infrastructure that sdesigned to provide securitythroughout the entireinformation processinglifecycle at Google And this infrastructure providessecure deployment of services secure storage of data  securestorage with end user privacysafeguards  securecommunicationsbetween services  and alsosecure private communicationswith customers over the internetas well as safe operationby administrators Building on top of thesecured core infrastructure the next layer is thesecurity products like Security CommandCenter Premium  whichis our security and riskmanagement platform and assured workloads forsecuring your governmentworkloads  which enable youto secure your deployments These form the lowest levelsof the security stack And then on top of this baselineare security foundationsblueprints These are securityblueprints whichinclude Google Cloudsecurity best practices thatare embedded within them So our goal here isto make it much easierfor you to start and operatesecurely on Google Cloud They actually enable you tojump start your deploymentsby handling some ofyour key advanced needswithout your teams first needingto take time out and learnall of the advanced skills They help you set up orgpolicies as a resourcehierarchy  networking  access logging  detective controls and then postureblueprints  whichcover specific sets ofservices and workloads Blueprints alsocome in three tiers which differ in their level ofnative support and lifecyclemanagement Supported blueprints are createdby the Google security teamsand managed asproducts  which areregular refreshesand attestationand testing of thefunctionality and resilience They re all verified So blueprints can be createdby Google teams or partnerteams or even customer teams And the key is that thelifecycle managementand attestationof fit for purposeis the responsibilityof the creating team And today  they reoffered via Terraform So compatible blueprintscan be created by anyone The requirementis just that theyneed to be correctlyformatted to operatethrough the blueprinttool chains And then we alsohave blueprints thataddress specific use cases like protecting storage bucketsor doing  protecting BigQuery users  AInotebooks  or Anthos  even So the topmost layerhere is comprised alsoof your own web app and APIprotection services  analytics or even high performancecomputing environments So the securityfoundations blueprintsprovides a strong startingpoint for customersto build and deploytheir organization So customers have accessto the opinionated guideof the full Terraformrepo of automation scriptsto deploy thefoundational landing zone And it can be deployedeither at the full org levelor at the folder levelin the hierarchy These blueprints provide anopinionated vetting patternfor resource hierarchy org policies  IAM separation of duties  networkarchitecture  data accesssegmentation  logging and detective controlsacross projects and accounts So that way  you can meetyour security complianceand have a starting point And then you can also startto build your own workloadsand projects on top It provides the foundationupon which you can furtherdeploy an increasing numberof opinionated Google createdsecurity posture blueprintsfor specific products sets of services  and workloads And we update these blueprintson a six month cycle We also provide a projectfactory Terraform modulethat enables you toset the parametersand constraints for projectswhen they are created You can access the latestreference guide  PDF and the Terraform scripts repofrom the Google Cloud SecurityBest Practices Resource Center And here  in addition to thecurated set of Google Cloudsecurity blueprints you can alsoaccess both theGoogle BeyondProdpaper  which lays out theprinciples and practicesof Google s approach forCloud native securityand how we think about andhow we operate Cloud native For customers who want todive deeper into the knowledgeand experience that Googlehas  then the Resource Centeralso includes a link toour building securityand reliable systemsreference book Additionally  a rangeof detailed backgroundwhite papers aroundour reference materialsis available to customersfrom that Resource Center And it covers topics likeinfrastructure designand how encryption atrest and in transitare handled on Google Cloud On the preventativeside  we alsolaunched SecurityCommand Center This is the canonicalsecurity and riskdatabase for Google Cloud So it includes a riskdashboard and analytics systemfor surfacing  understanding and remediatingGoogle Cloud security and datarisk across your organization So this means fromone dashboard you can uncovercommon web applicationand vulnerabilities  likecross site scripting oroutdated libraries inyour web applicationsrunning on App Engine GKE  and Compute Engine And you can alsodetect threats usinglogs running in Google Cloudor common container attacks including suspicious binaryor suspicious libraryand reverse shell Security Health Analytics is avulnerabilities dashboard here And it helps you viewsecurity misconfigurationsby severity  compliancestandards  or project And you can takeaction from there So with this  you can takeinventory of your Google Cloudassets and partner solutions You can also understand exactlywhat changed in your assethistory in near real time andrespond to the most pressingissues first You can also receivenotificationsabout new findings or updatesto findings within minutesand be able to take action And also  you can identifysecurity misconfigurationsin your Google Cloudassets and resolve themby following actionablerecommendationsthat they actually have there Lastly  you can catchweb vulnerabilitiesbefore they hit production andreduce your exposure to risksand identify compliance   so seeviolations in your Google Cloudassets and resolve themwith these recommendations And of course  you wantto be able to export them So you can exportcompliance reportsto ensure all your resourcesare meeting your compliancerequirements We have support forcompliance standards like Payment CardIndustry  whichis PCI  the InternationalOrganizationfor Standardization  ISO There s National Institute ofStandards and Technology  NIST and the Center forInternet Security  CIS 1 and 1 1  and benchmarks This industry leadingthreat intelligencedetects threatsusing logs runningin Google Cloud at scale And it also useskernel level instrumentationto identify potentialcompromises of containersand  lastly  combinesthreat intelligencefrom first  andthird party providersto better protectyour enterprisefrom costly andhigh risk threats Thank you so much forjoining this session I know there s alot packed in there So I hope you found itas much of an adventureas it felt liketo go through it If you do want tolearn more  check outthe resources that we llinclude for this session And always feel free toreach out to me on social like on Twitter   stephr wong Thanks again ANITA KIBUNGUCHY GRANT Hello  and welcometo Google Cloud Next In this session we ll walk through howto accelerate your move to thecloud with managed databases If you re here to learnabout Cloud SQL  Bare MetalSolution for Oracle  DatabaseMigration Service  and more then you are in the right place We ll also explore why GoogleCloud for your databases My name is AnitaKbunguchy Grant and I am the productmarketing lead herefor the Databases organization And I have the pleasure of beingjoined by GG  group productmanager for databases So let s go aheadand get started Here s a quick agendafor this session We ll start withwhat we re seeingin the industry across trends go through five reasonswhy Google Cloudfor your databases We ll hear from GG aboutreal world customer use casesand finally end withhow to get started including some importantresources you should check out So some migration trends Managed services justmakes sense for databases I have never met someonewho loved managing databaseinfrastructure  atleast just not yet When you think aboutall the care and feedingnecessary to keep yourdatabases healthy there are a lot ofthings to consider  all the work to provision set up  and maintainyour hardware  the operatingsystem  and database software plus all the managementtools and  INAUDIBLE  patchesfor security and maintenance scaling the system ensuring that your backups andrestores are meeting recoverygoals  and that theapplication can continueto run during various failurerecoveries and scenarios on top of all this monitoring all of these thingsto make sure thatyour system is runningthe way your business needs to This is a lot of workand pretty diverse setof skills that are needed Consider thethousands of databasesrun by many companies This work really adds up Most importantly these are thingsthat are just tablestakes for youand typically notthe things thathelp you outmaneuveryour competitors It s therefore nosurprise that databasesare moving fast to the cloud According to Gartner 75  of all databaseswill be deployed or migratedto a Cloud platform by 2022 and this is massive It s important to note that theCloud DBMS market is not new but what is new is a growthin cloud revenue  whichis actually projected to accountfor 50  of the total DBMSmarket revenue by 2023 This trend actually helpsreinforce the notionthat cloud service providerinfrastructure and the servicesthat run on top ofthem are becomingthe new data managementplatforms And because ofthis  it s no wonderthat so many companiesare turning to managedservices in the cloud  for their databases  that is So why not let someoneelse who does itfor millions ofdeployments and who sbeen automating systemslike this for decadeshandle this for you Let s talk about why GoogleCloud offers the best placeto migrate your databases We ll start withreason number one We offer acomprehensive platformthat allows youto securely unifythe data across yourentire organization This allows you to breakdown silos  increase agility innovate faster  and supportbusiness transformation We connect the differentroles that work with dataand provide them a commonplatform to build upon So no matter whereyou start  there salways more places to go acrossanalytics  databases  AI and machine learning services BigQuery  our fully managedenterprise data warehouse allows you to run analyticsat scale up to 34  lower TCOthan other cloud datawarehouse alternatives Spanner providesunlimited scale global consistency acrossregions  and high availabilityup to five nines Looker provides asingle shared placefor people andapplications to interactwith it no matter the cloud And these are just afew of the key servicesyou can take advantage of onthe Google Cloud data platform As I mentioned earlier no matter where you start there s always more places togo with best in class productsacross the entiredata lifecycle Let s take a look atreason number two At Google Cloud  weunderstand that most companieshave a multidatabase strategy In some cases  it s intentional In others  it s a naturaloutcome of growth and changeover the years plus the fact thatreplatforming a databasecan be very intimidating So we ve been steadily focusedon building options thathelp meet youwhere you are todayand also give you apath for the future When you look at some ofthe most popular databases there are great optionson Google Cloud These are for Oracle  SQLServer  Postgres  and MySQL And for Postgres MySQL  and SQL Server the Database MigrationService providesa serverless experience forquickly and safely migratingyour databases fromanywhere to Cloud SQL We also support nonrelationaldatabase workloads If you re usingRedis  you can nowtake advantage of Google CloudMemorystore for Redis or RedisLabs  one of our keydatabase partners For Memcached  you can nowuse Memorystore for Memcached For HBase and if you re readyto move off your Hadoop stack you can easilymigrate to Bigtable which has been in productionfor over 15 years at Googleand is the originalinspiration for HBase For all that MongoDByou re self managing you can use MongoDBAtlas on Google Cloud All of these provideoptions and flexibilityto quickly and safely migrateto the cloud  helping meet youwhere you are Let s take a look atreason number three We ve talked about some ofthe product capabilities Let s do a deeper diveinto some of them Let s start with Cloud SQL Cloud SQL is our fully managedrelational database serviceand one of Google Cloud sfastest growing top services Over the past year the team has beenfocused on addressingmany of the major blockersfor customers  and we nowhave unique capabilitiesin the market We added the DatabaseMigration Serviceto migrate your MySQL Postgres  and SQL Serverworkloads to Cloud SQL We ve significantlyreduced maintenance windowsby over 80   and we ve alsolaunched Cloud SQL Insights which bring databaseobservabilityto your developers We were first to market tosupport the latest versions including Postgres13  because wewant to make sure thisis the simplest migrationexperience for you Cloud SQL alsooffers integrationwith Kubernetes  which makesconnecting your applicationseasy and secure It also supportsfederation with BigQuery and we ve seen users carryingover 125 petabytes of datain Cloud SQL  onaverage  in one month And so Cloud SQL is prettyready for you criticalrelational workloads on MySQL Postgres  and SQL Server The next service we lldo a deeper dive onis the Bare MetalSolution for Oracle We ve taken an optimizedapproach with our BMS solutionfor Oracle We provide certifiedand optimized hardwarefor your Oracleworkloads  allowingyou to run all versions ofOracle database  including RAC and all the tools andplaybooks you use today such as Data Guard and RMAN You get to maintain youron premises licensing  runbooks  DBAs  andsystem integrators Google provides managementof the data centerwith enterprise gradesecurity and reliability We re available in13 cloud regions and we re less than2 milliseconds awayfrom Google Cloud resources With Bare MetalSolution for Oracle you have full controlover how the data ismanaged using optimizedand certified SAN storage And finally  we ll take alook at some of the toolsand partners available As I mentioned earlier  weannounced Database MigrationService to migrate your MySQL Postgres  and SQL Serverworkloads onto Cloud SQL  andwe re seeing a lot of customersstart to use DatabaseMigration Service to migratetheir workloads We also hear that DMS is easyto use  and more than 85 of the migrations using DMS areunderway in less than one hour We also announced Datastreamearlier this year a CDC and change data captureservice that provides accessto streaming low latency datafrom Oracle and MySQL databaseswith integrations acrossBigQuery  Cloud Spanner Dataflow and Data Fusion We also work with partnersolutions like Streamand also offer assessmenttools  including StratoZoneand migVisor to get you quicklystarted on your migrationjourney Reason number four forwhy Google Cloud is weprovide robusttools and technologyto protect and govern yourdata throughout its lifecycle We realize that securityand governance arekey concerns for manyof our customers and providing thesecontrols are key to peaceof mind for your databases We offer built in dataprotection at scale and this is bydefault  All your datais automatically encryptedwhile in transit and at rest We ve also addedcustomer managed encryptionkeys for some ofour key services including Cloud SQL and Spanner Secondly  we supplytools and technologyto efficiently govern your data Cloud Identity andAccess Managementprovides accesscontrol and visibilityinto security policiesacross your organization Thirdly  we supportcompliance requirementswith third party auditsand certifications And finally  webelieve that trustis created throughtransparency  and so we providetools such as AccessTransparency which give youvisibility into accesses byeither support or engineering And all thesecapabilities ensurethat your data is protected And finally  let s lookat reason number fivefor why Google Cloud It goes without sayingthat your databases areas good as your availability We offer five ninesindustry leadingSLA across our cloud nativedatabases  Spanner  Firestore and Bigtable  and fournines SLA for BigQuery And these databasesare actually builton Google s dedicatednetwork  and they rebattle tested by GoogleCloud Services usedby billions of users and they re actuallymade available for your use All these services are simplyunmatched for speed  scale security  and capabilityfor any sized organization and these availabilitySLAs ensurethat your business continuesto run without disruption So there you haveit  five key reasonswhy migrating yourdatabases to Google Cloudis a good movefor your business from the fact thatwe offer managedservices for your Oracle MySQL  Postgres  and SQL Serverworkloads  includingRedis  MongoDB  and HBase to the sophisticated securityand privacy controls all the way to the five ninesavailability across someof our key services Whew I realize that was aton of information so it would be goodto be able to bringall these things to life So let s do a deep diveinto three key customer usecases across some ofthese key services So I m excited to welcomeGG  group product managerto talk more to us Welcome  GG GURMEET GOINDI Thank you  Anita Indeed  that is a greatoverview of our portfolioand the reasons why customersare choosing our solutions And I 100  agree with you In my 20 years of my careerhere in Silicon Valley I have not met a single personwho wakes up in the morningand said managing databasesreally excites me What they do wakeup and tell us ishow we can help them run thesedatabases more efficiently more reliably  help them buildnew customer experiences at the same timelowering their cost One such customer thatI like to talk aboutis a leading travel serviceprovider or travel company you might say This industry isin a lot of churn and customers are tryingto   these companiesare trying to buildnew experiencesfor their customers  and thecustomer base is also evolving So this company turnedto us  and the reasonthey like the strategy isthe breadth of our productportfolio  whether it be theworld s leading five ninesdatabases such asSpanner or our abilityto provide a stable secure  high performing homefor their legacy databases atstates which might be runningon databases like Oracle This particularcustomer had a lotof Oracle databases running onpremises  on AWS  you name it The permanent solution  wewere able to provide thema home where they canmigrate as is and thenupgrade thesedatabases to Spanneror other cloud nativedatabases at their own pace Not only they were able tokeep all of their run booksand playbooks and partnersas is  at the same timethey were able to benefitfrom the extreme low latencyto Google Cloud so that theycan iterate their workloads backand forth between acloud native databaseand a traditionaldatabase such as Oracle What was a littleneedle mover for them they had  INAUDIBLE of Oracle versions and they probably used everypossible Oracle HA technology With Oracle BMS  they vebeen able to leveragethose technologiesas is and movethe databases to productionin a record amount of time On the otherspectrum of industry I would like to share anexample with a leadingGerman bank we are working on All of you might know banks arereally particular about datasecurity   not thatothers are not but they re regulated towardsdata security  data residency and privacy With the OracleBare Metal Solution they were able to meettheir stringent regulatoryrequirements placedupon them as a business At the same time  with thehelp of our engineering teams they were able to migrate over ahundred mission critical Oracledatabases to Oracle Bare MetalSolution within three months Now they re usingthe data sittingin the databases inBare Metal Solutionto bring newexperiences by usingcloud native databases in GCP Last but not the least  Iwould like to talk about oneof my favorite use cases isthis big ticketing marketplacewhich built the entire businesson a set of clustered databasesrunning on Oracle And they looked to GCP to helpthem take this marketplaceand transform it inan extensible scalethat Spanner provides  extensible and five nineavailability thatSpanner provides But this journey isnot simple and needsa lot of steps anditeration between them So they decided to move theirclustered databases  whichis running on aging hardware to the state of the artinfrastructure provided byOracle Bare Metal Solution thatenabled them to not onlymodernize or at least lowerthe cost of runningthe databases usingmodern hardware onOracle Bare MetalSolution   at thesame time  startmodernizing their existingapplications with Google CloudSpanner Last but not least this customerhas been live with running oneof the largest Oracle clustersin any public cloud ofBare Metal Solution and we are very happyto have them here With that being said  Anita I ll hand it back to youto take us to this presentation Thank you for your time ANITA KIBUNGUCHY GRANT Thank you so much  GG Those are all suchgreat example use cases and I m sure many of that willactually resonate with you So how should you thinkabout moving forwardwith your migration journey A good migration plan isgoing to have four key phases First  yourassessment phase whereyou work to understandexactly what you have already Secondly  based onthat assessment you start to planwhat you should moveand how you should move it Third  you start to executethe migration itself And finally  you fine tune andoptimize what you ve migrated I think this shouldsound pretty familiar GG just talked about thecustomer who actuallywent through these typeof four key phases and they re actually stillgoing through that journey And I realized that we rerepresenting this migrationplan as a single circle but in reality  it sa bit more recursivein that you llhave your high levelmigration plan but then you also havegroups of applications calledmigration waves that have theirown specific plans as well and each subsequentplan in theoryis adapting the lessons fromprevious migration waves In addition  it s notuncommon to go back and forthbetween each of thefour different phases It s all about buildingthat migration muscle memoryand realizing thatthis is a journey And so what tools do we offer And all of these should nowsound very  very familiar For your databaseassessments  youshould check upmigVisor and StratoZone We have 50 plusregional championstrained on migVisor that area combination of CEs  PSO and partnerengineering teams whoare ready to help youcarry out your assessments We also offer partnerprograms as well We also talked about theDatabase Migration Servicefor your MySQL  Postgres and SQL Server workloads And finally  we re alsoworking really hardto improve that customerexperience across our teamsand services to help you quicklyand safely and efficientlymigrate your databasesto Google Cloud And last but not least make sure you check outthis great white paper resourcewith a lot of good informationaround migrating your databases It has a lot of thecontent we spoke aboutin this presentationand actually provides itin much more detail It will be includedin the Resourcesection of this presentation You can also get started withsome hands on labs as well I know that Cloud SQLis widely represented and so you can actuallyget started on checking outthe hands on labs there And so I d liketo thank you againfor joining us forthis presentationand for this session  and weabsolutely value your feedback so please take thetime to provide it Let us know what we didwell and what we could havedone to improve for next time Thank you so much and thanks again GG  for joining me this session GURMEET GOINDI  Thanks Anita  for having me VINOD RAMACHANDRAN  Hello Welcome to our talk addressing advanced use caseswith new CloudFunctions capabilities I m Vinod I m a product manager at Google SARA FORD  Hi  I m Sara and I m a Cloud DeveloperAdvocate at Google VINOD RAMACHANDRAN  Justthe overview of our talk First  we ll provideyou a product overviewon Cloud Functions Then  walk you throughkey FaaS and functionas a service use cases Then  walk you through theFaaS programming model Then  we ll cover keyperformance  scalabilityand availability featuresin Cloud Functions Then  walk you throughkey new security featuresin Cloud Functions  as wellas cover the cloud eventsupport that is being addedto Cloud Functions as well And to conclude  we ll covera couple of exciting demosprovided to you by Sara So let s get started Cloud Functions Cloud Functions is GoogleCloud s event driven serverlesscompute platform Cloud Functions lets youwrite small snippets of codethat react to eventssuch as HTTP as wellas asynchronous events For example  uploading a fileto a cloud storage bucketand doing someprocessing after thator sending a messageto  INAUDIBLE and processing themessage accordingly Cloud Functions aretruly serverless They scale seamlessly You don t have to manageany of the infrastructure and you pay for what you use Further  functions of astreamlined development modelwhere you can getstarted very quicklyand have it running as well In addition  functions areintegrated with different GCPproducts throughevents as well as HTTP Key FaaS use cases thatcustomers and usersuse Cloud Functions areserverless webhooks For example  doinga GitHub push commitand then processing a functionand posting a message to Slack Real time data processingwhere you can upload filesto a Cloud Storage bucket andprocess the data using MLA APIsand store the result back IoT backends  wheretemperature and humiditysensors from all over the worldupload temperature and humiditydata This process is byCloud Functions and you take correctiveaction accordingly Intelligent applicationssuch as Twilio chatbots that upload messagesto Cloud Functions which are processed by anatural language API or BigQueryfor trend analysis Functions in the FaaSprogramming modelare very easily deployable You can deploy a functionfrom the UI  from the CLI through the IDE using functionsframeworks  as well as the API Here s a simple example ifyou have a snippet of code where it s going tobe deployed usinga single command ready toserve traffic in minutes You can deploy these functionsin your favorite programminglanguage We are super excited toship these languages to you Think about Node  Python Go  Java   NET  Ruby and PHP Along with theprogramming model functions offer key performance scalability and availabilitycapabilities We are super excited toship the min instancescapability in Cloud Functions Cold starts is a realproblem in serverlesswhere whenever yourfunction starts upor your serverlessservice starts up you have a startup text Min instances lets youmitigate this problem which are pre run instances So for example  in this podcasttranscription application you can have a set ofpre run instances with eachof your functions in orderto improve end to end latencyof your serverless workflow Here is a GitHubtutorial as well Please feel freeto check it out Longer processing We re super excited toship longer processingwith Cloud Functions Now you can process yourfunctions up to 60 minutes This capability is in preview This is 6x the execution time Think about longer dataprocessing pipelinesand longer ML pipelines Along with longerprocessing  we realso shipping larger instanceswith Cloud Functions Now you can have up to 16 GBof memory  which is in preview as well as the abilityto have multiple CPUs So think about largerin memory operationsas well as like  INAUDIBLE parallel applicationswith your functions They ve also expanded the numberof regions that are availablewith Cloud Functions  goingto about 22 regions globally Along with the othercapabilities we talked about we re adding some newsecurity capabilitiesin Cloud Functions We are super excitedto ship the integrationwith Secrets Manager foryour Cloud Functions Now you can securely setyour environment variablesand access them using SecretManager in your Cloud Functionsin a programmatic fashion andmaking the developer experienceseamless as well Along with secrets we are also shippinga capability calledCustomer Managed EncryptionKeys So users can provideuser provided encryption keys which can then be used byGCP to do all the encryptionthroughout the function flow This gives morecontrol to the usersand create a level ofencryption as well And if the key does not work Google cannot even accessthe data  and it s completelyin the control of the user Shared VPC support We re also super excitedto ship this new networkingcapability  whichlets your functionsaccess resourcesin a shared VPC You can access ComputeEngine resourcesas well as on prem systems Further  this is within asecure VPC SC perimeter which is a logicalboundary that addsadditional restrictions suchas only authorized userscan deploy a Cloud Functionwithin the perimeteras well as only authorized userswithin the perimeter can invokea function Further  the function can beconfigured behind a Cloud LoadBalancer for loadbalancing as well In addition to thenetworking capabilities we re also shipping a capabilitycalled private pools thatgives users morecontrol on theirbuild time environmentfor their Cloud Function Users can have more controland how their Cloud Functionis built  thereby letting userscontrol the entire softwaresupply chain of code  build and invocations per run Along with thesecurity capabilities we are adding cloud eventsupport through Cloud Functionsas well Cloud events areindustry standard events which are open and portable making your architecturefor the long term as well We added cloud eventsupport for Cloud Functionsby supporting cloud audit logs This lets you invoke functionsfrom over 90 plus GCP products For example  every time youcreate a compute engine or a VMinstance  you cannow invoke a functionto do some processingafter the VM creation such as labeling the VM Every time a BigQueryjob finishes you can do some processingafter the job is completed The same applies for SQL Spanner  so on and so forth Now  I would like tohand over to Sara who has a couple of interestingand exciting demos for youon these capabilities SARA FORD  Thanks  Vinod In the first demo you ll see howyou can use mininstances to keepa certain number ofyour applicationswarm to improve performancetime and minimize cold starts And the reason why youmight want to do thisis if  for example you know you regoing to be receivingadditional traffic soon Perhaps it s due to like a BlackFriday  Cyber Monday event or you know thatyou just need to getthe best possible latency time So by keeping a numberof instances warm you can help reduce coldstarts and get the best latencyand the response time possible So you ll see how thatworks in this upcoming demo In this first demo  Ihave a Node js function and it s already deployed withmin instances set to zero which is the default So let s takea look at this function code Now  the entrypoint is helloWorld So that means that all thecode outside of the entry pointis going to be in global scope So in other words  here is theevent handler  the entry pointfor the function So any code outsideof that function like here  as you see is in global scope So line three  where wecall the start up method this is going to get executedany time there is a cold start like the instances warming up So we ll log thatthis is a cold start and then you ll see mesimulate a long connection This could be something to  a call to establishing aconnection with the database This could be an API callthat takes a while to respond but something that you wantto do once  cache that valueand have it available for futureinvocations of your function And so once this time outto sleep comes back then you ll see melog  I m ready now and then return  ready  back So here is a code thatis invoked every time  that is executed every timethe function is evoked And I m using the utility forjust pure debugging purposesjust to illustratewhat is happening And it s getting the currentstatus of the promise And if it s stilla cold start  itreports that  it waits untilthe 30 second time out hascompleted  and thenit says  now I mwarmed up  using the warm emoji Otherwise  if it salready a warm instance then it just reports backthat it s a warm instance And  of course  itneeds to respond backto the request withrequest completed So let s invoke our functionand see this in action So on the trigger we ll simply click here And this is going to take a  because of the 30 seconds So we can go to cloud logging And as that isspinning up  we llsee it report  as we saw inthe code  I am a cold start That will be logged first So there it is I m a cold start I m still a cold start So if we go back here we ll see I m a cold start And since we invoked itwhile it was still pending that s where we see this And then as soonas it comes back it will say I m warmed up We see that therequest completed and we ll see in the logsthe function completed So now I am warmedup  which is great But let s say weknow in advance we regoing to get futureinvocations  what canwe do to   you mightbe thinking  what canI do to have this ready to go Maybe this connection  databaseconnection already established or this API call  havethat value already cached ready to go when there is  when you can predictthat you re goingto have future invocations And that s where mininstances comes in And now you ll see meredeploy the function usingmin instances So let s go to edit And then  in these advancedfeatures  scroll down you ll see min instances And instead of using0  let s give it 5 And deploy that Now that this function hasbeen deployed with the new mininstances  we can go here andsee in details now that wehave five min instances We can confirm that If we go to the logs nowand rerun this query you ll see the callfunction s updated And you might think that thisis off by one  but insteadof five cold starts because remember  we re spinning upfive instances  youmight be thinking  well why am I seeing six Well  this first one iscalled a health check This is just to make sure thatyour call function builds And as a part of  not being invoked but just builds And since this isin global scope that s going to get executed But this is just ahealth check justto make sure that the coding  there wasn t like a compileerror  or build error or I m missing dependencyor the function  identity lock permissions  that swhat this health check is So that s why yousee six of these And now  if we were to go  we ll go back to the function Go back to trigger And you see  now you noticedthat it hit instantly And if we go to cloud run  I m sorry  run query We may see that all theinstances were warm and you see I m a warm instance So as you justsaw  you might wantto consider usingmin instances to keepa certain number of yourCloud Functions instanceswarm in case you re expectingan increase in trafficor you just need to get thebest latency times possible Now  for the seconddemo  you re goingto see how a Cloud Functioncan listen for and respondto a cloud audit log event For example  let ssay that a companyhas a policy wheretheir engineers needto put their user nameas a label on any VMthat they create Well  you can have a checklistfor their engineers to follow but they might forget orthey might have a typo And so one of the best waysto reduce human error is to quote   automated out  So instead  you couldwrite a Cloud Functionto listen to a cloud auditlog event for whenevera VM is created and have theCloud Function automaticallylabel the VM with theiruser name instead In this seconddemo  you ll see howyou can use a Cloud Functionto listen for and respondto a cloud audit log event In this scenario  you ll seea Cloud Function automaticallylabel a VM using the user nameof the person who created it So first  let s go overhow to deploy a functionto respond to a cloudaudit log event Now  notice in this deploymentyou ll specify the triggerevent filters So we have a type cloud audit log Service name is compute And insert instances whena new instance is created So let s deploy that And while that deploys  let stake a look at the code So the first thing tonotice is that the signaturefor the function eventhandlers is differentand now it s cloudevent And so we grab the data fromthe cloudevent data payload And just in case an eventhas more than one audit log we want to make surewe re respondingto the very last event The resource names  they looklike in this particular case And so we want to break itup and grab the instancename and the user name And we log appropriately Now  if you re tryingto   like in this case set a label  the API make sure that you resetting the most recent label And so you need to get theinstance label fingerprint otherwise you re not goingto be able to set the label And then we caninvoke the VM label And then  finally we log the results So let s look at thesetVMLabel  the more interestingof the two So here s the API call We set up instanceName zone  project Resource here  I have thelabel named as creator And then you ll see myuser name for this account And then we make the API call toset the label for the instance Now  let s take a peek And yes  our cloud functionhas finished deploying So let s jump overto Cloud Console and every refresh we ll seethe vmlabelerdemo is up So let s go to the VMsand create a new VM And we ll just use the defaults And as soon as that is created you ll see the label is added And the nice thing about havinga Cloud Function automaticallyperform this taskis one less thingthat you have to ask yourengineers or your developersor whoever is doingthe particular task It s one less thing forthem to remember to do So instead of having a longchecklist of  did I do this Did I do that The more you can automatethat out  the less human erroryou will have So let s go to the instance And now you can seethat the creatoris drofaras  whichis my user name or my full name  Sara Fordspelled backwards for my demoaccount purposes VINOD RAMACHANDRAN Thank you  Sara for those amazingand exciting demos So that s a wrap  folks  withour session on Cloud Functions Please check out Cloud Functionsat cloud google com functions Please also sign up forthe advanced featuresthat we ve coveredin this session You can do so byreaching out to us Further  if youhave any questions please feel free toreach out to us as well Thank you  and havea great conference ANU SRIVASTAVA  Hi  everyone Welcome to Cloud Next 2021 We re so excited you re herewith us on developer day So we re going to be showingyou how you can incorporateGoogle Cloud s AI and MLofferings into your appor service With our APIs  it doesn tmatter what platformor stack you re using we ve got you covered Today specifically we ll betalking about Google CloudDocument AI and how touse it with our serverlessofferings such as CloudFunctions for businesslogic and workflows fororchestrating your services But before we getstarted  who are we My name is Anu Srivastava and I m a developer advocatein New York focusingon AI and ML bringing these services todevelopers around the world GUILLAUME LAFORGE Hello  everyone I m Guillaume I am also a developeradvocate at Google but I m based inthe Paris office And I m focusing mainly onour serverless technologies like Cloud Functions  CloudRun  App Engine and alsoour orchestrationsolutions like workflows ANU SRIVASTAVA  Awesome So let s start withwhy we re here It s Google Cloud s missionto help you innovate fastertowards your businessgoals  and thereare several differentways that wecan help you dothis  whether it sthrough storagesolutions  infrastructure AI services  scalablecomputing  data analytics and somuch more Enterprise are after smarteasy and scalable businesssolutions A lot of these toolson GCP are thereso you can focus on yourcore business mission not all theseauxiliary functions So for example  if you havecomplex business tasks the cloud helps youdeploy  develop and scale You don t have to worry aboutinfrastructure management system updates  allof those things Maybe you have data inseveral different sourcesthat you need to use ina streamlined fashion And then  last but notleast  machine learning By leveraging our AI platform or our pre trained models you can incorporate machinelearning into your applicationwithout having to geta PhD in this field We just want you tofocus on the problemthat you re actually tryingto solve and nothing else So these are the fourtrends that we re actuallyseeing in this space We re going to demoan expense reportapplication backed bymachine learning and runningon Google Cloud So now let s actually diveinto the specific productsthat we ll be working with So let s start with documents So some of the mostimportant data in companiesisn t actually livingin databases but ratherin documents So think about everytime that you vehad to file an expense report Nearly all business processeseither began  include or end with a document So this is whereDocument AI comes in This is a platform thatwe launched at GA earlierthis year in April and it helps youturn your unstructuredcontent into structured data There s a lot of importantbusiness information in PDFsand images  and theseformats aren t actuallyeasily consumable to extractdata by a computer as it is So this is what weactually call dark data So data thatcompanies have storedfor either compliancereasons or something else but you aren tactually harnessing yetto help decision making help customers or makebetter business decisions And this is what Document AIis there to help you with So there are lotsof different waysthat you can use Document AI First we havegeneric Document AI So we actually havea toolkit of OCR We have OCR trainingfor type formsas well as handwritten forms This OCR is powerful And then we even have somethingcalled the form parser  whichwill understand any form thatyou feed it  it will understandthe question and answernature of that formand feed it back to you  keyvalue pairs in a JSON formatthat are much moreeasy to use than doingthat parsing yourself from OCR Then  in the middle  we havespecialized Document AI So we have trainedseveral modelson the world s most commonbusiness form types youcan leverage into your system This is going to be thefocus of our talk today but let me also talk aboutcustom document processing So we also havetools in our toolkitthat you could train a model onyour proprietary business formtype so you can ensure accuracy So specialized Document AI We have differentsolutions optimizedfor differentindustry use cases Let me talk about that First  we havelending Document AI So this is a set of specializedmodels for forms such as W 2s W 9s  bank statements All of these aredifferent piecesthat you d have in atypical mortgage packet So using this solution  you canstreamline data capture and allof the different steps that needto happen in a mortgage processto make it faster Then  in the middle  we haveprocurement Document AI So this covers invoices receipts  utility statementsand more And this is what we llbe demoing today But before we getthere  let me talkabout what we justlaunched at Next this year contracts Document AI It s been saidthat contracts canbe some of the mostimportant business documentsthat you have So we ve trainedmodels to extractkey pieces of informationthat you can thenuse to make importantdecisions  likeprogrammatically understandinghow long the contract is valid things like that So what s the magic in Doc AI It s not just OCR It s a whole pipelineleveraging the technologythat we ve been buildingat Google for decades So first we have Doc Prep So let s say you uploadan image of a wrinkledor a coffee stained document We ll use image toolsto fix SKU  betterenhance the text so data canbe more easily extracted Then we get tosplit and classify We want you to beable to leverageyour data and the rawform that it comes in So think about alarge PDF packetwith several differentform types in it Well  by using our splittingand classifying tools you can actually identifywhere these sub documents startand end and what documentsare there so thenyou can take those specificpieces of your packetand then feed them throughthe correct processor Then we have entity extraction So this is where we extractthose key pieces of informationthat you need per doc type So even though you mighthave high variance doc types every receipt and invoicemight look different We know that there are pieces ofinformation that are universalto these document types So for procurement every invoiceis going to have aninvoice ID  every receiptwill have several line items They ll have currency They ll have total amounts They ll have tax values So we ve designed aschema per form typethat you can guaranteeyour data willbe merged into and returnedto you in your JSON response So this makes itmuch easier whenit s schematized to theneither feed into a databaseor give the piecesof informationthat you need to adownstream service Now  let s talk aboutsome more magic pieces Now we havenormalize and enrich Have you ever been codingwith dates and timesand have a formatting mismatch I m in America andGuillaume s in France and we use differentdate time formats Well  Document AIwill do normalizationon monetary  dates and addressesso it s more easy towork with whatever styleyour system is in Here  we also use for enrichmentthe Google knowledge graph So the Google knowledge graphis a service that powers searchas well as severalother applicationsthat lets you know contextualrelevant information So let s say on your documentyou have an incomplete businessname or maybe an addressthat isn t full or correct The Google knowledge graph willsupplement this informationwith the key piecesof informationthat you could thenfind in a search query Then  last butnot least  we knowwith these criticalbusiness processes you have no room for error So with thehuman in the loop platform you can select a confidencethreshold where you can havea document reviewed if certainkeys don t meet a certainvalue  say 95   98  confidence or if the whole documentdoesn t meet thatconfidence threshold So I know we went through thatreally fast  but now let swork on taking thisAPI and turning itinto a full fledged application So I m going to handit over to Guillaumeto tell us how we regoing to do that GUILLAUME LAFORGE Thank you  Anu Anu described the bigpiece of the puzzle  whichis where most of the smarts are but we ll need some businesslogic  some business processesto integrate everythingtogether  to create a full blownuseful application for our endusers In a short momentI ll show you howwe use Document AI  CloudFunctions  and workflowsto craft a smallexpense report solution But how are we going to do it Well  we are going to use GoogleCloud s serverless solutionsfor business logic thanksto Cloud Functions What is serverless and why should weuse that insteadof your homegrownservers or virtual machines Serverless solutions allow youto focus on your applicationand not on infrastructuremanagement or provisioning Serverless productslike Cloud Functionsfully manage the environments Developers can use toeasily deploy and scaletheir logic without hassle So from a programmingmodel perspective we usually createstateless servicesbased architectures which scale more simply And last but not least since the scalingis handled by theplatform for you we also apply only forusage pricing model where you pay really  proportionalto the actual usageof your application Cloud Functions is abit like some cloud gluefor a more programmable cloud Tying various usefulservices together reacting through eventsflying through your system There are  INAUDIBLE  availablefor all major programminglanguages  whether it sNode js  Python  Java and more Cloud Functions react to events like incoming HTTP request background eventsfrom the cloud like a new file stored onCloud Storage or a new documentin Cloud Firestore our NoSQL database And you can easily deploysmall units of business logicon the platform from theconsole  the Google CloudConsole  or from thecommand line as well Applications are often composedof several services or APIsthat you can call in turn tocreate the overall businessprocess  like invokingthe Document AI APIor calling Firestore to storesome structured data as wellas some Cloud Functionsto implement some logic like finding the rightinformation from the DocumentAI s output For orchestrating thoseAPIs and services we can take advantage ofGoogle Cloud Workflows Workflows is asolution to orchestrateGoogle Cloud andHTTP based API services including third party It doesn t have to be somethingsolely hosted on Google Cloud and to tie them intoserverless workflows It s about automatingcomplex processes Workflow is a fullymanaged servicerequiring no infrastructureor capacity planning It offers fastscalability support scaling down tozero when not usedand a pay per usepricing model per stepin the Workflow  per step model And last but not least  forthe developer experience it deploys instantaneously You have a YAML baseddeclarative language You can also use JSON actually  with variables with expressions and formulas You can apply some conditionallogic for conditional stepexecution and branchinginto your workflow stepswith  what else Built in JSON parsing When you need to parse theAPI output from the APIthat you call witherror and retrylogic when an API fails andyou need to retry and call itagain  and various otheruseful built in functions There s also built inauthentication for Google CloudAPIs and services So for example services that wouldbe hosted on Cloud Runor Cloud Functions are pretty muchtransparently authenticatedwhen called from theworkflow definition It executes workflowswith the reliabilityrequired for enterprise andline of business applications Workflows offers fast schedulingof workflow executionsand transition between thevarious tasks and steps And also  predictableperformance with no coldstart at all In terms of pricing as I said  it sa per step executionpricing approach So let s have a lookat the demonstration It s an expensereport application We ll use Document AI for thesmart parsing of the receipts Cloud Functions for some logic for invoking the workflow calling callback endpointsfor the manager approvalor also transforming somelight ETL for transformingsome complex data intosimpler data structure thatwill store in CloudFirestore as structured data And  of course  Workflows toorchestrate the whole process So let s have a lookat the demonstration Here is our expensereport application As an employee  I haveaccess to this page whereI can select and upload somereceipts to create my expensereport So let s click onthe Select button I m going to picka couple receipts Then I m going to clickthe Submit Report button So the receiptsare being uploaded and the report is submitted And now it s already inthe processing queue It s going to be handledby Document AI who sgoing to look at the items  theline items  the supplier  etcetera  that are recognizedin those receipts Now  let s have a look at theadministration page  whichis the page thatmanagers will beable to look at toaccept  reject  approveexpense reports So here we already have onereport waiting in the queue So the previous reportthat was submittedcontained two receipts I can click to see the details So there s the RaintreeRestaurant and the RaphaelRestaurant  and I can see somedetails of what was ordered the price  the totalamount  the currency And Document AI is ableto do some data enrichmentso I could potentiallysee the knowledge graphinformation  forexample  with the detailsabout the addressof the restaurant Then  if I m happy withthis expense report I can just click onApprove or  if I m unhappy I can click on Reject And all thoseactions are actuallygoing through the Workflow the Workflow definition And for example  now Ican see that my reportcame into the approved section That s the one thatI just approved And  yeah The one I had submitted justa few seconds ago arrived This time  the vendor wasn trecognized  but at leastthe amount  the currency andthe details were handled nicely So let s have a look at howit s done under the hood So first of all I m going to showyou the Workflow definition So as I said  everythingis going to the Workflow or through the Workflow So whenever there san action in the UI it s going through some CloudFunction invoking the Workflow So  for example  here I m preparing my workflowdefinition with somevariables  but let s havea look at this one With Workflow you cancall REST APIs  any API whether it s fromGoogle  your own APIs or it could be thirdparty external APIs But here  I m not callingDocument AI directly I m actuallycalling a connector which is a dedicatedwrapper  runthe REST API for Document AI And what s great with thatis that  although invokingthe batch process of goingthrough all the receipts I don t have to handlethe waiting  logic queuing  et cetera I just need to say  OK  I msubmitting all those receipts They are here You ll find everythingthere  and I minterested in the results Please put the results ofthe processed annotationsin Cloud Storage So in a few lines  we reable to call Document AI And then  so either it sworking or whether there san error potentially So we might branch in differentplaces of our workflow And by the way  you can see thevisualization of your workflowon the right side here I m updating Firestore toexplain what the status is But here  if theparsing succeeded we can have a look at theannotations  the resultsreturned by Document AI And we have a littleCloud Function herethat s going to parse the JSONfile generated by Document AIto find the relevantdetails I m interested in So this function process annotations and what it s doinghere is that I mgoing to tell this CloudFunction  OK  let shave a look at the filesin this storage bucketwhere the JSON generatedby Document AI is located Then I m going to gothrough all the files Each receipt has a fileassociated with it So I m downloadingthe file locally and I m passing it with JSON And then I m going to handfor interesting fields For example  I m curiousabout the supplier name I want to learn about all theline items that are available each line I ordered a pizza I order some drink I m also interested inthe total amount of money but also thecurrency  because it simportant to know whichcurrency was used And then I mcreating this summaryand then I m storingthat in Firestore All the metadata  I mstoring that in Firestore And let s go to Firestore whereI can see the results here So what s stored in Firestore that s things like the status whether here it sawaiting validation but here I see the lineitems for this first receipt the line items forthe older receipts But if you comeback here  you llnotice that the reportis awaiting validationbecause we want the manager toapprove or reject this report So that s why we have theApprove and Reject button But what s happening here We ve got the workflowthat is running but there s a very usefulfeature in Workflowthat s called callbacks A callback is a way to waitfor some external input whether it s a human input Here  it s the manager that sgoing to approve this report But it could besome external systemthat is going to callback the workflowto continue its execution And let me show you thispart of the workflow I m creating a callback here So I m saying  I m ready toreceive an HTTP post on the URLthat workflow is goingto generate for me And then I m also going towait for the workflow to be  the callback to be called So this is here  await callback And I m waiting just anhour  but I could wait longerpotentially  up to one year And then either it worksor not  it s rejected or perhaps there s an errorand I update Firestorewith this information And that s where here  again I interact with Firestoreto store the approvalor rejection So that s about it for the demo So to recap what yousaw in this demo  here san architecture diagramthat sums it up So we have the two web pagesfor the employee submittingthe expense report and for themanager approving the report So the first step is to uploadthe receipts into Cloud Storagebuckets We ll do that thanksto Firebase Storage which uploads the documentinto Cloud Storage buckets Then  again  from the UIfrom the employee view we ll go through aCloud Function thatis going to invoke workflowsto create an executionand stop the executionof our workflow What Workflow is going to do  well  it s going to storesome data about the expensereport that s beensubmitted  but then it sgoing to invoke Document AIand let Document AI fetchthe receipts from CloudStorage  analyze the receiptsand then store theresult into a JSON filein another bucket  aCloud Storage bucket Then  the workflow is goingto call another function whosejob is going to actuallygo through the JSON dataand pick up just theright information we reinterested in Line items  supplier name amount  total amounts currency  et cetera So we ve got the data In the manager view the manager willbe able to click on eitherbuttons  the one approvingthe expense report or the onerejecting the expense report And we ll go through thecallback  the callback URL From this web page  we llcall back into the workflowthrough a CloudFunction to continuethe execution of theworkflow and storethe result  approved orrejected  in our CloudFirestore database And that s about it forthis architecture diagramof our expensereport application So thanks a lotfor your attention We d like to encourage you tohave a look at those few linkshere to get started on DocumentAI  Workflows and CloudFunctions We are really lookingforward to hearing from youabout what you re going to buildnext with those technologies ANU SRIVASTAVA  Yeah So you can take alook at the sourcecode of what we showed today Guillaume and I had alot of fun building this We used Node js withall of these services We ve publishedthe code to GitHub Let us know what you think Tweet us what you regoing to buildor what you re excited for And let us know how we can help JASON DE LORME  Hi Welcome to oursession on automatingthe modernization of VM basedapplications to containers My name s JasonDe Lorme  and I mjoined by my peer Mike Coleman There s three areasof modernizationthat you can considerwhen moving applicationsto the cloud The first is infrastructuremodernization And when we think about thison the axis of complexityand value  infrastructuremodernizationis by far the easiest step As you graduatethrough this scale complexity increases withchanging code and buildingcloud native applicationsall the way upto process automation Moving and adoptingcloud native technologiesalso requires optimizingDevOps practicesand optimizing yourpeople and processes So really to land thosefirst few workloads we re going to talk to youtoday about infrastructuremodernization and some of theoptions that you have here Often customers go through anapplication rationalizationexercise in order to determinewhich applications to moveto the cloud in which order So the first categoryof applications often called legacyapplications and sometimeseven referred to asmonolith applications that all of the functionalityis into a single binary or onecomplicated application And monoliths aren talways a bad thing But when they re VMbased  potentially theytake a long time to start They load logs They load cache They don t necessarilylend themselvesto being able to be replicatedinto multiple instances We think of these traditionallyas not ready for cloud native However  they can be rehostedinto infrastructure VMs so moving from on prem VMsinto a VM in infrastructureas a service  for example The other option isto start thinkingabout maybe decomposingthat monolith and reimaginingit  and refactoringto cloud native services The second area are what wemight think of as containerready And this actually mightbe bigger than you think The options here you can clearlyrehost these applications intoan  INAUDIBLE  based approachwith VMs  or youcould replatform themto something like Kubernetesand put them in a container and have them orchestrated This is really going to be thefocus of our talk here today As you think about someof these other container  cloud compatible or cloud friendly or cloud nativeapplications thatmight already be in containers those are certainly candidates We ll dive intonumber two today So what are containers If you ve been under a rockfor the last five  six seven years  thenyou ve probablynever heard the term container And ultimately they re a methodfor packaging up an applicationand all of its dependencies The most common phrase youhear related to containersis it works on my machine hence it will work in the cloud It will work onsomebody else s machine And we often get away from someof those mysterious dependencyissues or configuration issuesthat weren t necessarilythere when you ran it on yourlaptop in dev  for example Other phrases that we hear oftenassociated with containers things like being lightweight Because it doesn t takethe entire operatingsystem   it s just yourapplication and allthe dependencies   itbecomes more portable It s using a standardizedformat that can be used It helps increase productivityand ultimately helpsdrive security as well So why do we do this Why do we build containers If you think of a traditionalVM based application often choices were madeto put an applicationper VM inside of thecustomer s infrastructurebecause of that bleedingover of dependencies Maybe one applicationleverages a shared librarythat can get overwritten and that can create havocfor the other application We ultimately optimize thoseapplications for the minimum VMconstraints for theoperating system  a certain amount of memory a certain amount of CPUand disk  not based on the application srequirements itself So we often see underutilizedhardware being leveragedand also a complicated rolloutprocess for every single VM because every singleapp has its own VM If we can move to aplace where applicationscan co exist and notstep on each other and not conflict witheach other in a single VM we can get to aplace where we reoptimizing theinfrastructure  optimizingeven the licensing costssince often the operatingsystem is licensed at theVM and not at the container If I can get two applicationsinto a single VM I ve halved my licensingcosts for that Or I can get to highavailability easierwith the same licensingcosts because Ican have two instances ofthis application running Each VM now hastwo instances here So I mentioned thecontainer based approach You can use containers forWindows or Linux machines Now  Windows is alittle bit more recentwhen it comes to adoptingcontainerized strategies For that reason there area few different behaviorsversus Linux  which wasbuilt from the beginningto support containers There are a handfulof things that youneed to think about when movingto a containerized environmentin an orchestratorlike Kubernetes Specifically  some ofthose Windows containersdon t support privileged pods That means those thingslike virus scanning softwarereally live at the VM level They don t live insidethe container itself Those types of servicesneed to be examined Anything that has low levelnetwork dependencies  whereit s dependent on somethingbelow layer 7 in the networkingstack  for example you don t necessarilywant to depend on the IPaddress of another machineor joining a cluster wherethere s a heartbeat in betweenand they need to knowa static IP address Can be a little problematicwhen you think about IPs And we can talk aboutways of addressing that If there s low levelGPU or TPU support this isn t available ina Windows container yet Cloud native features likeIstio to create a service meshin order to help with trafficmanagement and servicediscovery This isn t supported inWindows containers just yet nor is being able to usesomething like CloudRunor Knative  the open sourceversion where you can stealthe zero in aserverless environment  not quite ready forWindows containers So let s talk aboutwhat is ready And what are the idealcontainer candidates So first off  I would look atyour web applications  your weband logic tiers  thingsthat   maybe they rewritten in Python  PHP Java  ASP NET  for example those things hosted in IIS Those are greatcandidates potentiallyto run inside of a containerin a container orchestratedenvironment like Kubernetes Another area to look at is batchjobs or console applicationsthat may be runningspiky workloads things that run maybeevery day at 6 00 PMor things that run andlisten to queues that don thave any kind of GUI to them They re an ideal candidate Even Windows Services those are applicationsthat are registered withthe Windows operating systemto begin at startup for example They may even log to theWindows application event log All these things can becontainerized and runin a container orchestrator Now let me jump to the lowerright hand side of thisand talk about thosecontainer workloads thatare potentially unsuitable And I m going tostart right away I mentioned GUI  thegraphical user interface Anything that hasa GUI is not goingto be a good candidatefor a container You can t run a remote desktopprotocol session  for example into a container So virtualized desktopinfrastructure  VDI for example  is nota fit for containers If the applicationyou re using is primarilycentered around user interactionthrough a Win32 console for example  thatis not somethingthat s going to runinside of a container Now  there s agray area of thingsthat you may want to investigatefurther and use a fitassessment tool  for example to determine whether it sappropriate for running in acontainer   things on the Linuxside where there s a GPU orTPU dependency  areas wherethe container requireselevated privileges Another big category hereis commercial off the shelfsoftware  or COTS Now  most ISVs have comeout with newer versionsof their applications thatare containerized already So it s often a good fit tothink about maybe a versionupgrade that will take you totheir container based approach However  there are alot of applicationswhere maybe the vendoris no longer in business or maybe it s criticalto the business but nobody really knows whatthe application was written in You just have the binaries You don t have accessto the source code They could be a fit However  we should runan assessment tool herein order to determine whetheror not it will work wellin a containerized environment And with that I d like tointroduce my peer Mike Coleman who can talk to youabout a solutionthat we have at Googleto help you with this Mike MIKE COLEMAN  Thanks  Jason I want to talk to youtoday about a tool calledMigrate for Anthos and GKE We built this toolto help automatethe migration of virtualmachine based workloadsfrom environments like Amazon AWS  VMWare  Azure  and evenGoogle Cloud Compute Engine This tool automaticallyinspects those container  those virtual machines  excuse me   and convert themto containers where then you canstart taking advantage of someof the benefits that Jasonmentioned previously  thingslike improved resourceutilization  unified loggingand monitoring  and thenpotentially day 2 operations So it s one thing totalk about a tool It s another thingto actually show it So I want to do aquick demo whereI m going to takeyou through a paymentservice based virtualmachine migration So I have an onlineboutique  an online shopping and a bunch of the services arealready running in containers So think of the frontend  the shoppingcart  some of those services They call back toa payment servicethat s running in a virtualmachine on Google Cloud We re going to migratethat VM into a containerand integrate it with therest of the application We ll start withthe fit assessment As Jason mentioned  it scritical to understandhow well your VM based workloadcan run in a container From there we re going tocreate a migration plan We ll make a few tweaks to that We ll run the migration We ll get a set ofdeployment artifacts We ll deploy those And then finally  we lledit the service discoveryso that it s not looking forthe virtual machine  but ratherthe payment service runningas a Kubernetes service This is the VM that we regoing to migrate todayinto a containerrunning on Kubernetes So let s go ahead I m going to go up here andclose this down because wedon t need it anymore And let s take a look at thisapplication actually running So if I come overhere and I clickon one of the products the antique camera I click Add to Cart and then I clickPlace Order  everything sworking as expected That order was processedusing the payment serviceon the virtual machine So the first thing we need to doto get this all working thoughis to stop thepayment service VM A virtual machine can t bemigrated until it s stopped With the VM stopped  let s goahead and clear the screen and close out of Cloud Shell Now let s take a look at themigration fit assessment I ve created a reportby running an agenton the virtual machine  and it sstored here in this JSON file I can upload this inthe Cloud Console and it s going to give me anindication that my applicationis a good fit for migration If I want to drill down I can click on the nameand get a list of allthe different things thatwere inspected We know that we regood to go here so why don t we go aheadand close out of thisand actually startthe migration I m going to goover here  and I mgoing to click onMigrations  and I m goingto click Create Migration I m going to givemy migration a name I m going to call itonline boutique migration We ll choose our source The instance name waspayment service VM And we re going tojust do the image So let s create the migration In the interest oftime I ve sped this upso we can move on to ournext step  which is to reviewand edit the migration plan So the migration plan comesup  and the first thingI want to do isadd an annotationto tell Migrate for GKE tocreate an image that canrun on an autopilot cluster I also want to go and adjust theimage name and the Kubernetesservice name and deploymentname to payment service insteadof payment service VMs sincewe re not running in a VMany longer So this takes a few minutes I ve sped the video up I ll rejoin youwhen it s finished With the process completed let s take a lookat the artifactsthat were created So I ve come downhere under Next Steps I click Options andReview Artifacts And that will bring up a listof what was created for us The first thing you llsee are a coupleof Docker images onethat will actually deployand one we can usefor day 2 operations We also get a Dockerfile anda Kubernetes deployment spec So if I open upCloud Shell  you llsee that I ve alreadydownloaded these files So I ll do an LS here You can see all the files Now  this Kubernetesdeployment spec yaml is goingto create a newKubernetes deployment and it s going to createa new Kubernetes servicefor our migrated workloads So let s go ahead and do that Let s make sureeverything s started We ll take a lookat the service And there it is running And let s take alook at the pod And it s running as well So what we re going to do nowis check on our application Now  actually  Iexpect this to fail So let s come in here Let s click on the typewriter And let s go ahead and add thatto our cart  and check out So it s in our cart Place order Now  what happenedis it s tryingto reach the payment serviceon the DNS name of the VM So what we needto do is actuallymodify the deploymentspec for our application And we need to change it fromlooking at the internal DNSname of the VM to theKubernetes service name So that s payment service So we ll chop off the DNSname here and just leavepayment service We ll save that out And then we can go back in andrerun that deployment YAML And that s going toreprovision our services So let s go ahead and do that We ll do a kubectl apply andapply that updated boutiquedeployment yaml It ll reconfigure the services So let s go ahead andclear the screen here Let s look at the pods It looks like the checkoutservice has been reconfiguredand is running So if I come in here andI click on the typewriter and I add it to cart and I place order everything isworking as expected We ve successfully migratedour payment servicefrom a virtual machineinto a containerrunning on a GKE autopilotcluster using Migrate for GKE So that s the demo And demos are great They give you a feelfor the product and you can see some ofthe things in action But it s important also tolook at how the tool s usedin the real world So I want to start bydiscussing a customer example This is The Telegraph They re a content management content publication serviceover in the UK And they had a legacycontent management systemthat they were running It was super importantto their business but it was a littledifficult for themto manually migrateit into containers So they used Migrate for Anthosto do that work for them  vastly simplified the process And they re alreadyrealizing savings not just on the infrastructureas Jason mentioned beforewith the binpacking and whatnot but also in the day to dayoperation of that CMS So that s one customer That s one example Now  what about yourself Now  if you re goingto look at migrating there s some things Iwant you to consider These are six items They re not insurmountable byany stretch of the imagination They re really presentedhere just as things for youto be aware of And the first wesaw in the demo We were using VM basedDNS hostname discovery So we had the DNS nameof that payment service We converted that toa Kubernetes service So we let Kubernetes take careof that service discovery usingits own internal DNS structure Along those same lines if you have host filesthat you ve customizedand you ve got hostnames you can use host aliases in thepod spec to take care of that So that s anothersimple migration Again  looking at thepod YAML  a coupleother things we could takecare of are file shares So file shares are great You can use them We just don t automaticallymigrate them for you You re going to need to defineNFS persistent volumes insideof the pod spec totake care of those And the same withenvironment variables You may be using anenvironment variablefor the URI for yourdatabase  for instance You can actually define thatsame environment variablein the pod spec and then you don tneed to change any otherapplications because you regetting the same informationyou re expecting So find the environmentvariable in the pod YAML and that ll take care of that Now  I want to say something  and I might say itkind of forcefullybecause I really believe it  just because you re migratingsomething to a containerdoesn t mean that you don thave to understand how it worksand how it runs And matter of fact  if you wantto have a successful migration it s critical that youunderstand what you haveand how it works When you go in and youlook at a migration you should be looking at theservices and the agents thatare running in that VM andmake a conscious decision onwhether or not you needthem in your container For instance  youmay have an agentrunning that s doinglogging and monitoring And we ll talk aboutthat in a little bitin the next bulletpoint  but youprobably want to disable that You also want tomake sure you rerunning at the right run level So go through  auditthose services Disable the ones you don t need That will help minimizecontainer image size and improve performance and reduce complexity So this is a reallyimportant onethat I want you to go through The other thing that snot specificallycalled out here along the samevein  file system structure Are there directoriesand files that you don tneed in the resulting image You can tell Migratefor Anthos and Migratefor GKE to not include certaindirectories  certain files Again  smaller imagesize  better performance And then finally  I alluded to it  application logging We integrate Migrate forAnthos and Migrate for GKEwith the cloud  GoogleCloud metrics  logging and monitoring services So you don t actuallyneed to have those agents You don t need to do thatwork in the container We ll just pick thatstuff up for you and you can monitorit in Cloud Consoleright there within Google Cloud So that s what wehave for you today I really want to thank youfor tuning in on behalfof myself and Jason Why don t you go out  havea great rest of your day and look at migrating someof those VM workloads RAKESH DHOOPAR  First ofall  I want to welcome youto Google Next 2021 In this session  Iwill focus on what roleintegratedobservability tools playin helping developing teamsbecome elite performing teams So without furtherdelay  let s get started My name is Rakesh Dhoopar I m a Director of ProductManagement at Google I focus on our productsrelated to cloud observability In today s session  Iwill touch on five areas First  I will startby talking about what do we mean by the term  elite performing teams  While transformation towardsbecoming an elite team usuallyrequires a wide varietyof changes  includingadhering to certainoperational principles evolving operationalprocesses  and adoptinga variety of tools in this session I will focus onobservability tools only We call our offering forobservability the Cloud OpsSuite I will then divedeeper into someof the specific capabilitiesthat help developersand operatorsreduce the time theyneed to spend on identifying diagnosing  and troubleshootingproblems Now  this part ofthe talk will focuson how we make iteasier to instrumentand collect telemetry signalsand analyze them effectivelyto improve troubleshooting Lastly  I will talk about howdevelopers and operators canovercome frictionbetween the two teamsso that they can move fastwhile maintaining reliabilityof their services So let me first startby defining what we meanby the term   elite teams  And to illustrate that  I willrefer to a quote from the Dorareport So some of you may be wondering what is the Dora report Now  at its very foundation Dorais an ongoing research programsponsored by Google It is the largest andlongest running researchprogram of its kind The research programcollects and analyzes datafrom over 30 000practitioners and analyzesthe practices that drivetheir software deliveryand operations performance Participatingorganizations are measuredon metrics forsoftware development metrics for softwaredeployment  and metricsfor service operations From this collecteddata  we thendo cluster analysis tobenchmark organizationsagainst the industry andidentify characteristicsthat are grouped into cohortsfor low  medium  high and elite performers Elite teams are the highestperformers on at least oneof the metrics measuredfor either softwaredevelopment  softwaredeployment  or serviceoperations Now  one of the most interestingfindings from this researchis that many professionalsapproach these metricsas representing aset of trade offs Now  these professionals believethat increasing throughputof software developmentwill negativelyimpact the availability andreliability of their services However  to thecontrary  this researchhas consistently shown for the last six years that speed and stability areoutcome that enable each other Simply stated elite performers areable to move fast anddeploy their code rapidlywithout compromisingon service reliability The research alsofound that elite teamswere able to deploymultiple timesa day  while low performerswere deploying less than onceper month Now  at the same time these elite performershad a time to restore aservice of less than one hour while the low performersreported between one weekto one month I m going to briefly talk aboutone of our customers in moredetail  and thatcustomer is Lowe s So Lowe s is one of the largestdo it yourself home improvementretailer in the United States Now  as a part of theirdigital transformation efforts they decided to modernizesome of their applicationsusing Google Cloud Now  as they went throughthis business transformation they worked closely with Googleto transform their organizationfrom a traditionalIT ops organizationto an SRE type organization For their observabilitytools  theypicked our cloud nativecapabilities offeredby the Cloud Operations Suite When they embarked on thisjourney in 2019  in early 2020 they had to acceleratethis transformation effort The pandemic forcedmore and more customersto transact online And they had to integratethese cloud native appswith pricing systemsthat were runningon prem and with many storebest inventory systemsso that they couldfulfill customer ordersfrom a particular storein a timely manner Then  during the BlackFriday  Cyber Monday period their transaction volumeincreased by 3 timesof their alreadyhigh daily volume Lowe s handled theentire workloadand their observabilitychallenges without any hiccups The results they sharedwith us were simply amazing They were able toreduce their mean timeto acknowledge from 30 minutesto 1 minute  their mean timeto restore a service from2 hours to 20 minutes while increasing theirdevelopment velocity by 300x They were able to movefrom one monolithic releaseevery two weeks to20 plus releases a day Now  that s quite a story So let s dive into theobservability tools offeringfrom Google Collectively  we call thesecapabilities Cloud OperationsSuite Cloud Operations Suite isGoogle s observability solutionthat was designed for scale It includes CloudMonitoring  Cloud Logging and advancedobservability tools Cloud Monitoringcaptures and analyzessystem and application metrics It also enables usersto define and monitorservice level objectives And it automatically trackscorresponding error budgets Cloud Logging offers users theability to aggregate systemand application logs and explore these logswith an intuitivelog explorer UI Our advanced observabilitytools include Cloud Trace And that enables users to trackhow requests are propagatingthrough your applicationsor microservicesand receive detailedreal time insights Now  it is important tohighlight the Cloud Loggingand Monitoring are bothbuilt on the same platformthat Google uses internallyfor its SRE operations Now  as a result  asyour applications scale you don t have toworry about scalingyour observability tools Google Cloud Logging processesabout 2 and 1 2 exabytesof log data every month Now  while mostenterprises may notrequire that levelof scalability you should feel comfortablethat logging and monitoringsystems will be ableto scale not if but when your businessneeds suddenly changeand you witness an orderof magnitude more usersor are faced with any kindof cloud burst use cases And that has actually happenedwith several of our customers And I will share acouple of examples here Gannett  a media companythat owns  USA Today  they saw an order ofmagnitude higher numberof users on electionnight  2020 in the US In another example  oneof our gaming customerssaw their users expandovernight every time theylaunched new games So they have easily seen morethan a million users join overa very short period of time And in both cases the customers wereable to scaletheir applications and along withthose applications their observability solutions Similarly  ourmonitoring serviceis also based on a highlyscalable time series platform When applications aremonolithic and deployedon a small numberof named servers the volume of metricsand monitoring toolswas not very high Now  with the move to Kubernetesand microservices basedarchitectures  the number ofcomponents generating metricshas gone way up In some instances  we haveseen that metric volumeshave actually gone upby almost 100 times Also  because of the ephemeralnature of components the granularity ofmetrics has shiftedfrom 1 minutecollection frequencydown to 10 secondcollection frequency And sometimes  even down to1 second collection frequency All of these have createdextremely high requirementson scaling the underlyingtime series platforms And we want to make sure that weprovide a platform that scalesto meet your toughestscaling requirements So with thathigh level overview let us first lookat how users caninstrument theirapplications and collecttelemetry signals fromsystems and applications thatare running on GCP Instrumentation and datacollection have alwaysbeen a challenge in IT And we have seen a proliferationof agents over the past twodecades Google has been a strongproponent of open standards And we ve been activelydriving OpenTelemetry standardfor service instrumentationand signal collection Now  as the standardsevolve and mature we have been adoptingit in our productsto make it easier fordevelopers to instrumenttheir applications The OpenTelemetry standardconsists of three key things A  language specific APIand SDK for developersto instrumenttheir applications B  the OpenTelemetrycollector and receiversfor collecting signals And C  theOpenTelemetry exporterfor sending data to differentvendors for storage visualization  and alerting We re currently working oninstrumenting our clientlibraries with OpenTelemetry starting with the gRPC library We believe that as theOpenTelemetry standards matureacross all threesignal types  itwill provide hugebenefits for developersto start instrumentingtheir applicationsfrom the very beginning rather than wait for incidentsto happen and then havethe operations team definewhat needs to be instrumentedin the application Secondly  to provideout of the box supportfor OpenTelemetry  we recentlyreleased the unified ops agent The unified opsagent makes it easierto install  configure  andupgrade the collection agent This unified agent combinesthe monitoring and loggingagent into oneinstallable component and provides a single YAML basedconfiguration language We re migrating ourthird party applicationstack monitoringplugins to becomeOpenTelemetry based receivers And those should be availablethrough the rest of this year Now  Prometheus is a verypopular open source monitoringservice in Kubernetesenvironments Now  in the spirit of opennessand support for open source we support and integratewith Prometheus in two ways First  we have integrationswith the Prometheus engineto make it easier for usersto ingest Kubernetes metricsand workload metricsfrom Prometheus to CloudMonitoring and leverage therich capabilities in CloudMonitoring That includes alerting dashboarding  and metric querylanguage Second  we recently announcedthe Google managed Prometheusservice for users who wantto continue using Prometheus but do not want to standup the infrastructurebehind Prometheus ontheir own  and thenmanage the Prometheus service Now  for GKE environmentsspecifically  wehave made signalcollection even easier One of the keybenefits of Cloud Opsis that users donot have to deployany agents forcollecting metricsand logs in GKE environments Not only does this reducethe toil of deploying agents this is very critical inKubernetes environments because the containersare ephemeral And in some cases the containersmay have very short lifespans as they are automaticallyscaled up and downbased on policies This automatic collectionincludes key metricsfor a variety of GKE objects including nodes  pods and clusters In addition to thesystem logs  applicationlogs that are writtenwith standard outand standard errorare automaticallycollected as well Now  more importantly the preconfigured agentautomaticallycaptures metadata andthe hierarchical relationshipsamongst all of the Kubernetesobjects  from clusters tonodes to services  all the wayto pods and containers As we will see later these relationshipscome in very handywhen users areexploring observability datafor diagnosing problems Let us now briefly lookat what capabilitieswe provide for signalanalysis and visualization As part of the Cloud Ops Suite we offer an out of the box GKEdashboard The dashboard provides a veryrich and powerful experiencethat was purpose built for GKE The dashboard aggregatesall metrics  logs service level objectives Kubernetes events  and alertsinto one place for allof your GKE clusters Entities and their data presented in this dashboard are related to each other usingthe context graph that I talkedabout on the previous slide When the context graphis not visible in the UI when you start filteringdata in this dashboard the presence of the contextgraph becomes evident The filtering is not just atext or label based filtering but the filteringin this dashboardhappens based on therelationships in the contextgraph So for example  when usersfilter for a specific cluster only objects thatbelong to that clusterwill be shown in the dashboard Each section of thedashboard  from namespaceto node to service  willbe filtered based onwhether or not they belongto the specific cluster Similarly  when a user filtersbased on the name of a pod again  all the objects thatare not related to this podwill be filtered out Only the cluster  thenamespace  the node and the service thatthis pod belongs towill be shown in the dashboard Now  this is extremely powerful And a lot of workhas actually goneinto scaling thiskind of filteringand presenting all of thisdata in an aggregated manner Cloud Operations offerspurpose built explorersfor each of the threedifferent types of signals Namely  metrics logs  and traces So for metrics  we offerthe Metrics Explorerfor analyzing time seriesdata for creating chartsas well as for addingthese charts to a dashboardFor advanced metric analysis we offer the metric querylanguage  which provides apowerful query based mechanismto manipulate thistime series data Using these queries  userscan compute ratios of metrics Users can performtime shift analysis Users can analyzearbitrary percentiles And users can evaluate arbitrarymathematical expressionson this time series data Similarly the Logs Explorerwas purpose built for analyzinglogs The Logs Explorer providesa histogram view thathelps users analyze log trends Writing log analysisqueries can be tedious So we ve made it a lot easier tocreate  save  and share queriesfor analyzing these logs For richer analysisof log data  userscan extract and analyzefields from log data as well as definestreaming alertson individual log events Logs can also beconverted into metrics And then  users can leverageall of the capabilitiesavailable in Cloud Monitoring which includes visualization learning  dashboarding or defining SLOson these logs based metrics Lastly  the Trace Explorerprovides an interfacethat makes it easy to visualizeall traces and associatedlatencies  filtertraces of interestbased on different criteria and define alerts on trace dataso that users can dobehavioral comparison of howthe latency of a specifictrace compares over time While GKE Dashboard provides acurated and tailored experiencefor GKE based apps many customersstill need to buildcustom dashboards We did quite an extensiveamount of UEX researchover the past yearto understand whatusers need when they re actuallycreating these dashboards Based on that research  wehave created a pretty intuitiveexperience for creating andediting custom dashboards There s a varietyof new chart types dynamically re sizablecharts  easy wayto drag and dropediting on a dashboard as well as use the full powerof the metrics query languagewithin the dashboard itselfto explore the data before youadd it to a dashboard Now  one of theinteresting charttypes we ve added recentlyis the Alert chart type When a user adds thischart to the dashboard they can also associate analert policy with that chart And users can then seethe alert thresholdfrom that policy on this chart Finally  to automate thecreation of dashboardswe have populateda GitHub repositorywith over 60dashboard templates and a Terraform module toautomate the deploymentof these templates In addition  we recently addeda sample dashboard capabilitydirectly into theproduct UI thatmakes it easy to ingest andadapt these sample dashboardsfrom the GitHub repository Users can get a preview ofeach of these dashboardswithout any effort before theystart adapting and adoptingthese sample dashboards Let us finally look athow development teams canbecome elite performing teams And the most important thingwe will discuss in this sectionis  how the two teams cancollaborate amongst each other Well  the collaboration betweendevelopment and ops teamscan be improved bydefining and measuringservice level objectives That is easier said than done To improve thatcollaboration  werecommend that bothdevelopers and operatorshave a seat at the design tableearly on in the software designprocess so they can agreeto a common language agree on a best way tomeasure reliability and create reliability as ashared objective for the twoteams Now  once they have agreed tothe service level indicatorsand what service levelobjectivesthey want to achieve  they canthen use these capabilitieswithin the Service Monitoringfeature in Cloud Operations Cloud Monitoring   orService Monitoring  will automaticallyand continuouslykeep track of howthe teams are doingwith respect to the agreed SLO The error budgetis automaticallycomputed and tracked as well As long as the error budgetfor a given time periodhas not beenexhausted  developerscan move with fullvelocity to continuepushing out new capabilities However  as the error budgetgets close to exhaustion the mechanism becomesself regulating New features are sloweddown  and both developersand operator shifts towardsimproving the servicereliability Now  as organizationsimprove the collaborationbetween developers andoperators around SLOs they also need to adoptsome best practicesthat we have shared These best practicesinclude A  acceptingthat failure is normal andnot striving towards 100  SLO B  measuring everything including toil and reliability and using automation andobservability tools to reducetoil so developers andoperators can contributetowards feature velocity Lastly  when anincident does happen we recommend thatthe two teams strivetowards blamelesspostmortems and drive servicereliability improvements Let us look at howcustomers can use servicemonitoring toadopt and implementsome of these SRE principles The service monitoringcapabilitiesenables users to selecta service level indicatorand define theservice level objectives Now  once the SLO isdefined  Cloud Monitoringwill automatically computeand track error budgets Users can also define alertson these error budgets So as you reach a criticalthreshold on your error budget alerts will be generated And users will be notified aboutthe exhaustion of the errorbudget For GKE services  thereis further out of the boxintegration that makes it easierto define the service levelobjectives Cloud Ops willautomatically discoverservices that are using Istioand ASM deployed on GKE So when a user defines anSLO for these auto discoveredservices  if they choose anavailability or latency metric Cloud Ops will automaticallypick or recommend the metricto be used as aservice level indicator Now  since some customerschoose Prometheus to monitorKubernetes services  and specifically  theirapps running in Kubernetes  we also provide out of the boxintegration for such customersto ingest metrics fromPrometheus and be ableto define SLIs and SLOson Prometheus metrics Of course  users candefine custom servicesor choose any other custommetric to use as an SLI and then define anSLO on that metric So let me summarize some of thekey highlights from this talk First  what we have seen fromthe analysis in the Dora reportand from the customerexample I talked about Elite teams can achievehigh feature velocitywithout sacrificingservice reliability Two  from a tools pointof view  Cloud Operationsoffers integrated capabilitiesthat brings together the threedifferent signalsto make it easierto improve MTGR andenable developers and SREusers to focus onincreasing code velocity Three  elite teamsuse SLIs and SLOsas a mechanism to formalizereliability targetsbetween developersand operators And customers can usethese capabilitieswithin Cloud Operations If you re interested in moresessions around this topicor interested in learning aboutour Cloud Operations Suite there are three additionalsessions listed on this slide And I welcome youto go watch those Please enjoy thosesessions and other sessionsin the Next 2021 And thank you again forattending this session KELSEY KRIPPAEHNE  Hey Welcome In this session we re going to betalking about applyingKubernetes appdevelopment and best practices We re going to talk aboutwhat are the best practices and then we re going to give ademo on how we use Google CloudDevTools to implement them First  we d like tointroduce ourselves I m Kelsey Krippaehne I m a developer relationsengineer at Google ABBY CAREY  And I m Abby Carey developer advocate for GoogleCloud s developer experiences KELSEY KRIPPAEHNE  So aswith any type of development following best practiceswhen working with Kubernetescan save you time andunnecessary frustration You may have to write lotsof YAML configuration files So unless you re aYAML expert  you llwant to have authoringsupport so that you don thave to constantly look upwhat property takes what fieldsor memorize the basiccomponents of a service For local Kubernetesdevelopment you don t need to replicateyour production cluster But it s important to havea cluster for local devthat s configured ina way that ensuresthat what you rebuilding will becompatible with theproduction cluster Using commands likekubectl logs to inspectpods and services worksOK  but trying to parselogs this way can be a pain especially for beginners A tailored logging interfaceis a more efficient wayto get the infoyou re looking for Debugging Kubernetes withCLI commands is cumbersome Once again  using atool that s tailoredfor Kubernetesdebugging can save youfrom a lot of frustration Excessive time tobuild and deploy  it s always a best practice toimprove development velocityand not get stuck ina lengthy build cycleon every changeto the code base And finally  it sa best practiceto share best practices If your team has specificpolicies for developingKubernetes apps those patterns shouldbe accessible asproject templatesthat your team can reuseto ensure conformityand to make it easy fornew devs to onboard So we re going to startimplementing these bestpractices withGoogle s Cloud Code Cloud Code is an IDE pluginthat automates dev workflowsfor cloud native applications You can see Cloud Codeas sort of a bridgebetween you developing inyour IDE and Google Cloud It s bringing the cloud to you Cloud Code isparticularly helpfulwhen developing withKubernetes because itcomes pre installed withtools like Skaffold  minikube and the Cloud SDK Skaffold is a commandline tool thathandles the workflowfor building  pushing and deploying your application Cloud Code uses Skaffoldbehind the scenes so it acts as sort of a userinterface for the Skaffold CLI minikube sets up a localKubernetes cluster It s small but powerful andextremely beginner friendly Cloud Code is available forVS Code and JetBrains IDEs like IntelliJ You can also try out CloudCode on Google s web based IDE the Cloud Shell Editor All of these tools  Cloud Code  Cloud Shell Skaffold  and minikube  are 100  free to use And we encourage you  if youwant to try it out right now go ahead and installCloud Code on your IDEor try it out inyour web browserwith this link to aninteractive tutorial So we ve talkedabout best practicesand how it can be difficultto implement them The question now is  how dothese Google Cloud DevToolsmake it easy to incorporate bestpractices into your Kubernetesdevelopment workflow First  you don t need tobe a YAML expert in orderto write functionalKubernetes resource manifests Tools like Cloud Codethat provide YAML snippetsand autocompletes makeit so that you can authorentire Kubernetesmanifests without lookingat YAML documentation You should be developingon a cluster thatreplicates your productioncluster as much aspossible to avoid issueswhen pushing to production You can configurea minikube clusterto quickly develop and testlocally with no added cost You can also use Skaffoldto create distinct profilesfor dev versus prodand incorporatecustomized  INAUDIBLE  foreven more powerful deploymentworkflows As mentioned before it s a good ideato use a dedicated loggingtool that makes Kuberneteslogs more accessible With Cloud Code snew logging support you can drill down into yourcontainers and container buildsand your deployment logs And you ll see that ina minute in the demo For debugging  sometimesprints and log statementsjust aren t enough It s useful to be ableto step through the code Cloud Code brings Kubernetesdebugging into your IDE It lets you dostep by step debugging livein your Kubernetes cluster locally and remotely using the IDE s debugginginterface that you re alreadyfamiliar with Having an automatedlocal dev loopis a general bestpractice  and youcan setup Kubernetes iterativedevelopment using Skaffold Skaffold has a filesync functionalitythat prevents you from needingto rebuild and redeployyour containers onevery little change And finally  you can use atool like Cloud Code s customsamples to createproject templatesor samples to easily distributeyour team s best practices With custom samples  you canhave a reusable Kubernetesconfigured app that yourteam can access rightfrom their IDE And now I ll hand thingsover to Abby for a demo ABBY CAREY  Thanks  Kelsey Let s get into IntelliJand see some of these bestpractices in action To kick off thisdemo  I ll show howyou can Kubernetifya non Kubernetesapplication with Skaffold I already have Cloud Codeinstalled from the JetBrainsmarketplace  whichmeans Skaffoldhas been installed  too  sinceit s a managed dependency I ll use Skaffold s init  generate manifests commandto get this app workingwith Kubernetes in seconds And I want to mention thatthis functionality will soonbe added to Cloud Code s UI You can view CloudCode as a frontendfor Skaffold  which isa command line tool And now Skaffold s going toprompt me to pick my builder And for my frontend  I willbe building with Buildpacks And for my backend  I ll usethe existing Dockerfile here Skaffold looks at myapplication and determineswhat the Kubernetesresources are and then it creates adeployment yaml filefor each one And now that that sfinished  I havemanifests generatedfor my application and a skaffold yamlfile that specifiesit s a build and deploymentpipeline We ll take a closer lookat skaffold yaml later This application isready for Kubernetes but these manifests that weregenerated are starting points You will likely wantto customize them Your organization may havecertain naming patternsor file system layouts In this case  thisapplication needssome environmentvariables and labelsfor the frontend andbackend containers and an additionalKubernetes resourceto deploy a MongoDBservice for our database I ve added a versionof this applicationwith these changes made to aCustom Samples Git repository Cloud Code s CustomSamples featureallows your organizationto easily distributebest practices by lettingdevelopers connecttheir IDEs to custom samplerepos  which I ve alreadydone here A team best practice is tohave starter projects thatare setup to fit your team spreferred configurations When talking in thecontext of K8s development this would includerun configurationsand certain YAML files And my new ready to gosample is called Nextbook so I ll create it The Nextbook applicationis a basic Kubernetes appwith a frontend andbackend service We ve kept this demo appsimple to better demonstratecomplex Kubernetes dev flows It includes logic to readfrom and write to a database and displays the mostrecently added database entry Let s take a look at thecontents of this project This project isconfigured to useSkaffold for both developmentand production workflows That s all setup withthis skaffold yaml file I ve also set uprunConfigurations in my  ideafolder By committing these filesto your project s repo this enables any developerto run and developyour application the same way This helps with onboardingin a team environment skaffold yaml tiesthe container imageswe re going to buildto the K8s resourceswe re going to deploy Skaffold supports a numberof container image builders like Docker  Jib  Ko  and more I ve already setupmy Skaffold fileto build my backend with aDockerfile and my frontendimage with buildpacks Buildpacks are a set ofopinionated image buildersthat encode language specificbest practicesfor containerizing applications They save you from havingto write custom Dockerfiles Skaffold profiles are verypowerful for separatingconfigurations for differentuses or environments including build  test  anddeployment configurations I have two already set fordevelopment and production which is a best practice My dev profile appliesdevelopment settings which shouldn t beenabled in production One of our new products Google Cloud Deploy leverages Skaffold profiles togather configuration detailsfor each target environment I ll dig deeper intothe Skaffold fileas I go through this demo Cloud Code automaticallyinstalls necessary tools like Skaffold  sowe re ready to startworking on this application Cloud Code comes witha Kubernetes Explorer This gives you a view of allyour deployed K8s resourcesand is an excellenttool for operators Looking closer at oneof my existing clustersI ve deployed to  I canexplore my deployed resources and even shell into a runningpod to run some commands Cloud Code supports anyconformant Kubernetes cluster whether that s a localcluster  like minikube or k3d or hosted by a cloud provider Today  I ll be deployingthis applicationto my GKE dev cluster For this sample app my run configurationsare already definedso that any developercan run this application onIntelliJ the same way I would I have two configurations  development and production It s a best practiceto tailor workflowsfor different contexts Let s take a look at mydevelopment configuration This is a Cloud CodeKubernetes configurationthat I ve built on My container registerURL is already set and this is needed because myskaffold yaml doesn t specifyan image repository  allowingdevelopers to easily deployto multiple locations An example of this is if youhave a personal dev repositoryand a production repository My Run configuration is setto run on my GKE dev cluster The option to automaticallyrebuild and rerunturns on Skaffold s watch modefor iterative development In the Build Deploysection  we alreadyhave a Skaffold file  sonothing needs to be done here As a side note  though  if youdon t have a Skaffold file this is the spot where you dbe prompted to initialize onewith just a couple clicks One last thing I want totouch on before runningis that I have a Deploymentprofile set named dev dev is one of mySkaffold profilesthat I already definedin skaffold yaml Looking at myskaffold yaml file again my dev profile specifiesa Dockerfile target  dev for my backend Dockerfile Now taking a look atmy backend Dockerfile you ll see thatit s multi staged With my dev target set running my configurationwill build the base image along with the dev portion here turning on my Flaskbackend s debug mode And if the targetwere set to prod which is for my productionrun configuration my Flask debugger would beturned off in production mode Now I ll focus on my dev clusterin the Kubernetes Explorerand run this with mydevelopment configuration Right now  Cloud Codeis building and pushingmy container imagesto my cluster Now that that s done Cloud Code port forwardsmy cluster s servicesto my local machine so all I have to do is clickthe link for my frontend You can also explicitlyset up port forwardingfor other resources  likedeployments or stateful setsin skaffold yaml Now let s take a lookat this application And this is myapplication runningremotely on my GKE cluster Everything islooking as expected so I ll enter a newentry to the database And yep  my output box isdisplaying the latest entry Deploying an app to a clustergenerates a ton of logs Loading container images deployments  spinning up pods and the logs streaming from yourcontainers all generate logs Figuring out the best wayto view all these logscan require runningmultiple kubectl commands Cloud Code splits the logsup into easily selectablesections This makes finding where errorsoccurred in your deploymentprocess easy Logs are also streamedfor each container letting you focus on differentcomponents of your application So right now we re stillrunning our application and Skaffold s watch mode iswaiting for us to make changes Now let s look at the YAMLof our deployed applicationand see if we canmake it better followKubernetes best practices We don t have a readinessprobe  and you should alwayshave one so that Kubernetesknows when your app isready to serve traffic This gets us astep closer towardszero downtime deploymentsand higher availability We re going to use CloudCode s YAML authoringsupport to createthis probe real quick I ll go down to my containerand press Control Space And there s the optionfor a readinessProbe Now to add some parameters I ll press Control Spaceagain and pick tcpSocket My port is 8080 What other parametersdo I need  though Hovering over readinessProbeshows me documentation and I see that I havea couple more options Now I ll addinitialDelaySeconds  set to 5 and periodSeconds  set to 5 That s looking good Now I ll save this file Skaffold s watchmode just kicked offand we re redeploying And now it s done Our readiness probe is ready This whole rebuildingand redeploying processis pretty fast  butwe can make it faster and that is with file sync By setting up Skaffold sfile sync feature you can copy over a localfile that you ve justedited with one in yourdeployed container in seconds This avoids the need torebuild  redeploy  and restartyour correspondingpod on a file change saving you localdevelopment time To get file sync workingwith my backend s Dockerfile a little extraconfiguration is needed But to save a little time I ve already done that I m using inferred syncing whenusing my dev Skaffold profile This means that Skaffoldwill look at my Dockerfileto determine the location ofwhere my edited files shouldbe synced to All you need to do is tellSkaffold what files to watch I ve added   py so that anyPython file in my backend thatI edit will trigger a sync My frontend automaticallyhas file syncturned on  because it sbeing built with buildpacks This autosync feature isspecific to buildpacks There are file type limitationswhen using buildpacks but JavaScript is supported so my frontend syncingis good to go File sync may requireconfiguring your web frameworkto detect and respondto file changes I m using Flask s developmentmode to restart the appon a  py change For compiled languagesor frameworksthat don t providethis support  you llneed to use anexternal file watcher like Nodemon or Watchexec If you re buildingwith buildpacks Watchexec isautomatically configured Now let s see if we cansync a backend Python file To visually test this I ll hard code a new entryinto the output text boxthat says  This is syncing Now I ll open my Runwindow back up and save it My logs are showingthat a sync justfinished  so let s check out myKubernetes app now and give ita refresh And there is that dataI just set in there With Skaffold s filesyncing  locally iteratingon a deployed Kubernetesapplication is super fast Now let s talk about debugging There are a ton of ways todebug a Kubernetes application But what if debugging itwas as simple as debuggingany application in your IDE You should run anddebug your applicationsin environments thatclosely mimic production so why not remotelyon a GKE cluster I could also switch tousing a local minikubecluster to cut down on latency But since we re alreadytalking about GKE clusters I ll stick with that So now I m goingto try and debugmy frontend s app js file And all I should need todo is place a breakpoint I ll set it to triggerafter I post a message And now I ll kick off debuggingwith Cloud Code s debug mode And once I ve set my pathsfor my frontend and backendcontainers  I can nowopen up my applicationand post a message to see ifwe can trigger this breakpoint And there we go I m now remotely debugging myapplication on my GKE clusterright from my IDE I ll just step throughthe code a couple timesand then finish running it Super simple It s a good practice to addtests for your container image and Skaffold supportsrunning tests For the last partof this demo  I mgoing to run containerstructure tests with Skaffold Container structure testsvalidate the structureof your container image can check command outputs and verify filecontent and metadata Here we have test files forthe backend and frontendcontainers These files are written inYAML  but you can also use JSON In the backend  I m checkingthat Python is installedand that the requirementsfile exists in the container In the frontend I m testing to makesure I don t have anycomments that include xxx leftin my code  because Idefinitely don t wantto push those into production So my tests are written Now let s go overto skaffold yaml In my prod profile I ve configured Skaffoldto run the container tests Now after Skaffoldbuilds the image it ll run the tests beforedeploying to the cluster I ll switch to myproduction run configurationand see if these tests pass Looks like one of mytests didn t pass Let s take a closer look So my backendcontainer test passed but it looks like my frontendcontainer has a comment that scausing this to fail I ll go fix that right now I ll delete this commenthere and give it a runone more time And everything passed this time This application is nowready for production thanks to GoogleCloud s dev tools like Cloud Code and Skaffold Thanks for followingalong with us today And if you want to checkout the demo we used you can find it on GitHub To try these tools out yourselfand maybe with your team install Cloud Code fromyour IDE s marketplace And finally  we encourageyou to join the conversationat Cloud Code s Slack channel And on behalf of Kelseyand I  thank you again and enjoy the rest of Next  MUSIC PLAYING MARCO GENOVESE  Hi  everyone I m Marco TANISHA RAI  And I m Tanisha MARCO GENOVESE  In theSecurity Spotlight we heard about the importanceof a zero trust approachto security TANISHA RAI  Andin our demo today we will show you how toimplement zero trust accessto be more secureand more productive And a quick shoutout to our audience This is truly live Dive into the chat We want to hear from you Tell us what you do andwhere you re joining from We can see you Check it out  Marco MARCO GENOVESE  Sure enough we can see everybody down here And we re so excited to interactand talk to you all today If you re still hanging outin the Next Event website click on the bluebutton that saysJoin the Interactive Experienceto activate these features OK So let s get into it We re going tocover three things One   how your entire workforcecan securely access legacyand modern applicationswithout exposing your networkto attacks  and two  how you can getbetter protectionagainst threats and data lossright from within your browser and three  how you can gain visibilityinto unsafe activity even if users aren t on acorporate network or device And we re goingto do all of thiswith BeyondCorp Enterprise BeyondCorp Enterprise or BCE as we call it is Google Cloud szero trust access solution With BCE  you can use Chrometo access apps and resources In the background Google s networkprotects and proxies traffic enforcing access policies These policies use factorssuch as identity  device info location  andthird party signalsto authorize accessto apps and datathat you need to do your job TANISHA RAI  That sawesome  Marco So let s get to the good stuff Think about it How easy is it foryour developersto work remotely andsecurely right now We know it s a tough task We know that makinglife easy for developerswhile also protectingyour code is critical That s why our new BeyondCorpEnterprise feature  ClientConnector  is so important It enables zero trust accessto legacy thick clientapplications MARCO GENOVESE Yeah  I think a lotof people run into that today So let s take a look at howdevelopers would use it We set up this laptopfor the Cymbal Group TANISHA RAI  By the way they re a fictional company Cymbal has been aroundsince the 1970s and they haven t modernizedall their applications Sound familiar One of their maindeveloper applicationsis still hosted in aprivate data center MARCO GENOVESE  Yeah I see this quite a bit Until recently Cymbal developersdidn t have a way to accessthis application remotelywithout a VPN The security teamhad constant concernsthat remote users couldexpose the network  or evenworse  their source code to attacks or hackers But now  using BeyondCorpEnterprise Client Connector they can access clientserver apps without a VPN Let me show you howCymbal developers wouldconnect via SSH totheir Git repository And before I show you  letme just say  don t blink or you might miss it It s that simple So let s go ahead and swing onover to our developer machine From here  I can simplyopen up Google Chrome Once Google Chromehas actually loaded you can see I have authenticatedto my Git ApplicationRepository already Up in the right handcorner  I simplyclick on my EndpointVerification Extension And from here  I clickon Start Connection My endpoint is beingpostured in the background and a secure connectionis being made to GitLab From there  I can simplythen   once it s turned green of course  minimize my browser And then on theleft hand side  I mgoing to go ahead and openup my terminal application This is my thickclient application I m going to then goahead and run a Git clone and that s actually going topull down the code from Gitto my local machine And there s my applicationpulled down locallyonto my endpoint I know that probably seemedpretty straightforward But what you might nothave noticed was first there were no clients It s all in the browser youprobably already had open Second  in the background continuously validatingthe identity  and the device and a bunch of other factorsis our Identity Aware Proxy And third  our TCP proxyseamlessly and securelyforwards all the traffic The fact that developersaround the worldcan do this remotely simply  and securelywithout all the hassle ofa VPN is pretty awesome TANISHA RAI  And withBeyondCorp Enterprise workers are only allowedaccess to applicationsthey re permitted to use So we prevent unwanted lateralmovement across the network So audience  we wantto know  would youuse this method for remoteaccess in your organization MARCO GENOVESE  Awesome Now let s see how yourworkforce can securelyaccess modern webapplications evenfrom non corporate devices Let s look at how Cymbal doesit for their extended workforce their call center contractors TANISHA RAI  Meet Rhonda She s one of thousandsof remote contractorswho help withCymbal s 24 7 supportduring the holidayshopping season So  security community can you share in our chat  in the past year have you onboardednew temporary workers Did you send them a laptopor make them install softwareso they could be more secure MARCO GENOVESE  Lookslike that would be a yes Contractors also tend to bemore vulnerable to attacks Last year  44  of organizationsexperienced a breach and 74  of thosebreaches were the resultof giving too much privilegedaccess to third parties So for Cymbal  keeping thoseusers off the corporate networkdecreases the riskof being exposedto attacks like ransomware And here s where usingBeyondCorp Enterprisecomes through again for Cymbal Contractors and otheremployees are able to BYOD onboard quickly and use a devicethat they feel comfortable withwhile BCE layers of protectionkeep them secure TANISHA RAI  Thatway  Rhonda canhave secure accessfrom her own devicewith clear separation betweenwork and personal activity Let me show you exactly whatshe sees as she begins her day She first navigates toher Google Chrome browser then she navigates to herCymbal Call Center application We ll enter in our credentials We ll then do two factorauthenticationwith our Titan Security Key Now that we ve passedtwo factor authentication we are in our CymbalCall Center application You saw Rhonda log into Chromewith her Cymbal credentials This is what we call aBeyondCorp Protected Profile It extends threatand data protectionas soon as she logs in As you can see  ouragentless approachmeans no additional software She gets right towork on her own devicerather than pickingup a laptop from ITand waiting hours whileeverything gets configured therefore saving time and money MARCO GENOVESE Awesome demo  Tanisha And the thing aboutzero trust accessis that we don t automaticallytrust Rhonda just because shehas login credentials Authorization is continuous not just when she first logs in BeyondCorp Enterprise enforcesCymbal s contractor accesspolicies based on heridentity and contextualinformation about herdevice and locationas well as the fact that she sauthenticated with her Titankey So security community we want to know  what do you all use formultifactor authentication SMS code  security keys  maybean authentication app  or maybesomething else completely Hopefully it s not nothing TANISHA RAI  For me  it smy security key every day especially since I don talways have my phone on me MARCO GENOVESE  Yeah  same here And they re so easy to use and they provide the strongestprotection against phishing Regardless of whatyou use  some formof multifactor authenticationis critical for basic securityhygiene And we highly recommend it especially for remote access So let s take a look I don t know ifanybody has anything Looks like mostpeople use  let s say security keys  B  Awesome That s what we love to hear So let s get back to Rhonda  whojust authenticated to the  appand explore how we can integratethreat and data protectionright from within her browser TANISHA RAI  Great Let s see her getto work and showhow BCE protects Cymbal their customers  and Rhondawith ease It s been a really busy day She doesn t haveenough time to finishprocessing a batch of customerrefunds before her next call She wants to save the customercredit card informationto a local file and do therefunds when things slow down but saving the sensitiveinformation to another locationis against Cymbal ssecurity policy Let me show you her experience Let s navigate toour credit card file and let s downloadthis so Rhonda cando the refunds at another time As you can see  she s blocked You can see a message appearat the bottom that prevents herfrom doing so MARCO GENOVESE  Yeah  exactly You can actually seewhere the credit cardPDF has sensitive dangerouscontent inside of it And I ll show youa policy that sbeen configured for Cymbal thatwill detect risky behaviorsjust like these So let s go ahead and lookat our Administrative Consolehere at admin google com Over on the left hand side you can see the Security menu And if you scroll downunderneath Security you can actuallysee Data Protection And we re going to go aheadand authenticate  obviously as administrator securely So let s go and typein our password And once I veauthenticated  I llbe able to go ahead and lookat my data protection policies You ll notice hereit says Manage Rule so we re going to go aheadand drill into Manage Rules And we can actually see nowthe credit card detectionpolicy that Rhonda ishitting right here We ve also got detectors forthings like Social Securityor even detecting code that sbeing copied  or pasted or shared by the developers So as the admin  youcan decide if youwant to automatically blockuser actions  as Cymbalas done here  or if you want totrigger a warning to the userinstead TANISHA RAI  And in additionto things like credit cardnumbers  you can alsoset specific DLP policiesto detect file typeas source code So that way  you can protectsensitive information and codeby monitoring  controlling or even blockingwhat people upload or download MARCO GENOVESE  Yeah  exactly We can use verygranular policiesfor this type ofinformation in orderto protect against exfiltration So we just showed you some ofthe types of data protectionpolicies you can set up You can also customizeaccess policies and the changes takeeffect in real time This is a really big dealbecause you get continuouschecking of whether a useris in or out of policy giving you up to the secondsecurity controls In fact  let s make an updatehere in real time for Rhonda So OK  community We re going to ask you whichpolicy we should change Is it her location  maybeher operating system policy or whether or not she s ona corporate owned device Please chime in here We d really love tounderstand and hearwhat you guys deem important While we wait for theresults to come in I will say that I seethese kinds of thingsall the time with my customers They want to beable to dynamicallychange policies dependingon their circumstances In particular some customers arereally interested incontrolling for location perhaps limiting accessto only certain countries Additionally  if a companyis going through an orgchange or maybe a merger  beingable to change access policiesfor select groups of usersor departments in real timeis crucial And beyond these threechoices in the poll there are many other waysto customize your accesspolicies based on whatyour organization needs So let s go ahead andlook back at the polls And it looks likecorporate owned device So awesome I love it And thanks foreverybody contributing Let s go ahead anddo this right now So we re going to goahead and jump on overinto our GCP Cloud Console And from here  we can seeour Identity Aware Proxy We can actually seeour Cymbal GitLabfor Developer application We can also see ourCall Center application And again  these applicationscould be anywhere For this demo  they re in GCP We re going to go aheadand select the Call Centerapplication And down at the bottom we can see Rhonda overon the right hand side  herapplication access policy Let s go ahead and edit that And we re going to go aheadand remove the existing accesspolicy that s allowing accessfrom Europe and the US And we re going to goahead and delete that one And we re goingto add a new role We re simply going to go aheadand select Cloud IP  IP Web AppUser And then from there  we regoing to go ahead and selectthe access policywhich everybody chose And that was RequireCorporate Device  right OK I just want to make sure Iremembered that correctly Let s go ahead andsave that in place Click Save And when we savethat policy  it sactually going to be propagatedacross the world within a veryshort period of time All of the proxies acrossGoogle s global networkare immediately updated So the next time any user triesto access a protected resource they re evaluated againstthat new policy set We mentioned continuousauthorization earlier and this is a key part of that Authorization is nota one time occurrence So even if youbegin a new session that doesn t mean you llhave perpetual access TANISHA RAI  Exactly And for companies like Cymbalthat employ hourly and tempworkers  the abilityto dynamically updateaccess conditions is important For instance  they could setthese policies so workers onlyaccess applications andresources during their shifthours or working days or only allow accessfrom specific geographies They also may want torequire that deviceshave the most up to dateoperating systems and securitypatches  so this isan important conditionto manage  especially forall the contractors usingtheir own devices MARCO GENOVESE  Exactly So because Rhonda doesn t meetthat condition which I justupdated  she will no longer beable to access the Call Centerapplication Now  I think we veall faced this And it s one of themost frustrating thingsabout remote access  especiallywhen you re using a VPN You think you shouldhave access to something but for some reason it s just not working TANISHA RAI  It s the worst So let s see if ourreal time change worked So when Rhonda tries to opena new task in the Call Centerapplication  let s takea look at what she sees Let s navigate back to her home And she s denied access asa result of the change Marcojust did But once  again BCEhas Rhonda covered She can report this error usingour new feature  the BeyondCorpEnterprise PolicyTroubleshooter  whichinforms end users thatthey re blocked and tells themhow to get help quickly So as you can seehere  she wouldfollow the prompt to emailthe admin to get help Let s go ahead andemail our admin Our admin is now notifiedof us being blocked With BeyondCorp EnterprisePolicy Troubleshooter admins can quickly fix issuesthat are blocking users keeping them productive MARCO GENOVESE  Yep  you bet Let me show you whatthe Cymbal admin wouldsee on the otherside of this requestand how they couldunblock users like Rhonda So we re going to goahead and switch gears Let s go over to Gmail another Google application And it looks like we ve got anotification for credit carddetection  which is good So we know if Rhonda is takingsome interesting actionswithin her endpoint   oh and it looks like we justgot an email from Rhondabecause she s actuallybeen blocked to an application There s the link that shesent over  so let s go aheadand click on that And we ll automaticallybe logged directlyinto the Google Cloud platform So from here  I can actuallysee the different policiesand bindings that are in place So let s go ahead and selectthe Call Center application And over on theright hand side here we can actually see the grantedconditions or the denialsthemselves So let s go ahead and lookat the binding details So interestinglyhere  we can seethat Rhonda failed to meet anyof the listed access levels And sure enough  it srequiring a corporate devicewhich was not granted So I would normallygo back in a blockerby updating that policy soher access level is evaluatedlike any othercontractor  but we regoing to keep moving justin the interest of time TANISHA RAI  Sounds good That was so easy  by the way MARCO GENOVESE Yeah  super easy Now let s look at how BCEcan give us better visibilityinto unsafe useractivities  whether they reunsuccessful accessattempts like we just sawor other anomalousactivities across the appsthat BCE protects Let me pull up the SecurityDashboards in Chromeand give you all a look So we re going to jump back overinto our Google Admin Consolehere And in the same Securitymenu on the left hand side we can actuallyclick on Dashboards We ll wait for those toload out for a second So something tomake note of hereis that with Chromedata protection you re actually goingto get a whole slewof different information We can see Chromehigh risk users We can see individualDLP incidents If we want to scrolldown a little bit we could actuallysee how many usersare forgetting theirpasswords if I can figure outhow to use a trackpad here So we can see userlogin attempts We can see  for example messages that are encrypted But what we re interestedin is whether or notthose credit card numbersare making it through So let s go ahead anddrill a little bitinto one of these reports So we can see every single fileuploaded  file downloaded  webcontent upload  for example And we can actuallysee every single timethat this took place for ourSocial Security detectionas well as creditcard detection And if we were to actually drillin on Credit Card Detectionhere   and I think thisis really cool  so I mgoing to show everybodyreal quick here since we dohave another minute And that is allthe sensitive datatransfers that are takingplace  blocked  detected or otherwise So even if yourorganization isn tusing allcorporate owned devices you can still monitorsecurity eventsand investigate those alerts TANISHA RAI  That sawesome  Marco Audience  so what do you think Let us know in the chat We definitely know thisis something of interest In the last 15 minutes  you veseen how an entire workforcecan access modern and legacyapplications securely how you can improve threatand data protection and how you can get bettervisibility into risky activity even on unmanaged devices MARCO GENOVESE  Yeah Our goal  at the end of theday  with BeyondCorp Enterpriseis to make your experience moreproductive and more secure And our team looksforward to supporting youon your zero trust journey TANISHA RAI  Thanksso much for joining ustoday and participating We have a liveQ A coming up nextto answer all of your GoogleCloud Security questionsas well as any questions youmight have had from the demo so please stick around MARCO GENOVESE  Yeah  please do And thank you all for joining We ll see you all soon TANISHA RAI  Bye PRAJAKTA DAMLE Welcome  everybody and thank you forbeing here today My name is Prajakta Damle and I lead product managementfor Dataplex at Google Cloud And joining metoday is John Allen head of data andanalytics productstrategy at Deutsche Bank John has been with DeutscheBank for over 9 years building their data platforms and enabling and championingbig data and data sciencewithin the organization Welcome  John  and thankyou for your partnership as well as for being here todayto share Deutsche Bank s datajourney with us JOHN ALLEN  Well thanks for inviting me It s great to behere with you today PRAJAKTA DAMLE  Likewise So let s get started Let s take a look at theenterprise data landscape which as many ofyou know is diverseand becoming increasinglydistributed with data thatis stored in data lakes  datawarehouses  specialized datamarts  and databases  notonly on GCP but on prem as well as in other clouds Now  each one ofthese systems havetheir own way of handlingmetadata  security and governance So on one hand  while the datais becoming more distributed the number of users within anorganization that need access self service access to this datain the tools of their choiceis also growing And as a result companies often findit challenging toconsistently manage and governtheir data while makingthis data universallydiscoverable and accessibleto the data consumers Honestly  it s anoperational nightmarethat often results indata silos and impactsthe overall analytics agilityof most organizations If you look at the data  67 of the data that is producedis actually never analyzed And 68  of the companiesare unable to realizeany tangible  measurablevalue from their data What is even moresurprising is that only 16 of the data consumersthat have access to dataactually trust the datathat they have access to And this is exactlywhy we built Dataplex  an intelligent data fabric thatunifies your distributed datato help automate data managementand power analytics at scale So what does that actually mean So let s double clicka little bit Dataplex is built from ground upfor distributed data  requiringyou to not move your dataor duplicate your data At the foundation ofDataplex is its abilityto logically represent dataand unify data from a metadataperspective regardless ofwhere this data is stored We have focused on threekey areas of value  intelligent  AI drivendata management centralized securityand governance and an integratedanalytics experiencebringing together both GCPnative and open source tools All of these areasare tightly integratedin a task centricexperience  helpingyou build a flexibledata platformand making this dataaccessible to your end users while ensuring that yourpolicies and best practicescan be consistently enforced Now  this is avision for Dataplex We are starting out by unifyingthe data that our customers usefor analytics on GCPbefore extending itto other sources outside of GCP Taking a closer look at thattask centric experience we ve built an experiencethat focuses on the operationsthat you need to undertake onyour data on a daily basis starting fromcurating and managingthat data to monitoring securing  analyzing this data We have built key integrationswith existing products such as BigQuery  Dataproc Google Cloud Storage  DataFusion  Dataflow  DataCatalog  and othersto really bring togetherthat single pane of glassthat you can use to curate manage  monitor  secure and analyze your data With Dataplex  you canreally organize your datain a way that is meaningfulfor your businessaround different domains basedon how this data is producedand owned or howthis data is used instead of wherethis data is stored You can definestandardized policiesaround access control  dataquality  data classification and lifecycle management And you can leverage thebuilt in data intelligenceand metadata propagationcapabilities of Dataplexto get out of boxaccess to this datafrom a variety oftools  both GCPnative as well as open source We have a numberof customers whohave been our earlyadopters using Dataplex And these customers come fromvarious different industryverticals as wellas geographies And they re using Dataplexas a foundational componentof their data platformand seeing valuein having this unified datafabric to manage and governtheir distributed data And today  I m reallyhappy to have Johnhere with us  who is goingto talk about how he seesthe value of data in the overallstrategy at Deutsche Bank his vision for the datastrategy  and the rolethat Dataplex plays inrealizing that vision So welcome  John We are super excitedto have you here Could you tell usa little bit moreabout yourself first andthen a little bit moreabout Deutsche Bank andhow you use data today JOHN ALLEN  Sure Well  thanks again It s great to be here So I work within DeutscheBank s chief data office which is part of our Technologyand Data and InnovationDivision And our goal is really to enablethe secure supply  preparation and analysis of data acrossthe entire organization Now in my role lookingat product strategy I spend a lot of my timeworking with the business teamsand technology vendorsto ensure that wehave the right roadmaps in placeto deliver those capabilities Deutsche Bank is theleading German bank We have 84 000 employeesand operate justunder 1 900 branches  withoffices in over 50 countries The bank comprises of fourclient centric divisions  a corporate bank  a privatebank  an investment bank and an asset manager  DWS which Deutsche Bank maintainsa significant stake in Now  being a globalbank  we relyupon a very complex globaltechnology estate comprisingof tens of thousandsof machines thousands ofapplications  and hundredsof operational andanalytical data stores Our total data footprint isin excess of 100 petabytes And we use that data fora wonderfully diverse setof use cases I m trying to think of some In terms of our  some of the more interestingones  our front office teamuse data to drivereal time analysisand low latencyalgorithmic trading and allows our productmanagers to develop highlypersonalized client offerings In the middle andback office  weleverage big data processingto help manage riskand to prevent fraud And like any business  we relyupon highly accurate and timelyfinancial data to serveregulatory and financialreporting requirements PRAJAKTA DAMLE  You operatein such a global market And the complexities ofoperations that you justmentioned across differentbusinesses within the bank it s quite interesting What role does dataplay at Deutsche Bank And what has your journeywith data been like so far JOHN ALLEN  Well  I agree I mean  fundamentallywe re a data business And data is thedigital manifestationof our clients  theirtransactions  relationships and our internal processes However  we didn t always viewourselves as a data business Instead  we had a moreapplication centric mindset which saw data is more of aside effect of that applicationprocessing We locked data awayin disparate siloswith inconsistenttaxonomies and formats and a lack of clear ownershipand an over relianceon bilateralpoint to point data feeds So to address this  weestablished a long term plan our data strategy  toimprove the availabilityof high quality data organizedaround a set of consistent datamodels and golden sources to uplift skills in dataand increase understandingof data problemsacross the entire organization And we set out to enable modernanalytics  such as big dataand machine learning The transformation tooksustained investmentsover many years and wasexecuted in an environmentof ever changing regulations Just as an aside last year  therewere over 6 000pages of new rulesthat we needed to examineto understand their impacton that data landscape Today  we are a much moredata centric organization which activelymanages and improvesits data as part of thenormal day to day operations We operate anintegrated ecosystemof analytics platforms And we have thekey trusted sourcesof high quality businessdata needed to driveintelligent decision making I know you asked for successes And I think  frankly  thereare too many to mention But from my personalperspective I think the workthe private bank hasdone creating newpersonalized offeringsfor our retail clients throughthe application of machinelearning is especiallycool  as is the big dataprocessing that our risk teamsuse to allow us to be much moreefficient with our capital PRAJAKTA DAMLE  It squite interestingto hear how you havemade this transition or the bank hasmade this transitionfrom an application centricview to a data centric viewin an industry that is so highlyregulated with  as you justmentioned  changing regulationsand compliance rules So what have been your toplearnings and challengesso far JOHN ALLEN  Well  for me some ofthe most significant learningsrelate to governance  ownership and how we as an organizationthink about data  our data culture  if you will Foremost for me  data needsto be treated as a first classcitizen within the enterpriseif it is to be used and reusedeffectively It must be seen as anasset in its own right Also  finding the rightbalance between securityand stability coupled with agilityto deliver analytics andhelp customers  is difficult Maintaining distinctly separateoperational and analyticalorganizations fails todeliver analytical stabilityand efficiency What I mean there is thisis when those teams do notuse and thus value eachother s representations of whatis  in essence  the same data Metadata is also a criticalfoundation for effective datamanagement and data governance However  buildingand maintaininga unified repository ofhigh quality metadatais a significant challenge intraditional IT environments And finally  I think whilecentralization of datacan seem like an attractiveapproach to achievingconsistency and standards andthe application of policies it very often leadsto bottlenecksthat inhibit business agility PRAJAKTA DAMLE  So giventhese learnings  especiallylearnings around how tobuild trust in data  howto avoid data duplication how to put metadataat the center and foundationof your data platform could you share with us what isyour short term and long termdata strategy for the bank JOHN ALLEN  Yeah  I d love to Well  our goal is to democratizethe use of data and analyticsby making it a secureand frictionlessself service experience Only then will webe able to makedata accessible toeveryone and notjust the data expertswithin the organization We intend to dothis by investingin the automation ofcontrols and policies which along with high qualitymetadata  technical  business and contextual metadata will allow us to build a datacontrol plane thatensures that our data iseasy to find and access yet always remains securewherever it is used Now  currently we recompleting our transitionto managing data as an asset changing our IT processesto allow data to thrive outsideof those owning applicationswe traditionally had We re also convergingon common approachesto data sharing anddata re use  whichuse streamlined datagovernance processes a new unifiedrepository of metadata and automation to makeour data more accessible To help our engineeringteams  we realso developing standardreference architecturesfor common data problems such as streaming and batchanalytics  and machinelearning in GCP And finally  we reinvesting in our peoplewith training in Cloud andagile data best practices PRAJAKTA DAMLE  So speakingof cloud and your partnershipwith Google Cloud  howhas Google Cloud helpedyou realize your data vision JOHN ALLEN  Well  thepartnership with Googlehas given us a fantastic baseto build our future bankingservices on Technologies such as GCS BigQuery  Dataflow  Dataprocare much more powerfuland easy to use than ouron premise equivalents Likewise  Cloud SQL  Datastore GCE  App Engine  and Pub Subfor transactional systems However  for mewhat s really excitingis the advancements in machinelearning that Vertex AIand BigQuery ML can bring But that s not to sayit s all plain sailing There is still a lot of workto integrate and assemblethese products intoeffective business solutions And this is especially true ina risk averse  highly regulatedindustry such as ours Speaking frankly  there isstill quite a gap to close I think this is an area wherewe see the partnership beingespecially beneficial So Google understandsthis complexity And together we aredeveloping approachesto solve theseproblems in the Cloud In fact  we are alreadyseeing some exciting productdevelopments from Google A good example of thatis indeed Dataplex PRAJAKTA DAMLE Your team has beenan early adopter of Dataplex And your close partnershipis extremely valuable for usas you provide us guidanceon product direction I would love tohear more about howyou see Dataplex fit inyour overall data strategy how Dataplex is helping youwith some of the data challengesthat you mentioned early on JOHN ALLEN  Yeah  absolutely Well  we are looking touse Dataplex to give usa way of organizing our dataaround our core businessand data concepts such as global marketsdata  paymentsdata  customer data without having to copy that datafrom its originating masteringapplication projects Dataplex s lakes and zonesgive us an elegant way to groupour data assets depending upontheir type  their quality or their stage intheir data lifecycle And we re experimentingwith the designsfor operating this atan enterprise level These zones are veryimportant concept for us In fact  we implementedsomething similar in ouron premise data lake However with Dataplex  we cantake this to the next level realizing a much morefederated system employing a type of data mesharchitecture  if you will where data remains insitu  owned and governedby its originators yet still partof a unified platformmaintained by Dataplexand integrated with a setof leading analytical tools I think GCP s Data Catalog alsogives us amazing capabilities It gives us visibility of keyorganizational data assets But with Dataplex  wecan go a step furtherand extend the Data Catalogwith the automated discoveryand classification ofsemi structured andunstructured data Now ensuring our data is secureis of paramount importance And we re investing heavilyin the automation of controlsand embedding those controlsinto our infrastructure as codeand as policies However  today we still needto develop those controlsfor each data service we use We also need to find away of enforcing themthrough infrastructureas code templatesand tools such as Terraform This adds complexity With Dataplex  wehave the potentialto develop these controls oncefor the lake  zone  and assets and let Dataplex do thehard work of applying themto the differentunderlying technologies And the plans Dataplex has forextending this policy drivenapproach to includedata governance  datamanagement  and data qualitytasks is particularly exciting We can imagine a world where weassess  transform  and promoteour high quality databased upon policiesand not hand coded datapipelines  allowingus to focus more on thevalue creating tasksand less on maintainingboilerplate data integration So in summary  I think wesee BigQuery for analyticsand Dataplex for data managementas cornerstones of our visionfor enabling a secureand frictionless dataexperience in the cloud PRAJAKTA DAMLE  Well thank you  John Thank you for sharing yourinsights and perspectivewith us today JOHN ALLEN  Your welcome PRAJAKTA DAMLE And also  thank youto you and your team foryour continued partnership So as you heardJohn just outline Dataplex helps enterprisesunlock the freedom of choiceby providing themthe flexibilityto keep data where they see fitand to use the tools that theyneed to run analysiswithin their organization Dataplex provides a way toenable consistent controlsacross enterprise data landscapethat is inherently distributed And finally  withDataplex our customerscan leverage the built indata intelligenceto automate many ofthe common data tasksaround data managementand governanceacross their data estate So that s all thatwe have today If you would like tolearn more about Dataplex do check our website And thank you  everyone for your time todayand for attendingthis session on howDataplex helps enterprisesbreak down their data silos Thanks again JOHN ALLEN  Thanks LEIGHA JARETT  Hi  and welcometo our session on buildingan interactive machine learningapplication using both VertexAI and Looker I m Leigha Jarett and I m a developeradvocate here at Google Cloud And I m also joined by mycolleague Sara Robinson Before we get intothe details  let stalk about why you might wantto build an interactive datascience applicationfor your team Well  we know that businessusers of all different typesneed access to the resultsof machine learning models And we really want toeliminate that bottleneckso that they don t needto go through a data teamto get access to the results We also want to make surethat our data scienceapplication has aneasy to use and simple UIso that people of alldifferent technical levelsare able to interact withmachine learning modelsand get the data that theyneed to do their jobs Finally  we wantto provide guidanceonto how businessusers should interpretthe results of machinelearning and also take actionon them so we re making surethat machine learning ishaving a real influence onbusiness decision making Now in thispresentation  we re goingto talk about howGoogle Cloud canhelp you build these datascience applications So let s pretendthat we are workingfor a marketing departmentof an e commerce company I might decide to create twodifferent models in Vertex AI First  I m going to createa classification modelto predict whether ornot different users whosee an advertisement arelikely to make a purchase Then we re going to createa time series forecastingmodel to predict our differentrevenue so we can accuratelymeasure the liftof those campaigns Now  in order to trainour Vertex AI models we re going to supply datathat s coming from our BigQuerydata warehouse This gives us thescale and reliabilitythat BigQuery offersso we re able to storeand transform and prepare ourdata for machine learning Finally  we re going to useLooker to actually createa custom applicationthat our businessusers can interact with Using Looker sExtension Framework we can customize the UIexactly for our needs And we ll enable our usersto interact with this UIand have the results oftheir interactions sent backto Vertex AI to create newpredictions from our models Now I m going topass it off to Sarato talk more about Vertex AI SARA ROBINSON  Thanks  Leigha Let s start by looking at whatVertex AI is at a high level Vertex AI is our end to endmanaged machine learningplatform on Google Cloud It s designed to help you withevery step of your machinelearning workflow  andit s designed for peopleregardless of ML expertise So whether you have deepexperience building custommodels  or you re just gettingstarted with machine learning we have ways thatyou can use VertexAI to add to your machinelearning applications So here we can see an overviewof all the different productsthat are included inVertex AI for each phaseof a typical machinelearning workflow We re going to focusjust on a few of thesein this presentation starting with data sets showing you how tolink data in BigQueryto a data set in Vertex AI Then we ll look at howto train models in Vertexand get predictionson those models And throughoutthis presentation we ll be focused on modelstrained on tabular datato build both a classificationand a forecasting model Let s start bylooking at data  whichis usually the first part ofany machine learning workflow So in Vertex AI  you startby uploading a data set And you do this byselecting the type of datayou re working with  alongwith the prediction taskthat you ll be solving In this presentation we re goingto be working withtabular data  but we alsosupport a lot ofother data types And if your data doesn t fitinto any of these categories don t worry You can still use Vertex AI totrain your own custom models Next let s look ata decision frameworkthat I use to helpcustomers figure outwhich parts of VertexAI are right for them The first question toask is whether or notyou have your own trainingdata to use for your model If the answer is no  we havea set of pre trained APIsthat give you access tomachine learning modelsthat have already been trainedon lots and lots of datathat you can accessvia a REST API For this presentation we re goingto be focused on the rightside of this flowchart We re assuming that youdo have your own trainingdata that you want to use totrain custom machine learningmodels The next questionto ask yourselfis whether or not you re goingto be writing the model code And whatever theanswer is to this we have tools to supportyou on Vertex AI We re going to be focusedon AutoML for training but you can also use VertexAI to train your own custommodels built with anymachine learning framework Let s take a closer lookat AutoML for training AutoML is a tool that lets youtrain state of the art machinelearning models in lesstime  because you re notrequired to write any of thatmodel training code yourself All you do is uploadyour data to Vertex select the columnsthat you re goingto be using for training along with the column thathas your label or the thing thatyour model will be predicting You press a Trainbutton  and Vertex AIwill iterate over differentmodel architecturesto find the best one suitedfor your prediction task When your modelis done training you ll then get access todetailed model evaluationmetrics along withfeature attributions Once you have atrained model  thereare a couple of optionsfor how to get predictionson that model The first is you candeploy your modelto an endpoint in Vertex AI And endpoints live inthe same place in the UIwhether you train the modelusing AutoML or your own custommodel code You can also access both typesof models via the Vertex AISDK You can split trafficbetween different models and you can also customizethe machine type whereyour endpoint is deployed And if latency isless of a concernand you have a lotof test examplesthat you want toget predictions on you might want to createa batch prediction job And we ll show how todo this in the demo To create a batchprediction  youcan do this either inthe Vertex AI console or via the Vertex AI SDK And you ll specifywhere your datais that you want to getbatch predictions on and then where you wantVertex AI to write the resultsof those batch predictions In the demo  we ll be creatinga batch prediction job basedon data we have in BigQuery And without further ado let s take a look at a demo In this demo  I mgoing to be showinghow to train two differentmodels using datafrom BigQuery We ll train a classificationmodel  along with a time seriesforecasting model And we ll use AutoML totrain both of those models When the modelsare done training we ll look at someevaluation metrics and then we ll see how we cancreate batch prediction jobs sothat we can then visualizethe results of those modelpredictions in ourLooker dashboard Let s go to the demo We ll train two models inVertex AI using our retail data First  we need to import ourdata from BigQuery into Vertex There s a direct integrationbetween Vertex AI and BigQuery Here we ll create a data set This data set will be used totrain a tabular classificationmodel We want to import data fromBigQuery  so we ll select here And this is theBigQuery table wewant to use to train our model It s some data onorders  and we lluse it to predict whether or nota customer will make an order We ll enter thatBigQuery table here And now we re ready totrain a model on this data We ll train ourmodel using AutoML which means that we don tneed to worry about writingany of the model code Our target column is made order This will tell us whether or notthe customer made a purchase Here we ll select the columnswe want to use for training Here we want AutoML to maximizethe accuracy on our modelfor the less common class We re doing this because thepercentage of customers whomade an order in our data setis much smaller than those whodidn t And with that  we reready to start training We ll follow the same processto train a forecastingmodel to predict sales fordifferent item categories Here we re usingseven days of datato predict what thecategory sales willbe seven days in the future Now that we vetrained two models let s see how each oneperformed  startingwith our classification model Here we can see that 77 of the time our modelwas able to correctlypredict when a customer wouldmake a purchase We can also seefeature importance This shows us which featuressignaled our model s predictionthe most In this case  it looks likethe advertisement categorywas the most importantindicator of whether or notsomeone would make a purchase With our trained model  we cannow deploy it to an end pointto get online predictions We can also requestbatch predictionsby creating a batch predictionjob  either in the UI or programmaticallyvia the Vertex AI API The result of ourbatch production jobwas written to a BigQuery table For each row  we can seethe percentage likelihoodthat a purchase was made Here it looks like there is a71  chance that the customermade a purchase Let s also take a lookat how our forecastingmodel performed Here we can see some evaluationmetrics on our model When we kicked offour training job we specified that we d liketest predictions writtento a BigQuery table Let s take a lookat that table now Here we can see the salespredicted by our modelcompared to the actualsales on that date Now let s see how tocreate a batch predictionjob  looking at one windowof our time series data When we re formattingour data for prediction we ll pass it a BigQuerytable with salesdata for our seven daycontext window and then we ll providethe date we wantto generate a prediction on For this model  it sseven days in the future We ll leave the salesvalue for that row blank since that s whatwe want to predict We can get predictions on asmany seven day time serieswindows as we d like Here we re just showingone for demo purposes When this batchprediction job completes we ll see the resultingpredicted sales valuefor the date that we specified In our Lookerdashboard  we ll beable to visualize thepredictions resultingfrom both of these models LEIGHA JARETT  Thanks  Sara So now we understandhow to actually createmodels in Vertex AI andeven hit an end pointto get new predictions But how do we extendthese capabilitiesso that non technicalbusiness usersare able to interactwith Vertex AIand analyze the resultsof those predictions This is where Looker comes in So you may already be familiarwith Looker s architecture Looker enables youto create a datamodel  your singlesource of truthfor all of yourdifferent metrics And it sits directly ontop of your data warehouse so BigQuery in our case For the datascience application we might create a modelwhere we re actuallydefining metrics associated withthe prediction results comingfrom Vertex AI This will allow our businessusers to explore the resultsand create visualizationsor dashboards and even operationalworkflows to take actionon those results But how do weactually enable themto interact with that endpointand create different inputsthat feed into Vertex AI This is where the ExtensionFramework comes into play Looker s ExtensionFramework allowsyou to create customJavaScript to controlthe UI for a custom applicationposted in the Lookerenvironment So the great thing about theLooker Extension Frameworkis it handles all hosting authentication  authorization and gives you accessto Looker s APIs ability to embedLooker components like dashboards and looks And also  we have a set ofopen source UI componentsto really accelerateyour time to value This diagram really helpsput it into perspective If you want to go andbuild a data scienceapplication onyour own  you haveto take care of all of thatstuff outside of Looker But with the ExtensionFramework  so much of itis taken care of foryou under the hood So just to put thesetwo pieces together first  we re going to create adashboard using native Lookertechnology We ll model theprediction results We ll explore them and create a reportthat business users can use Next we ll use the ExtensionFramework to actually customizethe UI so it s purposebuilt for our models and so that users cancontrol the inputs thatare fed back into theVertex AI prediction job So let s actually see a demoof all of this in action I ll jump into my datascience applicationso we can see how a marketingperson would interact with it In Looker  I can navigate tomy data science applicationfrom the left side panel underthe Applications dropdown or if you re using anearlier version of Looker from the Browse dropdown Now I can see mycustom built applicationfor the marketing team tocreate targeted advertisementcampaigns On the left handside  marketers areable to controlinputs to the model like selecting usersfrom a certain trafficsource  country  gender  age and previous purchase history They can also specifywhat type of advertisementthey re planning to run For example  herewe re predictingif we fed an advertisementcampaign for accessoriesthrough a search channel On the right hand side we have our dashboardto actually analyze theresults of the model So here we can see thatwhen we ran the model we fed in over 65 000 usersthat had an average age of 53 a certain averageprior spending habits and we can also see theirgeographic location Scrolling down  wecan start to analyzethe results of the batchpredictions from Vertex AI So here I can see that2 6  of our user basewas expected to make a purchase We can also use thatinformation to calculateour predicted campaign costif we just targeted that 2 6  And we can slice ourlikelihood to make a purchase so that predicted measurecoming from Vertex AI by other fields inthe user information like how many ordersthey previouslymade  what theirtraffic source was and what their age group was Next  we can look at the resultsof our time series forecastingmodel So to actually see what ourpredicted revenue if we don tdo anything is forthe next seven days and see that compared toprior historical sales Now this dashboard is justlike any old Looker dashboard So as a marketer  I canexplore from here  drill in So I might want toexplore and actually sendthese users who we predictto make a purchase offto an externalapplication to target themfor that advertisement Now that we understand howa marketer would interactwith this application let s startthinking about howwe actually wentand built this application Like we said earlier  we builtthis using Looker s ExtensionFramework So what we did was created somelightweight custom JavaScriptto control the UI thatwe re seeing here On the left handside  I actuallyused Looker scomponents to createeach one of these inputs Let s see what one of thesecomponents might look like For example  theCheckboxGroup componentis used for our first input Now  I m a data person  andnot a front end developer And although I did have to learna bit of JavaScript and Reactto get everythingworking appropriately most of theapplication was builtleveraging thesecomponents and working offthe Extension Framework codetemplates that are available Now if a marketer wants tochange the inputs to the model for example only focusingon people in the USand maybe seeingwhat would happenif we ran an advertisementfor jeans  I can click Run And I m going to haveto verify my identityto make sure I have permissionto run the Vertex model And then in theExtension Framework my code is actually going tosend this information to Vertexto create a batchprediction job Now before Lookeractually talks to Vertex it uses the Looker API tocreate a BigQuery table thatcontains one row for each personin that audience we createdto predict on And you can see we re goingto have the jeans searchcampaign  which is thefields that the user enteredto actually seewhat would happenif we ran a certaintype of advertisement And then the details ofthe BigQuery table  wecan see that it s setto expire tomorrowso that we aren t storing toomuch unnecessary information And also  it s namedwith this long hash And this is basically a uniqueidentifier for the predictionthat we re running Now if we open upour extension code we can see that afterthe BigQuery table hasbeen created  it s time to makeour Vertex prediction call Here we re usingLooker s extension SDKto create a POST requestto Google Cloud platform And in the input we re going tospecify things like our modelID  the location wherethis needs to run and also our input data  so thatBigQuery table we just created and our outputBigQuery data set Since batchpredictions sometimestake a few minutes to run we send the user a messageon Google Chat whenthe results areready with a link to review Here we also include thesame hash used earlierto bring the user back tothe results of the modelso that they canuse the dashboardto investigate their results The dashboard itself is justa regular old Looker dashboardthat s embeddedinto our applicationusing Looker s embed SDK So just like previousLooker API calls we made this requires justa few lines of codeand no authentication needed We use the embed methods topass in values for the filtersbased on what the userhas entered on the left You ll also notice that we havea parameter for our table name This is the hash that was usedto identify the prediction and it s what s goingto be used to make surethat we re leveragingthe right table thatstores the results of the latestprediction results from VertexAI Now the great thingabout this entire appis that it was simple to build since Looker and Vertex do allthe hard parts  like modeland front end development deployment  and authentication And my marketers caninteract with machinelearning all on their own toget data when they need it Thanks again forjoining us today So we talked about how tobuild a complete data scienceapplication usingVertex AI and Looker Sara walked us through theVertex AI platform  includinghow to create  train and deploy models and I showed you how to explorethe results of those modelsin Looker and create yourown custom application usingLooker s Extension Framework So to get you well on yourway to creating your own datascience application  wehave listed some resources like Looker and Vertex AItraining courses and labs We also included someother Next sessionsthat might be relevant towhat we talked about today So thanks again  Sara for joining us and talkingabout Vertex AI And thank all of youfor joining us today We hope you enjoyedthis session and that you enjoythe rest of Next 2021 DAVID FEUER  Hello  all And welcome to Dev 302  Buildingand Managing GraphQL APIs I m Dave Feuer  and I mjoined by Greg Brail We re a part of the Apigee team And we re super excited topresent this to you today GREGORY BRAIL  Hey  I m Greg I m also part ofthe Apigee team And I m also excited DAVID FEUER  I guess the firstthing I wanted to get outof the way is thatsometimes  in technology we can overemphasize how newtechnologies and exciting ideasare dominant inthe conversation And so I just wanted to beclear   and I think both Gregand I share   that RESTis still the king of APIs If you look at this graph which is from Google Trends it clearly shows that RESTis still the king of APIsand still dominates interest And so while we re here totalk to you about GraphQLand some other APIstyles  it s importantthat we realize that REST stillhas a very important and veryprominent position inbuilding and designingAPIs for the future If we re going to contrastGraphQL and REST APIs I think there s threespecific areas that we dwant to talk to you about The first is that both GraphQLand REST offer a consistent wayto access differenttypes of data So while GraphQL and RESTconstruct things differentlyand construct requestsdifferently and get responsesdifferently  fundamentally  theyboth allow you to access data GraphQL offers a querylanguage that s different of course  than theREST query style And that can potentially workacross many types of back ends But fundamentally  they reboth requesting data and they re both offeringa consistent accesspattern for the data The second is efficiency While it s possible to constructREST APIs in an efficient way GraphQL is specificallybuilt to avoidover fetching orunder fetching data The query languageis super powerfuland allows you torequest just the data youneed for your client  whetherit s a mobile client  a webclient  or anotherstyle of application And the last isimplementation complexity Implementing an HTTP serverto parse URLs and returndata from a variety of sources that s a straightforward task And many people do that intheir environments today But GraphQL requires specificsoftware and a specific engineon the server side thatcan parse the queriesand break them downinto execution steps And so that s asignificant differencebetween GraphQL and REST To recap some of thebenefits of GraphQL I think data load controland granularity aresuper important The fact that GraphQLhas a single endpointand allows you to reallyexecute one requestand then have the fan outon the southbound sidemake a whole bunch ofsubsequent requestsand return that  it svery  very powerful It also means that ifone resolves or fails other queries can stillsucceed and still resolveand bring back useful data In the REST world  REST doesn tnecessarily work that way REST has multipledifferent endpoints And so each queryto each endpointhas to be treated individually GREGORY BRAIL  So couldyou maybe talk a little bitabout  as a software engineer maybe I m building an app Why would I choose GraphQL and why would I choose REST DAVID FEUER  Great great question I think there s a lotof apps   specifically apps built using the ReactJSframework   that really benefitfrom the fact thatGraphQL will onlyget the data that s in view So that forinstance  if you weretracking sportsstatistics  you onlyretrieve the sportsstatistics for the playerthat you re lookingat  or you onlyreceive the sports statisticsfor the specific tablethat you can viewin a mobile app As opposed to REST which might requireadditional APIs oradditional parametersto accommodate allthe different viewsthat might exist in an appand create individual APIsfor those different views So being able to leveragethe GraphQL querylanguage to really onlyget the data that you needfor that specific user createsa certain amount of efficiencyand a certain amount ofdeveloper safety  wherethey know they re getting thedata that they expect when theyexpect it  versus having tomake multiple calls in orderto fill out whatevertable the usermight choose to viewin that application Does that make sense GREGORY BRAIL  Yeah So with REST  I m used to usingan API gateway like Apigee where I might have a couple ofcustomized versions of my APIfor different devicesbecause I wantto have it be more efficient Are you saying  withGraphQL  I have more options I have moreflexibility in the datathat I return to my client DAVID FEUER  Absolutely Along with the controlthat gives you the abilityto request exactly what you wantand retrieve exactly the datayou want also comes a tremendousamount of responsibility right Suddenly  you have to figureout up front what data you want versus simply calling an API That being said with that controlcomes a certain amount ofdeveloper safety  as we said where calling thatone API just becomesvery simple and very easy And that querylanguage is reallywhat provides that controlin the GraphQL world GREGORY BRAIL  Soa mobile developermight really likethe fact that theycan choose exactlywhat they want send one request to the server and get it all back at once rather than having to navigate awhole forest of individual URLsin order to get thesame information DAVID FEUER  Absolutely Yeah  that s agreat point  Greg The third point is thatwhereas in the past GraphQL required a wholebunch of specific gluein order to work withdifferent programminglanguages anddifferent frameworks we re really seeing aproliferation of GraphQL clientlibraries So GraphQL really is availableas a polyglot protocol and is no longer simplyreduced to a few languagesor a few frameworks GraphQL is really fairlyopen and really portable And so we re seeing a change a shift  in adoption of GraphQLas a result of that So I think if wetake a step back we ll see that there arereally two communities of usefor APIs There are APIprograms  which aretight knit teams wherethe client and server aretight knit group Or sometimes  they rethe same personthat s developing boththe server applicationand the client application And then  there aredeveloper programs which are really aboutcreating APIs as products and exposing those APIs tothose different communitiesso that different developerscan build their own frontends of their ownapplication serversthat leverage your APIsto create different valuepropositions So in that specific instance it could be a different businessunit It could be a different company It can be a partner But you no longerhave the same scopeof control over both theserver and the client And that s a significant change But that s a significantdifference between API programsand developer programs that in API programs you really don t haveto be as strict aboutdesign of your API Because since you controlthe client and the server you can assume a certainamount of knowledge and mediate betweenthat on your own But when you have adeveloper program the clients don tknow what you know And so you have to makesure to actually productizeyour API in sucha way that makesit discoverable andaccessible and makesit easy fordevelopers to leverageand consume that APIin order to drivebusiness value not just for you but also for their application GREGORY BRAIL  So ifI m using GraphQL how do I know adeveloper is not goingto do something crazyand query a bunch of datathey aren t supposed to get DAVID FEUER  Exactly Greg  perfect setup I think that s the fundamentalchallenge  is that we don t We don t know whatclients are going to do In REST  we knowwhat APIs we expose And so we know what clientsare going to request because that s what the API is The API exposes that resource In GraphQL  when you reexposing all of your resources like a graph  clients can reallyrequest whatever they want And so we re seeing GraphQLstruggling in that area in the area of developerprograms  where it s been superpopular with API programs And I think what we re doingwith Apigee is super excitinghere  in taking all thelearnings that we vehad from productizationof APIs and bringing themover to GraphQL Greg  why don t youtalk a little bitabout API productization and howwe re bringing that to GraphQL GREGORY BRAIL  Awesome  thanks So if we look atthis chart  thistalks about a lot of thethings that an API teamneeds to thinkabout when they retrying to turn anAPI into a product taking something that s justan HTTP endpoint with some JSONand turn it intosomething that developersfrom different organizationsand different companiescan consume And a lot of thesethings are thingsthat if you ve been followingthe world of API managementor seen in theApigee things before you ve probably heard ofabout a lot of these things Timing to First Hello World  for instance  that s what thatacronym is for  is a really importantmetric for an API programif you want developersto adopt your API Having a community  havingstandards  like oAuth as part of your API  and havingthings like developer portals And this enables  really  oneof the most important thingsthat s a differencebetween an APIused by a small team and adeveloper program used by lotsof developers anddifferent organizations is the ability todo self service And when we talk aboutAPI productization a big part of itis self service How does a developer whowants to use your APIfind out about the API figure out what the API does and get credentials If they can do thatin a few minutes your API is readyfor consumptionby almost anyone in the world And that applieswhether your APIis a REST API of the style we vebeen talking about for yearsand years in the world of APImanagement or a GraphQL API Now  when we talk about this we talk about control  security observability  and governance And in the world ofREST APIs  we oftendo a lot of these things basedon the fundamental principlesof REST  like URLs and verbs We ll do things like we ll say  if you vegot credentials forthe API  you areauthorized to invoke certainURLs  but not other URLs Or perhaps you re authorizedto get all the URLs but if you want topost some of the URLs you need a differentkind of credentials All of these same things aregoing to apply to GraphQL We re going to ask a lotof these same questions So when David talked aboutmaking a GraphQL API readyfor consumption bydevelopers  the developerswill love it  becausenow I can do anything I have access toyour whole schema I can query all kinds of stuff And I can build a great app And what I ve said for yearsand years about this stuffis that it s not just that youhave to worry about an attackeror someone who s malicious You also have toworry about peoplewho just make mistakes  right Like  oh  I didn t realizethat by executing that GraphQLquery  I would download 10 000rows of data from the databaseand clog up the internet andmake my mobile device crash Now  I know that So I know I have toinclude a parameter But wouldn t it be nice ifwe could put some controlsin front of the GraphQL API so a developer can t evendo that sort ofthing by accident So what we redoing at Apigee is we re starting on ajourney of figuring outwhat we need todo to make GraphQLAPIs the same kind offirst class APIs as the RESTAPIs and the other kinds ofAPIs that Apigee traditionallysupports So in the traditionalworld of API management we have this chart where wehave API teams build APIs developers build apps for APIsto create user experiencesfor end users And that transition froman API used by an API teamto a developer program iswhat this is all about The question is  when weare working with GraphQL how does this differ from theway it looks for a REST API And the answeris  not very much All of the sameprinciples apply The developer wantsto be able to discoverthe API via self service They want to get credentials They want to use the API The API team wantsto have controlover who s allowedto use their APIand how much they reallowed to use the API Are they allowed to usedifferent parts of the APIfor different purposes Is there a quota that stops themfrom using the API too much Are there securityrules or audit rules Is there even a billing planassociated with that API We want to be able todo all of these thingsfor GraphQL APIs  which meansthat we have to not only lookat URLs and verbs  but wehave to look at the structureof the GraphQL query And we have to understand when you make a query  whatis this query trying to do And as an API team what kind of thingscan I let the developers do And you can do this by havinglots of different GraphQLendpoints But one of the advantagesof an API management systemis that it allowsthe API team to bevery sophisticated in turningthose APIs into products So if they want to launch withan API that does five thingsand sell it to customers fora certain price or for free But then  they want toadd two more things but they only wantcertain customersto be able to do those things And then  they wantto add one more thingthat only a fewcustomers can see but no one elseshould find out about You can do all of this inyour API management productwithout changingyour API at all You do this by restricting In Apigee  you do thisby creating something something called an API product And an API productbasically specifieswhich parts of the API areavailable to which kindof consumers  whatkind of quotas on them And we can attachadditional data to itthat we can use tomake other decisions We want to do those same sortsof things with GraphQL APIs So if you look at whatwe ve added to Apigee Apigee now has a way inside any Apigee proxy to parse a GraphQL request First of all  just to rejectit if it s invalid  or evendo schema validation That can be supervaluable  because itmeans that if you have malicioususers or developers whoare still learning theAPI and making mistakes Apigee can nip all ofthose invalid requestsright in the Apigeeproxy  and noteven let them get to yourback end server  which takesa load off your GraphQL engine But Apigee can nowparse that query understand what it s tryingto do  and make decisions So a very simple thing youcould do  for instance is say that certain developersor certain applicationsare only allowedto execute queriesagainst the GraphQL API Whereas you have to have adifferent kind of credentialand  possibly approval from the APIteam in order to make mutations Which means  basically we can very easilyseparate the read only APIfrom the read or write API And then  we can do more And I think what we redoing in the future is we re looking atwhat more we can do So for instance  we also now that we understandthe shape of the API  we canput a bunch of informationinto the Apigee message flow We can then use other policiesto make other decisions So it gives a lot offlexibility for youas a developer  right now  tobuild some interesting GraphQLAPI related access controland other decision makinglogic into your Apigee proxy And it s going tomake it possiblefor us to do even morewith this in the future DAVID FEUER  And Greg do you see this affectinghow enterprises areable to positionGraphQL APIs for consumption Like  is thissomething that you seehelping application developersbuild new experiences usingGraphQL APIs GREGORY BRAIL  Yeah So if you re an application  I mean  you and I havebeen around the worlda little while Once upon a time  it wasconsidered terribly bad formto expose your relationaldatabase directlyto developers Because developersmight be able to dosomething like selectstar from customers  or worse yet  deletestar from customers  and overwhelm the database  orexecute some ridiculous querythat is nested 17 levels deepand joins 12 different tablestogether and slows down theapplication for everybody So that s where three tiercomputing and transactionprocessing monitors andall these things came in Same thing with GraphQL If you just put a GraphQL APIin front of your core dataand give access to developersto get at it directly then you have to have sometrust in your developersthat they re not goingto execute queries thatcause performance problems orsecurity problems or complianceproblems So if you have something sittingin front of that GraphQL APIthat s able tolook at the queriesand make decisions about what sallowed and what s not allowed I think it makes iteasier for an enterpriseto be OK with the idea of usingGraphQL for critical data DAVID FEUER  Awesome GREGORY BRAIL  And withthat  we have a demo You re going to listen to me And what we re goingto do is  we regoing to show some of the newGraphQL features of Apigeeand what we can do with them  VIDEO PLAYBACK   I m going to start with anexample of a mobile applicationthat uses GraphQL This application isa product catalog And it makes one API call toget the product information plus an API call for every priceand every stock information So as you can see  whenI refresh the screen we have a lot ofthese blue circlesthat show API calls being made And when I click onan individual product we make three API callsto get the result However  I can alsoswitch my applicationto take advantage of GraphQL In that case  it makes oneAPI call to my back end It s querying the exactsame database tables In my case  it s evencalling the exact same gRPCs But the mobile app is sendingone request to the back end And the back end isfanning all of thatand returning one response The result is quite abit more responsive There s only one API callrequired to get all the productinformation I need When I scroll up anddown  it s nice and zippy And there s none of thatrefreshing going on So as you can see  GraphQLcan be a very effective wayto make an efficientand snappy mobile app Now that we velooked at the app let s actually see what sgoing on with the APIunder the covers Underneath  of course I have a GraphQL API And I can do somethings in this GraphQLAPI  like  for instance got a list of products And like in allgood GraphQL APIs I can merge data fromdifferent data sets I can sort I can decide how many And I can get informationabout an individual product And once again  I candecide exactly what fieldsI want to see or don t see And on this particular case this particular GraphQL queryis being securedvia an API key whichis being enforced by Apigee Using this particularAPI key  I can alsotry and get informationabout the price of an item And if I use that key  here smy query to get the price You ll see that I get an error This is coming from Apigee Because Apigee parsed the query It saw that thetop level selectionset was called  price  And this particular API key isnot allowed to make that query However  if I changethe API key to an APIkey that uses adifferent API product I now can get the price Now that we ve seenwith the API lookslike  let s see howit works in Apigee First of all  Apigee hasalways had API products They re key to API management because they let you package upan API without changing the backend into different combinationsof functionality different quota settings and even differentaspects  like security depending on who sconsuming the API We ve added GraphQLsupport to API products So now  in addition to filteringon URL paths and verbs like we couldbefore  we can alsofilter on  for instance the GraphQL operation type like whether it s aquery or mutation And we can set differentquotas for different products like we always could So for instance you could set it upso a certain API key or acertain set of applicationsthat use oAuth might beallowed only to do queries Or they might have a differentquota than another set And they mighteven be restrictedbased on the name ofthe GraphQL operation Now  the way this is enforcedis in the proxy  of course And we have the first partof functionality to do this is our new GraphQL policy First thing it does is parse thequery to make sure it s valid It can also validate theschema if you d like And then  it sets abunch of variablesthat we can usefor other things For instance  it s usedin the CheckAPIKey policyto enforce the API productlogic that I just talked about I wanted to go a littlebit further in this demoand use the name of theselection set  whichis the top of thatquery  to determinewhich API keys could query whichparts of my GraphQL schema And I did that usingsome JavaScript I set a variablein the product thatspecifies what top levelselection sets are allowed And I use a variable that isset by the GraphQL policy whichcontains that information After that  a littlebit of error checkingfollows  and some cleverbut simple JavaScriptthat simply looks to see ifthe name of that selection setis in the product That s all there is to it And now I m able to have somevery interesting controlson my GraphQL application  END PLAYBACK GREGORY BRAIL  Sowe ve just seena demo that shows how youcan use the new GraphQLfeatures of Apigeeto parse queriesand understand them  andhave additional logicto make decisions aboutthose to make your API moreready for the enterprise Over to you  Dave DAVID FEUER  Thanks  Greg That was awesome We re super excited aboutGraphQL and our abilityto bring it to our customersand prospects to build new appsand experience withthis new technology Please go tocloud google com apigee to tryit out We d love to hearwhat you think Thanks for joiningus today  and havea great rest of the summit JAMIE KINNEY  Hi  everyone I d like to welcomeyou to INF300 And thank you forjoining us todayfor this Cloud Next session exploring GCE VM families Today  we re going tostart by giving youan overview of theGCE VM families And then we re going tohave a deep dive lookingat each of these VM familiesto see how they re tailoredto meet the needs ofindividual workloads looking at ourgeneral purpose VMfamilies  compute intensive VMfamilies  memory optimized VMs and then finally  options forrunning accelerated workloadson Google Compute Engine I d like to start byintroducing myself I m Jamie McKinney I m a product manager And I am responsible forthe Tau T2D VM family I joined Google a littlebit over three yearsago initially as a cloudsolutions architect focusing on both scientificand technical computing and also working with someof Google Cloud s largestcustomers as they migrated theircloud native workloads to GCE Subra  would you liketo introduce yourself SUBRA CHANDRAMOULI  Sure Thanks  Jamie Hey  my name isSubra Chandramouli I m also a productmanager at Google Cloud covering Google ComputeEngine products My background has been primarilyin the semiconductor industry focused on processorsand FPGA acceleratorsfor data centermarkets  including  amongst others JAMIE KINNEY  Thanks  Subra OK Just to kick thingsoff  I d liketo start with anoverview of what GCE is So GCE  or GoogleCompute Engine is Google Cloud smanaged servicefor offering virtualmachines as a service With GCE  you caneasily choose a VMthat meets your needs either selectingone of our predefined shapesor custom defining your own Following our recommendations if you d like or if you have theexact specifications you can dial in that VM toprecisely meet your needs GCE also takes care of a lotof the operational overheadof running applications offering technologieslike live migration for example  whichwill allow us to easilymove your virtual machineto a server while we performmaintenance on a machine thatmight need some assistance We can also give youa number of mechanismsto reduce the cost of runningyour workloads when you runon top of Google ComputeEngine with featureslike sustained use discountsthat automatically providediscounts forlong running applications Or if you d like to geteven deeper cost savings you can look at our one yearand three year committed usediscounts Today  our focus is goingto be on GCE VM familiesand how to choose the rightfamily for the needs of a givenworkload Just to provide alittle bit of historyon Google Compute Engine  weinitially launched the serviceback in June of 2012 And at that time Google Compute Engineoffered just a singletype of virtual machine And over the past nine years on top of that single VM family we also built anumber of features things like livemigration  the abilityto attach persistentdisk  and other featuresthat make those virtual machineseven more capable and powerfuland a better fit for yourapplications and their needs In recent years  we veexpanded the numberof virtual machine families thatare offered by Google ComputeEngine to includecompute optimized memory optimized  acceleratedworkload optimized VMs and even moregeneral purpose VM families So today  we re goingto look into eachof these young families andsee how they re best applied Starting with a map tohelp you orient yourselfaround the different VMfamilies that we offer we generally divide these intotwo high level categories general purpose VMs andworkload optimized VM families On the general purpose side we have the cost optimized VMfamilies This includes E2 And these are our VMfamilies that are designedto minimize the price per vCPU We also offer abalanced VM familythat gives you access toall of GCE s features  the flexibility to attachlocal SSDs  to attachany of our persistentdisk variants access to thefastest networking and the ability to dialin the custom VM shapes For customers that arerunning scale out workloadsfor a wide rangeof applications be it web servingapplication servingor even high throughputcomputing workloads we ve introduced a new categoryof VM families designedto meet those needs  currentlyincluding the Tau T2D VMfamily that you ll behearing more from Subra On the other sideof this map  yousee the workload optimizedVM families These are VM familiesthat are specificallytailored to meet the needsof a possibly niche or a verywell defined setof applications This could include thecompute optimized VMfamily that s really well suitedfor high performance computing game servers  and othercompute intensive applications memory optimized VMs forvertically scaled applicationsthat need access tolarge amounts of memoryand balanced computeto go with it and accelerator optimizedVMs with TPUs and GPUsto meet the needsof machine learningand other workloads thatbenefit from these specificaccelerators In addition to VM families Google Compute Engine as I mentioned  overthe past several yearshas developed anumber of featuresthat provide even more value toyour virtual machine engines This includes things likecustom machine types the ability to launchsole tenant VMs  we ll talk about it morelater in the presentation  the ability to attachvarious forms of storage be it local SSD or persistentdisk storage of various speedsand capabilities  and alsoenhanced 100 gigabit networkingthat deliversfantastically low latencyand the bandwidth neededfor some of your mostdemanding workloads Now I d like tohand it off to Subrato give us a deeper dive on whatwe mean by general purpose VMfamilies and talk about theworkloads and the families thattend to go with those Subra SUBRA CHANDRAMOULI Hey  thanks  Jamie As Jamie was alluding to one of our core philosophiesat Google Cloud is to enableour customers the flexibilityto choose the rightvirtual machineto run their variousdifferent workloads General purpose workloadsmake up a whole wide varietyfrom simple web serving  tomore complex media transcodeor processing types ofworkloads  to data analytics to name a few You can also think of dev andtest as a separate workloadthat  whose requirementsare pretty unique In addition  variousenterprise applicationsmay be categorized underthe general purpose categoryas well Our general purpose N2and N2D machine typesare a good fit forsuch workloads Our general purposeN2 and N2D VMsare designed to supporta variety of workloadsby offering a perfect balanceof compute and memory resources up to 224 vCPUs with864 gigabytes of memory N2 and N2D machine types providea high degree of flexibilityfor our customers Custom machine types enable youto rightsize your resources enabling great costsaving feature More on this later They also enableyou to auto upgradeto new CPU generations whenyou launch a new CPU platform This means you can simplychoose a minimum CPU platform and Google willautomatically run your VMon the latest generation CPU And also attach up to 9terabytes of local SSDper node And you get highnetworking performance upto 100 gigabits per second These VMs are also availableglobally across GCE regionsnow The key differencebetween N2 and N2Dare summarized in the table onthe right side of the slide N2 VMs are a general purposeIntel processor based VMs supporting second generationIntel Xeon scalableprocessor  or CascadeLake  and offeringVMs that go up to 80 vCPUs The N2 VMs offerup to 20  to 30 price performanceimprovements compared to N1 N2D is our AMD EPYC basedgeneral purpose VMsleveraging second generationAMD EPYC Rome processors They come with up to224 vCPUs per nodeand offer you a lower pricepoint  with approximately 13 lower pricing comparedto our N2 machine types N2 VMs are ideal forworkloads that requirehigh per core performance You can see some benchmarks thatcompare N2 VMs to our N1 VMson the right side of the slide N2 machines are pricedat parity to N1 family enabling much higherprice performance whencompared to our N1 machines N2D machines are ideal forhigh throughput workloadsthat can benefit from havinga large number of threads N2D machines areideal for workloadsthat can benefit fromconfidential computingfeatures in addition Switching gears abit  our E2 VM familyare ideal for applicationswhere cost is a priority E2 VMs offer machinetypes up to 32 vCPUsand 128 gigabytes of memory We also offer the flexibilityof choosing custom machinetypes with our E2 family E2 family runs on Intel Haswellthrough Cascade Lake and AMDRome processors E2 machines are ideal for devand test types of workloadsor for workloads that may bebatch processed  for example This family provides up to31  cost saving when comparedto our N1 family per VM Switching gears again earlier this year in June we announced our latest VMfamily under the brand of Tau The T2D is ourfirst family of VMsthat will be offeredunder the Tau brand Currently underpreview  Tau VMs aredesigned to meet theneeds of customersowning scale outworkloads on Google Cloud We chose the Tau brandas the Greek lettertau is used to represent bothtorque and the golden ratio The Torque is ametaphor that is apt because T2D VM family allocatesan entire physical coreper vCPU  delivering excellentper thread performance while the goldenratio tau representsT2D s focus on onlycore features neededfor scale out workloads inorder to deliver excellent priceperformance Focusing on T2D sperformance  wecan see that it outperformsVM types available elsewhere Here s a quick look attwo benchmarks wherewe showcase T2D sprice performancefor estimatedSPECrate and CoreMark For estimated SPECrate T2D provides 42 better price performance whencompared to other major cloudproviders Back to you  Jamie JAMIE KINNEY  Thanks  Subra Now we d like to take a lookat compute intensive workloads And I thought we wouldstart by giving youan overview of what we mean bycompute intensive workloads While I ve focused on scientificand technical computingin the past  compute intensiveworkloads actuallyfall under a number ofverticals and industriesaround the world This includes workloads suchas high performance web serversthat need access tofast clock speed CPUs as well as the networkingthat allows pages to be servedwith incredibly low latency Also really importantfor triple A gameservers that need toboth scale up verticallybut also ensure thatthere s no jitter that sbeing viewed as a result of anyinconsistencies in performancewith the CPU ornetwork variability High performance computing near and dear to my heart tends to fall into twocategories   tightly coupled which will usuallyrely on low latencynetworking between anumber of machines and also high throughputcomputing thatrelies on many machines workingin concert to solve a problem where the per coreperformance of those machinesis going to indicate howwell that application runs And a great example of ahigh throughput computingworkload is ElectronicDesign Automation  or EDA which needs accessto both fast cores but also a network thatmakes it easy for allof those cores toaccess resourcesthat are stored on ahigh performance shared filesystem In order to meet the needsof these unique applications we ve developedcompute optimized VMs including the C2family  that offera combination of excellentper core performance consistent performance and also isolationbetween virtual machines The C2 VMs  as we can see herewith the benchmark results thatcompare C2 to theN1 family  offerbetter per core performance offer low latency high bandwidthnetworking  and are reallydesigned to meet the needsof these unique applications Another element ofthe C2 family isthat the standardmachine types for C2have a lowermemory per core ratio as these types of applicationstend to focus on CPUneeds as opposedto taking advantageof large amountsof memory per core Speaking of memoryper core  I d liketo now transition over to thememory intensive workloads So this could include anumber of different typesof applications It could be in memorydatabase like SAP HANA It could be in memoryanalytics  OLAP systems and other analyticsystems that needto keep data sets in memoryfor low latency analysisand access  as well asin memory caching that scommon componentsof many distributedand cloud native applications Think of Redis orElastiCache  for example For these workloads  theM1 and M2 VM familiesare likely to be your best fit M1 and M2 VMs can offer upto 12 terabytes of memory while also offering alarge number of coresto go hand in handwith that memoryso that you re ableto analyze at scale The largest M2   ultramemand megamem VMs  for example offer over 400 cores or 400vCPUs per virtual machine These VM families also takeadvantage of the fully flexibleinfrastructure  allowing youto adjust the CPU and memoryneeds provisioned to meet theneeds of your applications And we also support theprovisioning models including flex cuts that allow your  to get discountswhen you re usingall of our memory optimizedVM shapes  both currentand future Now I d like totransition us backto another class of workloads These are the acceleratedworkloads thatneed access to GPUs and TPUs Subra  would you liketo give us an overviewof these workloads and theVM families to go with them SUBRA CHANDRAMOULI  Sure Thanks  Jamie There are several workloadsthat can take advantageof hardware GPU acceleration Typically  these aredata intensive workloads For instance  you may havean AI inference or a trainingtype of a workload Some of the trainingworkloads could be bursty where they have to deal with alot of data in a bursty mode Or you can thinkof a data analyticswhere you have massivedata coming through and you have the ability toprocess the data parallelly GPUs are a great fitfor parallel processing Other types of workloadsthat can take advantageof GPU acceleration arehigh performance computingand severalvideo intensive workloads Our A2 VM familyincludes the latestNVIDIA A100 GPUs with upto 40 gigabytes of memory In these machinetypes  we support upto 96 vCPUs with 1 3terabytes of memory These machines come in variouspredefined machine typeswith support up to16 NVIDIA A100 GPUs delivering up to 10teraflops of FP16 performanceor 20 teraflops of int8performance in a single VM JAMIE KINNEY  Thanks  Subra Really appreciate that In addition to each of the VMfamilies that we ve mentioned we also have a numberof GCE featuresthat span multiple VMfamilies and that are reallygoing to be relevant to abroad range of workloads We just want to touchon a few of these today And I d like to start by talkingabout custom machine types This is a feature that sunique to Google Cloud And it s a feature thatis giving our customerssignificant cost savings On average  they reseeing 19  savings And in many cases they re seeingsubstantially more than that Custom machine types  as youcan see in the little slideor diagram in themiddle of this allows you to precisely dialin the CPU and memory that sallocated to the VMs that you relaunching within Google ComputeEngine This is often applied in caseswhere the standard machinetypes that we ve predefinedfor ease of use and flexibilitymight not meet yourneeds  and whereyou need to allocateperhaps a little bitmore memory or a little bitmore CPU for your workload And by allowing you to tellus exactly what you need we don t over provisionthose resources for you This allows us to then reducethe cost of Google ComputeEngine  and we pass thosesavings back onto you Some additionalGCE features thatalso benefit the widerange of workloadsyou might want to runon Google Compute Engineinclude sole tenant nodes This is most oftenapplied for applicationsthat have licensingrequirements thatmandate that you are theonly customer  or only user running on a givenphysical server The sole tenant nodes  you haveaccess to the entire machine and you have the ability tocarve that up into whateverVM shapes you need Very helpful forapplications thathave those specificlicensing requirements Shielded VMs are afeature that allowyou to ensure thatyour workloads arerunning on trustedand verified hardware taking advantage ofthe TPM UEFI firmwarethat we re running onGCE infrastructure And confidential VMsprovide additional isolationand sandboxing for especiallysensitive applications You can read moreabout these in someof the resources that we veprovided below  accessing boththe GCE documentationtutorials  casestudies  and other resources We ve also provided some linksto additional Next sessionsthat you might wantto attend  as well asa couple of Courseracourses for those of youwho might be new toGoogle Compute Engineor simply want to expand yourfamiliarity and understandingof our services With that  I d reallylike to thank Subrafor joining me today And most importantly  I d liketo thank you for joining usand for listening to this talk I hope that it improves yourunderstanding of Google ComputeEngine And I hope that the restof your Next experienceis a wonderful one Have a great day Thanks DAMITH KARUNARATNE Hey  everyone Welcome to the sessionon CI CD anywhere In this session  we ll bediscussing why CI CD anywherematters and how we re helpingcustomers streamline theirbuild  test  and deployprocesses across cloudand on premise environments My name is Damith Karunaratne and I m a Senior ProductManager at Google Cloud And this is mycolleague  Ben Houston who is a KubernetesSpecialist at Google Cloud Today  we will be speaking toyou about Google s new approachto doing CI CD anywhere Before I dive intoproduct details I d like to turn it over to Ben He s going to givea quick overviewon why a hybridapproach to CI CDis such a hot topic these days BEN HUSTON  Damith  thanksfor the great introduction And to those of youin the audience I have an importantquestion for you Why should we careabout being able to docontinuous integration andcontinuous delivery anywhere Well  we should carebecause CI CD flexibilityis an extremely importanttopic right now This is due to thefact that enterprisesneed build processes thatcan keep pace with constantlychanging business  regulatory and security requirementsand which will work acrossmultiple environmentsthey have made massiveinvestments in More specifically  enterprisesneed transformationaldevelopment capabilities like build anywhere or deployanywhere  because these cansupport massive on demandscaleup  are inherentlyresilient and highly available can be authored and managedin a completely versioned declarative manner  andare instrumented without of the box toolingwhich provides end to endobservability acrossthe entire build system Unfortunately  mostenterprises are stillusing flexible legacybuild tools like Jenkins which predate the cloud These tools simply can t meetenterprise s current needs because they don tprovide a uniform wayto do continuous integrationor continuous deliveryacross different environments And they also can t easilyscale to accommodatethe type of extremely highvolume  high concurrencyCI CD that dominates modernsoftware development The good news is that mostof the performance problems which have traditionallyplagued build systems have been solved in recentyears by Kubernetes  whichis an extremelypopular orchestrationframework for operatingsystems at massive scale The most highlysought after at scale systemattributes  such as flexibility scalability  resilience control  and performance  arenow provided out of the boxby the Kubernetes API its unique approachto distribute applicationlifecycle management Next gen build tools  likethe one that my colleagueDamith will introduce next understand they do notneed to reinvent the wheel These tools explicitly harnessthe power of the Kubernetes APIand extend it with the rich new primitives specificallydesigned to address the mostdifficult CI CD use cases Now  let me explain whyKubernetes is such a big dealfor modern CI CD Let me turn thefloor back to Damithto introduce Google s newapproach to CI CD anywhere DAMITH KARUNARATNE  Thanks  Ben With all of this in mind  I dlike to introduce Cloud BuildHybrid With Cloud BuildHybrid  Google Cloudis taking the power flexibility  and simplicityof Google Cloud Build andbringing it to anywhereyou can run Kubernetes Cloud Build Hybrid is poweredby Tekton  which means outof the box  youavoid vendor lock inand get thereliability that you vecome to expect from Kubernetes Capabilities like self healingand graceful failoverallow for no singlepoint of failure Being able to run directlyon your infrastructurebehind your firewallsnow means youcan get the advantagesof cloud native CI CDfor workloads that can teasily be moved to the cloud But Cloud BuildHybrid is not onlyabout providing the benefitsof Kubernetes in Tekton Since this is an extensionof Google Cloud Build Cloud Build Hybrid providesyou a consistent interfaceand workflow for building anddeploying your applications You can seamlessly switchbetween using the fully managedor hybrid version ofCloud Build without havingto change your workflow You also get asingle pane of glassthrough the existingCloud Build interface so you can easilysee activity acrossall your CI CDenvironments in one place regardless of whether you reusing the fully managedor hybrid version With Cloud Build sdeveloper centric syntax you re able to easilydefine your CI CD workloadswithout having advancedknowledge of Kubernetes So developers canspend more timeon building anddelivering softwarethat drives the business And upgrades havenever been easier All Cloud Build Hybrid instancescan be centrally managed which includes automatedupdates and patches to makesure you re always secure In addition to this  with theGoogle managed control plane you get the availability andsupport your business relies onto ensure smooth operation With Cloud Build Hybrid we handle the complexityof multienvironment CI CD Just add environmentconfigurationto your Cloud BuildYAML  and your pipelineswill easily andtransparently runthe workload against registeredKubernetes environments regardless of wherethey re located Scaling these workloadsacross your environmentis also simplified Cloud Build Hybrid harnessesthe power of Kubernetesto provide robustautoscaling and concurrencyfor all your CI CD processes Not only is scalingsimplified but it salso more efficientwith resource pooling All workloads are entirelycontainerized and managed usingKubernetes  allowing them tobe scheduled right alongsidenon CI CD workloads Workloads also runcompletely hermetically ensuring that no buildtime dependencies evercross contaminate the runtimedependencies relied uponby other colocated workloads For hard to update resource constrained on premise environmentsthis scheduling fungibilityis extremely valuable Now  let s take a peek underthe hood to see how this works On the left  we have GoogleCloud in the fully managedcontrol plane On the right  we have ahybrid multicloud environmentrunning a Kubernetes cluster With Google Cloud sConnect service we can connect a Kubernetescluster to the Google Cloudcontrol plane Doing this will installthe Connect agent which will allow thecontrol plane to communicatewith the cluster Once the clusteris connected  wecan enable theCloud Build featureand install hybrid worker pools These hybrid workerpools consistof Tekton and someadditional componentsand configuration that makeinteracting with Google Cloudseamless Once hybrid workerpools are installed Cloud IAM will need to beconfigured for Cloud Buildservice accounts toproperly communicatewith the worker pool And finally  we llneed to make surethat the Kubernetes serviceaccount on the worker poolis configured to interact withour source control and artifactrepositories Once Cloud BuildHybrid is set up you can leverage thesame Cloud Build workflowas the fully managed offeringto build  test  and deployyour applications Your workload isexecuted as a seriesof build steps which are definedas containers we call builders Since each build step ishermetic and only existsfor the duration ofthe step  any datathat you wish toshare across stepscan be shared througha common workspace Google Cloud provides a set offully supported and maintainedbuilders for avariety of workloads but there are also over ahundred community buildersavailable You can also createyour own builderto address unique use cases This workflow cannot only be usedto build containers but alsononcontainers  such as Java or Go artifacts  orgolden VM images The Cloud Build syntaxremains the samefor executing workloadson hybrid worker pools All you need to do is specifythe hybrid pool that sgoing to execute the workload For instance  in this Hello  world  example which has six lines of code we specify the hybrid poolthat the build isgoing to run on This build will be transformedinto a Tekton task runbefore it s sent tothe hybrid worker poolwith optional support forDocker and Docker builds There s a large communitythat uses Docker today and we want to make sure theexperience for the Dockercommunity is asseamless as possible This means theydon t have to worryabout the overheadof orchestratingDocker and Docker buildprocesses in Kubernetessince we take care of thisbased on the hybrid poolconfiguration The resulting Tektontask run in this example which executes aDocker and Docker buildends up being 156 lines This translates to roughly26 times more lines of code We re taking care of theboilerplate and Kubernetesconfiguration required to runthis build while integrating itwith Google Cloud Services  suchas Cloud IAM  Cloud Logging and source control integrations This allows developers to focuson building great softwareand not worry about theunderlying technologies runningtheir CI CD workloads acrossall their environments Cloud Build andCloud Build Hybridare part of a much broaderend to end GCP DevOps toolchain Some of theseplatforms and serviceswill be highlighted in the demothat Ben is about to show you Ben  over to you BEN HUSTON  Thanks  Damith Awesome product overview I m going to quickly walkthrough the logical flowof a short demo we ve prepared First  we ll see adeveloper check new codeinto version control Next  we ll see Cloud BuildHybrid detect the code changeand begin to kick off builds Then we ll see fourprocesses concurrentlybuilding and deployingthe same applicationto four differenttarget environments  GKE  Anthos attachedclusters in AWS and Azure and an Anthos bare metal clusterwhich is running on premise Finally  we ll see the overallbuild progress being monitoredwith built in tooling  likethe Cloud Operations suiteand Cloud Build dashboards OK  on onto the demo All right  you re now readyfor a quick demo of Cloud BuildHybrid Here  we see fourdifferent typesof Kubernetes clustersregistered with the GoogleCloud control plane  GKE cluster  two Anthos attachedclusters for EKS and AKS and Anthos bare metal clusterthat is running on premise The great news is thatonce you register a clusterto Google Cloud installing Cloud BuildHybrid it s dead simple Now we re going to opena Cloud Shell sessionand run a quick CLI commandto confirm that our cluster isready for action Yep  the install  INAUDIBLE is OK across the board OK  now that the environmentoverview is out of the way let s quickly explain how CloudBuild Hybrid jobs get queued Cloud Build Hybriduses a configurationconstruct called the triggerto associate a build eventlike a pull requestwith a build pipelineand kick startthe build process You can see we have fourtriggers to find  one for eachof our Kubernetes clusters In a moment  we regoing to switchto an IDE  a k a   anInteractive DevelopmentEnvironment We ll commit a config changeand push it to our Git repo Afterwards  thetriggers we defined herewill listen for thiscommit  and they kick offa Cloud Build Hybrid job foreach of our environments In our IDE  we seethree important files  cloudbuild yaml deployment yaml  and the Dockerfile Cloudbuild yaml definesa build pipeline Here  you can see thatwe ve defined three steps  Build Image  Push Image and Deploy to a Cluster Next  deployment yaml containsour Kubernetes deploymentconfiguration Here  we see that the KubernetesServiceAccount  a Deployment and a HorizontalPodAutoscalerhave been defined Finally  the Dockerfile shows what isbaked into our container image Here  we pull anupstream nginx image write an awesome echostatement  and then take a nap To show Cloud BuildHybrid in action we re going to increasethe sleep to 11 secondsand then commit andpush this change Now that code is committed we ll switch backto the GCP console  wherewe see four new buildsin this schedule Each build correspondsto one of the triggerswe showed earlier  one for Google Cloud  AWS Azures  and on premises Importantly  both theCI and the CD stepsare happening at the sametime within each clusterusing Tekton Now that a fewminutes have passed we ll see that our buildsare starting to complete Next  we re going to clickon one of these buildsto see what that looks like Here  we can seethree build steps  build  push  and deploy  and our build logs You can also see metadatarelated to the build For example  we can see thatit ran on hybrid worker poolgke cluster 001 Finally  I want to show yousomething else that s prettycool about Cloud Build Hybrid If you want to do a deepanalysis of your build logs these are already preintegratedwith Cloud Logging This means thatall of your buildlogs are already defined withthe powerful Common ExpressionLanguage You can modify as yousee fit and then useto rerun your queries This allows you to easilydive in and exploreany complex build errorsyou might run into Now that our buildsare finished I want to show you onemore important thing When we explore thebuild for another cloud like AWS  we see thatthe build interfacelooks identical to theone we just saw for GKE which is totally awesome Now it s time to check thestats of the workloads we justdeployed In the Google Cloudconsole  we see a screenthat shows what workloadsexisted in the defaultKubernetes namespaceof our clustersbefore we committed new code Hitting Refresh shows wenow see four new nginxapps running acrossGoogle Cloud  AWS  Azure and on premise And just for fun let s confirm whatthe dashboard for a fewother cloud providers shows Hitting refresh in EKS showsthe same workload deployedabout five minutes ago The view from the AKS dashboardtells the exact same story Same workload again   thistime  it s four minutes old And now we ve reachedthe end of our demo We ve seen howCloud Build Hybridcan be used to bothbuild and deploy appsacross multiple environmentsusing Tekton powered Kubernetesawesomeness andslick Google tooling Enjoy DAMITH KARUNARATNE  Thanksfor the great demo  Ben To quickly recap you just saw Benshow how Cloud Build Hybridprovides a unified CI CDexperience that abstractsaway the complexitiesof cloud native CI CD withKubernetes while providingthe flexibility to build  test and deploy to any environment Google Cloud s fullymanaged control planedprovides a single pane ofglass to gain visibilityinto all yourworkloads  regardlessof where they re running Finally  Cloud BuildHybrid harnessesthe power ofKubernetes and Tektonto run your workloadsreliably at scale All of this will beavailable in public previewlater this quarter In the meantime  to learnmore about Cloud Build take a look at the CloudBuild documentation And don t forgetto check out someof the other sessions onCloud Build  Cloud Deploy and securing your softwaredelivery pipeline Thanks for tuning in andlearning about Google Cloud sapproach to CI CD anywhere Enjoy the rest ofGoogle Cloud Next TIMOTHY PEACOCK  Thanks forjoining us for Security 209 Cloud posture andworkload protectionwith Security CommandCenter Premium I m Timothy Peacock  a productmanager here at Google Cloud I look after ourthreat detection capabilities And in my spare time  I co hostthe  Cloud Security Podcast by Google I ll be joined today byDr  Jasyn Voshell  directorof product and solutionsecurity at Zebra If you re interestedin learning howto protect your organization suse of Google Cloudas a platform and protectyour workloads on top this session is for you We have a packed agendatoday   an overviewof SCC Premium for those of younew to the product  a deep divewith our friendJasyn from Zebra and a lightning roundof announcementsfrom across the Security CommandCenter platform  misconfig and threat detection teams Let s dive right in Security Command CenterPremium brings together in one central tool security insightsacross your organization andacross the security lifecycle I think of SCC Premium asproviding four pillars of valuefor security teams First  asset andresource tracking Staying current with resourcecreation  update  and deletioncan be a challengein public clouds If you choose to giveengineering teams flexibilityin creating new services Security Command Center s CloudAsset Inventory makes it reallyeasy for organizational adminsto stay aware of what schanging inside their organd easily query fordeleted and changed assets Second  hardening Misconfiguration andvulnerability detection Thanks to our real timeasset updates SCC Premium provides real timemisconfiguration detection Cloud Security PostureManagement  CSPM is a must have forsecurity teams in Cloud Security Command Center Premiumprovides  out of the box  actionable posture insightsfor our top GCP services Understanding and respondingto misconfiguration exposureis crucial for preventingbreach in Cloud The third pillar isthreat detection And we all know that despite ourbest investments in hardening bad guys are still goingto try to do bad things I ll talk a lot moreabout detecting threatsin just a minute Fourth and finally SCC Premium helpsyou tie your success in CloudSecurity Posture Managementback to leadingcompliance standards Not only do our findingsmap to leading standards we also provide the righttooling in the platformto help you reporton your complianceacross your organization Now let s look in depthat two of these areas  first hardening  andthen threat detection SCC Premium provides out of the box platform and workload hardening Our posture findings are  likeI said  generated in real timeand come withremediation instructions making these insightsusable and actionableby engineering teamsand project owners whoare maybe not security experts Above the infrastructurelayer  we renow bringing in operating systemvulnerability insights thanks to our partnership withGoogle s VM Manager service Users can expect to see moreupdates on SCC s presentationof CVE detection froma couple of other teamsfrom Google Cloud really soon Finally  on top of thosecommon vulnerability insights SCC Premium alsoprovides the same web appscanning for OWASP top10 vulnerabilities as we use ourselvesin house to protect Google Our web security scanner iseven easier to use than ever with automated discovery andscanning of your web apps All of this comes together tohelp security teams proactivelysecure both their cloudenvironment and their workloadsrunning on top without bringingin third party dependencies exposures  and risk And with that  let s turn totalking about the most unwantedof third partieswith a quick lookat threat detection in Cloud A few years ago atconferences  we allgot to feel very smartand smug by saying oh  the cloud is justsomeone else s computer The truth is  in Cloud there s a lot more than justsomeone else s computer We have new planesof attack surface and with them  new opportunitiesfor instrumentation At the same time asthe world of cloudhas brought tremendouschange to the security job our needs  as securityprofessionals haven t reallychanged all that much We re still focused onkeeping data confidential guaranteeing integrity and ensuring availability Tolstoy wrote that quote   All happy familiesresemble one another Every unhappy family isunhappy in its own way  Today s major public cloudsare our proverbial unhappyfamilies Adversaries attack and pivotthrough clouds differently And security teams need deepcloud specific expertiseto detect the most dangerousthreats in their environments With Security CommandCenter Premium our customers get access to thesame real time threat detectionas Google uses to protectits own use of Google Cloud They say all models arewrong  but some are useful Here  I offer whatis definitely wrongbut a hopefully useful six layer model for thinkingabout protect surfacein cloud environments SCC Premium offersdetection modulesat each layer ofthis six layer cake with special emphasis ondetecting identity compromiseand protecting Kubernetesworkloads at runtime We handle platformthreat detectionwith our real timelog processing service  EventThreat Detection  andworkload threat detectionwith our ContainerThreat Detection service We ll talk more aboutsome new capabilitiesin both of thoseservices in just a bit With that  I want to welcomeour guest today  Dr  JasynVoshell of Zebra Technologies Jasyn  thanks so muchfor joining us today JASYN VOSHELL Thanks for having me TIMOTHY PEACOCK Tell our audiencea little bit about yourself your role in the org and what your company does JASYN VOSHELL  Absolutely So I m Dr  Jasyn Voshell I m the director for productand solution securityfor Zebra Technologies Zebra may be thebiggest smallest companyyou ve never heard of Everything fromscanning a packageto tracking anasset from point Ato point B   that s theproducts we put out And my responsibilityand accountabilityis to make sure thoseproducts we put outare as secure as possible A good example   ifany of you have evergotten a packagedelivered to you and it has a barcode on it that tracking from whereit was all the way toarrives at your doorstepwas using Zebra Technologiesproducts and possibly solutionsas well And so my job is to  again make sure that everythingwe do with that is assecure as possible TIMOTHY PEACOCK  That san amazing impactthat a company likethat has so invisibly So they say to start securestories at the beginning So could you tell theaudience a little bitabout getting started withSecurity Command CenterPremium  how longthat took  and whatit was like rolling it out toyour different business units JASYN VOSHELL  Yeah it did take some time I think with any new productthat comes into play there s always someresistance at first People are used to doingthings a certain way But with the GoogleCommand Center we use a lot of GCP Google Cloud Platform in the solutions that we build The old ways of trying tomake sure that s secure the apps on topof it are secure tracking the inventoryof what we have  it took a little bit of time But as we progressforward  it really helped Good example   wehad a customer during an RFP   a Requestfor Proposal   discussion And we were stepping through They were asking us  well how are you securing this How are you doing this And at one point we said  oh  we reusing the GoogleSecurity Command Centeron top of this application And they stoppedand went  well  Idon t think we need to askthe next 10 or 20 questions because they knewabout this product and they knew thatthose questionswere being answered already And so with that  when that tooka hold  when customers startedto realize  yeah  you re usingthat  our internal teams went that s a lot faster We can answer questions quicker We can make surethe risk is reducedor to an acceptable amountwhere we need it to be And with that  we were able toget better internal adoption You always want to carrythat big stick around but you never want to swing it You want the teams internally  to want to use it to show them the value And after a little bit ofthat  getting over the hump having a customer go  yeah that makes it a lot easier having the salesteams realize  hey you just answered anRFP quicker on securitybecause you reusing this product  they re happier about it  too We got a lot of  quicker adoption much easier It also helped a great deal  and I think we ll diveinto this a bit morein a second   but being able togive those teams back feedback Hey  we found something  andhere s how you can go fix it and here s how quickwe gave it to you and not getting a lot of FUDor a lot of cloudy stuff  pun fully intended  for those teams So that does help across the board TIMOTHY PEACOCK  Yeah so let s dive in on that because one of thethings that comes upfor me  with mycustomers  is really how do they scale theirsecurity operations as theygrow their cloud footprint One of the featuresthat we built  really two things here  to help addressthat   one is whatwe call scoped ordelegated views of SCC where you can give the teamsresponsible for a projector folder a view oftheir findings in SCC And the other is packing alot of detail and remediationinstructions intothe posture findingsso that non expert teamscan really respond to them So how has it gone rollingout that delegationof posture issues to teams JASYN VOSHELL  Yeah  so Idon t know if anyone has hadthe old schoolway of  how do I  IT or some team finds abunch of vulnerabilities and they want to give thatto the engineering teamsand say  go fix it In the past  they mighthave gotten a spreadsheetor logged into a website And it s just aplethora of data And anybody who looks at thatgoes  like  my head hurts I forgot what happened And they kind of dismiss it With the SecurityCommand Center the ability to focus in onthis business unit for thisproject  so you re not lookingat every project just  hey  this businessunit  this project Here are the things we refinding for you to go look at It cuts right to the chase And I think being ableto give those teamsor delegate to a certainengineering team or securityteam only whatthey need to see  They don t needto see everything In fact  if they do  they relooking at the squirrel trying to figure outwhere it s going They can focus onthat  and they canfix it a lot quicker  right It s not too overburdened So with that  it s helpedus speed up production in many cases  becauseas we re moving  we ll talk about shiftleft in a second  as we re moving that furtherinto the development lifecycle we re finding things quicker And we re onlyshowing the teams this is what you need to fix And so it feels alot quicker for them TIMOTHY PEACOCK Let s talk about shiftleft  because we ve got alot of different sessionson shift left atthis year s Next We ve got talks aboutsupply chain securityand securing that Could you tell usa little bit howyou see SCC s hardening worksupporting your organization sshift left efforts JASYN VOSHELL Yeah  so shift left for anyone who s not familiar  hopefully everyone is  it means putting securityearlier in the developmentlifecycle So as you re evendreaming about a productthat you want to build you re thinking securityalmost upfront  right away And so this tool allowsus   so as we stand up even at development  anon production instance we stand up GCP We put  INAUDIBLE  on top of it That Security CommandCenter is stillscanning that  still takinga real time type of lookinto what we re doing And we re able to fixthose items quicker And if anybody knows when you shift left  because if I find avulnerability late in the game it s about to go out thedoor  and suddenly I realize oh  no  thefoundation is crackedon this house we alreadybuilt  it s a little lateto fix the foundation now Or if I m going to fix it  it sgoing to cost me quite a bit And so we really want to dothose things early on  or left in the process The Command Center allows usto find those vulnerabilitiesquickly  all during the process So we re not  like  moving 10feet and then doing a check Every time we remaking a little move it s always checkingon us so we can say hey  we found something Oh  we ll take careof that right away It helps to actually speedup the development lifecyclewhile keeping theproduct secure And that s veryimportant for us It helps us get theproduct out the doorquicker  which meansour customers have itwhen they need it And that s the mostimportant for us TIMOTHY PEACOCK  That ssuch a nice storyaround both helpingwith securityas well as helping withdevelopment velocity So often  those are intention Could we also talk alittle bit about howthis is being used at yourorganization for threatdetection and how that sgone for your teams JASYN VOSHELL  Yeah  so oneof the nice things we liketo look at threat detection  tons of tools in the world Everyone is going  oh  I canfind out what s going on here The hardest part  for me  false positives You see something Hey  we think there sa threat at the door Well  is it or is it not And if you re makingme put a resource on itto go investigate it you re chewing up resources I can t expand my teams The Security CommandCenter  in my opinion has driven that false positiverate down quite a bit  INAUDIBLE  with alittle bit of tuningthat we can do with thehelp from Google to helptune that as well So when we get an actual threator detection of a threat it feels more real We ll say  in the past you get a threat detection Maybe there s a 40  50   60  confidence When we see them now  throughthe Security Command Center we re in the high 90son a confidence of yes  that is an actual threat It is actionable It s an event Let s go take care of that What does that naturally do It frees up ourresources to do thingsthat are more important more risk adverse  as opposedto chasing ghosts  chasing FUD So that helps us agreat deal  providesconfidence in the product It also helps us  even whenwe re talking to our customers telling them  yes we re using this tool OK  what kind of thingsare you looking for We look for this We re able to knockthings down quicklyand move to the next problemor vulnerability or threat thatpossibly comes up Also  getting to thefalse positive piece it does allow  again  it frees upthose resources which means theycan go look at other things So as teams aredeveloping a project we re not chasing a ghost We re looking at real threats And we can getback to looking at is there a misconfiguration Are we now alignedwith a benchmark Is there a standardwe re not aligned to And fix those thingsto keep it hardened and not just chasingthings around TIMOTHY PEACOCK Oh  that s awesome I want to touch on onearea before we wrap up which is the user interface We ve made a lot of changessince some people startedusing the product I understand you may haveseen the more recent versions We ve got a lot of goodchanges coming soon How is your team finding the UI JASYN VOSHELL Yeah  so you and Ihave talked about this before I don t like to call it a UI I like to call it a UFI User Friendly Interface The ability for anyoneto just log in on a whimand not have to crack open a16 page or 160 page guide thatlooks like somemajor book and justget to where you want to getto is extremely important So as a director  I m not inthe tool every day  right Our teams  ourSecOps division  isin there looking atwhat they need to do They re more advanced on it But when we havesomething come up or leadership asksa question on Jasyn  what s oursecurity posture What s our risk look like Or we have a customerthat says  wewould like to know alittle bit more about what are you doing withyour vulnerabilities How often do you see things As a director  I canhave the team open it up I can look inside And without reallyknowing much morethan just my technicalspace  I can figure outwhat s going on in thatdashboard fairly quick And I ve got to tell you that user friendly interfaceis very beneficial I  as a director can get an answerreally quick withouthaving to knowthe technical details of it Now  of course there are some thingsthat that team is better for But from where I m sitting I can get an answer quick I can tell our leadership our customers  or evenour internal people thatneed those answers  evenfrom where I m sitting So I m very mucha fan of your UFI TIMOTHY PEACOCK  Jasyn I love that answer And with apologies todirectors of securityeverywhere  I thinkwhat we just heardis that our userinterface is so easy  evena director can use it Jasyn  thank you so muchfor joining us today I really appreciate yourthoughts and insightson the product We didn t want to leaveout your CISO  either He s got this to add And what I loveabout his quote hereis that it really shows how SCCPremium brings together boththe Cloud Security PostureManagement side as wellas the Cloud WorkloadProtection capabilitiesto give your company reallya one stop shop for allyour needs inprotecting Google Cloud So I promised announcements and I didn t forget We ve got a coupleof new capabilitiesthat are excitingin the platformas well as in the hardeningand threat detection sidesof the house So with that  a couple ofreally great announcements here The first one is that I vehad so many users ask me over the years  howthey can tune and manageour systems that I suspectsecurity engineering isactually full of musicians We re rolling out afeature now that we recalling Mute  whichis essentiallyGmail filters foryour SCC environment We know that not everyone of our findingsis always right foryour environment Organizations can havecompensating controls and threat intelligenceis occasionally noisy With the Mute feature  teamscan easily silence and controlthe scope of thosesilences to ensurethat their findingsqueue is full of findingsthat teams care about Muting would be really hardto get right without findingstandardization And so we vestandardized attributesacross our different findings With Mute  we re simultaneouslyrolling out new attributesand making sure that ourvulnerability and threatteams are usingthose to give youa consistent experience as yougo through the findings set We know that teamsalready have workflowsfor their othersecurity tooling And we re makingit easier than everto export SCC s findingsinto your channels of choice including  now  yourSlack channels of choice Finally  a big ask from usershas been scoped or delegatedviews Jasyn and I touchedon their value And using the normal IAMpatterns from Google Cloud teams can now give teamsresponsible for projectsand folders insightsinto the issueshappening in those environments On the misconfig andvulnerability side we ve got some really excitingannouncements as well Security Health Analytics  ourreal time posture managementsystem  now supportsuser defined detection This has been atop ask  and we rereally excited aboutwhat s now possible I mentioned that we re bringingin OS level CVE detectionwith our VM Manager integration This is a great collaborationwith our friendsover on Google Compute Engineand will help organizationsto understand howwell they are doingat the eternaltopic of patching It s a bit like flossing   notexciting  not necessarily fun but definitely among themost important thingsyou can do to shiftyour security outcomes Finally  on the misconfigand hardening front we re also bringingin detection of CVEsin widely deployed applicationswith a managed deploymentof our open source Tsunami tool Now  with that  on to thethreat detection updates  three areas in EventThreat Detection and onein Container Threat Detection Google Groups is usedby most organizations Rather than assign IAMroles directly to users roles are assignedto groups  and usersbelong to those groups This makes management ofonboarding and offboardinga lot easier  but thereare some gaps and risksthat get introducedby Google Groups ETD has now integratedwith Google Groupsto give teams findingsthat highlightwhen a group with permissionsbecomes more widely joinable as well as when a grantis made to a group thatcontains external members Additionally  we verecently releaseda set of detection techniquesusing User Entity BehavioralAnalytics  or UEBA It s one of those acronyms thatonly our friends at Gartnercould love But it describes thegeneral techniqueof profiling and detectingdeviation from those profiles We re using it today todetect account takeoverand compromise forboth IAM accountsas well as service accounts On the Kubernetes side we re rolling out a numberof new detection modules One that we releasedvery recentlyis a machine learning poweredbash script classifier Our general theoryof defense with KTDis to take away from adversariesentire classes of techniquerather than specific procedures With this detector  we havea robust and generic approachto detecting malicious scriptexecution inside of containers It leverages Google sstrengths in ML and we re reallyexcited about howit s performing againstproduction data already One more thing We ve integratedSecurity Command CenterPremium with Chronicle This is a wonderfulcase of the teamstruly being better together SCC s real timethreat detection joinsChronicle s powerfulinvestigatory capabilities With this integration users can seamlesslypivot from a  INAUDIBLE  detailin Security Command Centerinto a curated alertview within Chronicle From there  users caninvestigate the threatand quickly pivot toassociated actions and eventsrelated to the finding For example  imagine a SOCoperator quickly investigatingif a principal observed makinga suspicious IAM grant hadrecent login activity or if theymade other suspicious changesafter that IAM grant We re really excited aboutwhere this partnership is going Beyond ingesting SCCfindings  Chronicleis also ingesting the stream ofCloud audit logs in real timeand pulling Cloud AssetInventory metadata to ensurethat analysts have thelatest and greatest contextwhen investigating threats I know this has beena whirlwind tourof SCC Premium  ourcapabilities  and what s new Thanks for hanging on If you re ready to learn moreor get started with SCC Premium you can check out ourdetailed documentationor contact your accountteam to get started today We re here to protectyour organization and we re definitelyno use of youif you can t followwhat s going on We take feedback on thesesessions really seriously so please do fill out thefeedback for the sessionand let us know how wecould help you better With that  I want tothank you for joining usfor Security 209  Cloud postureand workload protection I m Timothy Peacock  andI want to thank you againfor taking the time to learnabout how Security CommandCenter Premium could helpyour organization with assetvisibility  misconfigurationand vulnerabilitymanagement  threat detection and compliance reporting Have a great day ANDY CHANG  Welcome toSCC 208  Shared Fate In this talk  you ll learnhow Google Cloud is workingto enable you to move froma shared responsibilitymodel for securityand public cloudto a shared fate model  wherewe as the cloud providercollaborate much morestrongly with youand take additionalresponsibility for makingsure what s deployedtogether is in the mostsecure and compliant state A core component ofthis ability to changehow risk is transferredis providing youwith production provenblueprintsfor foundational infrastructure workloads  and applicationsbuilt with Google Cloudbest practices and patternsso that collectively wehave a much better viewand understanding ofthe quantified riskand the securityposture being deployed I m Andy Chang  senior productmanager for the Cloud Securityteam As we ve seen withmany customers an ongoing desire is toget started and operatemuch quicker but withalso additional securityso that the value of Cloudcan reflect in the businessesthat you run The approach we re now takingto enable and accelerate thisis building out a set ofopinionated security foundationand posture blueprints toautomatically allow youto leverage predefined GoogleCloud security best practicesfor your infrastructureand for your workloads This is part of theoverall Google Cloudvision in security for goingeven further in our partnershipwith our customers We want to move the industryand help move the industryfrom current sharedresponsibilityapproach to a shared fatemodel  where we re trulypartners in your successand in achieving your goals And this won t happen overnight But the key piecesare coming into place First  a necessarycondition for thisis that the underlyingCloud infrastructureis highly trusted and secure that we re providing youwith that securecompliant platform as wellas a portfolio of securityand compliance productsand solutions that you can use The good news is thatour mission to makeGoogle the most trusted cloud  we re well along the way there Next  we re providing customerswith opinionated foundationaland posture blueprints matchedto the type of workloadsthat they need  whichenable you  again to build on top of theunderlying services to stack and build postureblueprints on top of landingzones  and which all haveGoogle best practices in termsof configuration architecture  security postureembed with them You re therefore able to stackthese opinionated postureblueprints on topof landing zonesand take advantageof the knowledgethat Google s alreadybuilt around security We then take the next stepand think about takingthe blueprints as the basis forend to end solutions to solvevery specificbusiness problems  for example  web applicationAPI protections  zero trust and BeyondCorp enterprisesolutions and securityanalytics at scale By combining thesetogether  you renow able to get the bestpractices in a deployable formtargeted towards solving veryspecific business problems Finally  by havingthe combinationof the blueprintsand the solutions it leads to a significantincrease in the ability for youto use consistent approvedend to end patternsfor complete businessprocesses and applications That consistencyand greater defaultsecure configuration andbest practice adoptionleads to a much morequantified risk model allows us and you to betterunderstand the architectureand risks of being deployedin your infrastructureand your workloads and thereforeallows us to take on moreresponsibility collaboratingwith you and enabling differentmodels of risk transfer As we launched both our riskmanager and our risk protectionprogram  this allows you  ifyou ve adopted these patterns to take advantage of somespecific cyber insuranceofferings  whichwill allow you to nowhave more options for that risktransfer and risk management Along the evolutionpath of shared fate in addition to changing how youas customers can manage risk we see paralleltransformation opportunitiesin other key tracks as well Today many customersin public cloudhave complex experiencesfor security and governance And we see anopportunity for usethe jobs to be doneframework to change theseinto much more consistent andcomplete end to end journeys Similarly  manycustomers in public cloudhave disjoint experiencesmanaging policy and governancefor both their individualinstances in public cloud as well as across publiccloud and on prem We feel  again  thatthere is an opportunityto change this fundamentallyinto a much more coordinatedand verifiable experience Blueprints play a keyrole both in movingproduct security from abolted on after effectto a built in part ofthe underlying solution and also from movingplatform securityfrom a set of complexconfigurationsto increasinglysecure by default As we think about the stackfrom a customer perspective as we talked about we first buildon top of that secure coreGoogle infrastructure And with the next layerof security productslike SCC Premium  AssuredWorkloads  Organization Policy VPC Service Controls givingyou additional capabilitiesdifferentiated in theirability to help you manageyour security posture your visibility and your segmentation These form the lowest layerof the security stack On top of thisbaseline comes boththe foundational andposture blueprints where the foundationalblueprintsinclude org policy  resourcehierarchy  networking access segmentation logging  detective controlsto allow you to takeadvantage of proven designsto build your foundationalinfrastructure And on top of that the posture blueprintscover specific sets ofservices aggregated together And both the individualservice specific controlsas well as theintraservices controlsnecessary for theworkloads to work properly The blueprints comein three tiers And they differ really intheir level of native supportand their lifecycle management The three tiers aresupported blueprints verified blueprints  andcompatible blueprints Supported blueprints arecreated by Google security teamsand managed as products withregular six month refreshesand attestation and testingof the functionalityand resilience  as well asthe interoperability testingbetween them Verified blueprints can becreated by Google teams partner teams  or customers The key is that the lifecyclemanagement  as well asa station fit for purpose are the responsibilityof the creating team Compatible blueprintsare just that They re compatible I mean  they can becreated by anyone And their requirement is thatthey need to be from a formatand also a stylestandpoint correctlymatched to be operated with ourblueprint tools and a blueprintengine And finally  on top ofthe posture blueprints we have security solutions And these solutionsaddress specific use casesand further add the wrappingsof go to market collateral  bestpractice  application notesso that you can adopt thesefor your particular use cases We focus today really on thesecurity foundations blueprintsas well as the securityposture blueprintsand their relationshipto each other We think about the corefoundational blueprint The key is to allowyou to start up frontwith best practices embedded in It creates a betterstarting pointas well for security andcompliance  as the securitybest practices encoded inour foundation blueprintgive you a greatstart to be compatibleand meeting certain requirementswithin CIS  PCI DSS NIST 800 53  aswell as ISO27001 In addition  they allow youto implement key securitybest practices andphilosophies coreto how Google protects itself These key principles reallyfall into three categories  first that all defense isalways executed at in depthand at scale so multiplelayers of protectionseparate something of interestfrom potential external threat They allow you to adoptthe BeyondProd modelfor deployment centricinfrastructure and the abilityto have a zero trustmodel around boththe infrastructureyou build as wellas the applications you build And then also  they de risk thecloud deployment and adoptionby moving you to theseproven patterns thatallow you to move towards amore shared fate relationshipbetween yourselves and Google To dive in a littlebit more detailon the securityfoundations blueprint it provides thatstrong starting pointwe talked about in baseline It s delivered notonly in documentationbut infrastructure as codein deployable terraformthat automates  acceleratesyour ability to use it It encodes those bestpractices and canbe deployed either at anorganization level or a folderlevel And it has a six monthlifecycle refresh We deployed the very firstversion back in August 2020 The second version wasdeployed in April of this year And then these three willbe launching in October And the posture blueprints thatwe ll talk about build on topof it are tested andverified against it One example of thesecurity posture blueprintis the one we re buildingfor secured data warehousing Secure data analyticsand data warehousingare a key reason many customerscome to Google  as we redifferentiated in our BigQueryproduct and the thingswe do in data analytics As you can see here we ve adopted architecturethat has the landingarea and ingest the data warehouse itself the storage and processing classification andgovernance pieces as well as security  posture and detection for threats In addition  there s alsothe optional presentationlayer  which in the case ofthe blueprints we ve built we provide an example inthe form of an AI notebook To dive this a littlebit more detail  whatyou see in the next level ofdetail of the architectureis we ve segmented thisinto four different projectswithin the BigQuery itself andthen separate set of projectsfor the AI Notebook These projects not onlycreate resource separation but are also theirown BPC serviceperimeters  whichallow us to manageboth sensitive andnon sensitive data The ingest containsthe ability to ingestboth batch and streaming dataand then de identificationof all the data so it canbe correctly classifiedinto the non sensitive andsensitive areas of BigQuery We manage the IM structures the data encryption  and alsodata classification sothat using Data Catalog youcan have column level accesscontrol of the BigQuery DataWarehouse And this architecture is basedon proven production instanceswe ve built together withsome of our financial servicecustomers so that we have goodconfidence that the scalesand works well for thoseregulated industries We dive into a little bit more  what is in the BigQuery DataWarehouse blueprint in additionto the terraform itselfand deployable instances it has a set of documentationthat comes with it around  accessible through either thearchitecture center or the bestpractice site  quick starts technical design docs solution guides  the code itselfwith the guide for deploymentof the code   terraform code  overview decks  licensing andpricing models  public talks the blogs  and also applicationnotes for specific use casesas examples to get youstarted more quickly It too is on asix month refresh cycle with the first version justbeing launched here in Octoberand then the V2 targetedfor a Q2 2022 launch Built on top and also availableas a standalone   whatthis means is that with eachof the posture blueprints they are first andforemost able to becompatible with our securityfoundations blueprintand tested from anintegration standpointwith that and the otherposture blueprints But also  they redeployable in standalonein your own Brownfield And then they includethird party attestationsaround the security architecturedesign  the stride threatmodeling as well asbeing pen tested And you ll seewhat this blueprintin Q4 being tested againstNIST 800 53 compliancestandard in detail When we think about thecontrols that are fundamentallybuilt into the BigQueryData Warehouse they fall in three categories Controls that manageconfiguration state  as misconfiguration is one ofthe largest sources of databreaches  weparticularly provideand code best practicesfor configurationof each of the services as wellas integration and interactionof the services within theBigQuery secure data warehouseblueprint Behavioral controlsfocuse specificallyaround the design andarchitecture of the secure datawarehouse to lookat specific threatsfrom either data exfiltration from privilege escalation from things that breakthe segmentation model the built in  and thenenvironmental controls As we talked about before  thesecure data warehouse blueprintcan be deployed either asa standalone  in which casethe environmentalcontrols are fully encasedwith that particular blueprint Or if they are deployed ontop of a security foundationsblueprint  they inheritthe environmental controlsfrom the underlyingcore infrastructure We re now goingto move to a demo We re going to walk you througha fully deployed secure datawarehouse and highlightsome of the configurationcontrols  the behavioralsecurity protections as well as theenvironmental controls Here we have a demoorganization in whichwe ve deployed thesecured data warehouseblueprint in its own folder The secured datawarehouse blueprintconsists of fourdistinct projects  a data ingestion project which includes Cloud Storage Pub Sub  and Dataflow  adata lake project  whichincludes the BigQuery instancefor non sensitive data privileged BigQueryData Warehouse which includes Cloud Storage aswell as the privilege BigQueryData Warehouse instance  anda data governance project which includes Data Catalog key management  and dataclassification through DLP When deploying theseinto your own Brownfieldor on top of thefoundations blueprint you can plug in thedata encryption keysinto your existingencryption key project logging into yourcentralized lodging project and the network connectionsto allow the Dataflowtemplates in throughyour existing system You can also run this standaloneif you want to experimentand play with it And to do that  we includetwo harness projects  whichsupply the Dataflow templateand the terraform sourcing The security perimeters aroundthe BigQuery Data Warehouseand the other projectsare set up as follows We have threeseparate perimeters  one around ingestion  onearound the privileged data and then one arounddata governance In addition  we set upbridges between ingestionand governance  privilegeand governance  and thenprivilege and ingestion If we look at the VPCService perimeter around dataingestion You ll see it includes theingestion project and the datalake  has its owndefined access contextlevels  and specificegress rules Similarly  if we look atthe VPC Service perimeteraround privilegeddata  it capturesthe privileged BigQueryData Warehouse projectas its own access levelsdefined at egress rules And then the last perimeteris around data governance which secures the datagovernance projectand has its ownaccess levels defined If we look now at thebridging  the first bridgeconnects the ingestion  thedata lake and governance The second bridge connects thedata lake to the privileged And then the final bridgeconnects the data governanceto privileged As you can see  we vedefined access policiesfor each of the separateVPC Service perimeters We ve also used our DLP tode identify specifically cardnumber and card pin data so thatin the non sensitive BigQueryinstance  they can be used bylower access levels and folkswith more generalized roles  separate from when thereare the actual valuesare being used in theprivileged data warehouse We use Data Catalogwith policy tagsin order to further providecolumn level control You can see we usecolumn level policytags for person name credit limit  and alsocredit card number So we give you additionalfine grained control of who what serviceaccounts  what humanscan access these pieces ofsensitive data as an example If we now look atthe data lake  whichhas the non sensitivedata set  youcan see that if we look at again  card number and card pinand create a query  yousee de identified versionof both card pinand card number thatis still format preserving If we now move to the sensitivedata and the privileged datawarehouse  if we were to look atthe card pin data  because I have theright set of accesses you ll see theactual card pin datawithin the privileged andprotected BigQuery instance However  to show you that we reusing column level control I m now also going to tryto query the card number And here you seeaccess has been deniedbecause this particularuser currentlydoes not have the IM permissionsto access the card numbers We can fix this by going backand granting the permissionsfor fine grained readeron that particular column And now if we rerunthe query  we llsee we get both the pin andthe underlying card numbers So we ve now shown youthe architecture examplefor the secure data warehousecontaining three VPC Serviceparameters and both datalake for non sensitive dataand a separate securedata set  whichhas column level access managedthrough Data Catalog policytext We ve now shown you a deployedversion of the secure datawarehouse blueprint And we walked you throughsome of the key configuration behavioral  andenvironmental controls So hopefully thisprovides you a viewof how we re embeddingbest practices in securityas well as thearchitecture to give yousomething as a reference starting point  and patternto jump start your workloadsand production around datawarehousing and data analytics To learn more you can explore atcloud google com security bestpractices the rangeof our current portfolio ofblueprints for foundationand for security posture explore the documentationas well as the terraformrepos that we ve supplied and then download kindof reference guides froma step by step basis We d highly encourage youto deploy these securityblueprints into your ownexample  organizations and folders so thatyou can experimentto see how they fitwith your environmentsand definitely encourageyou to customize and usethe module as ascript if they createadditional differentiationfor your particular needs As we talked about these blueprintsare refreshed ona six month cycle So we highly encourage youto also feed back to uswhere you ve seen gapsand things that you d wantto see in future iterations We can put that in thebacklog and many of your needsmay show up in futureversions of the blueprintso you can use a standardblueprint as opposedto one that you ve INAUDIBLE  yourself And lastly  we encourage youto start trialing the SecurityCommand Center Premium  asit provides an overall CloudSecurity posturemanagement around not onlyyour organization butthese multi project andfolder centric postureblueprints as well Thank you so much for yourtime and the opportunityto share with you whatwe re thinking abouton the shared faithstandpoint as well as someof those core blueprints thatallow you to jump start and getstarted with the bestpractices and patternsthat can help yoube more successful STEVE FRANCIA Welcome to my session  Enterprise Ready ModernApplications with Go  The Rise of Digital Chaos  For years  one of themost talked about topicshas been digital transformation It seems like everyone sfeeling the needto modernize their legacyenterprise applications As technology has specialized it has grown exponentiallyin complexity Two decades ago there mighthave been a dozen choicesthat needed to be made  withtypically two or three options Today the amount of choicesis simply staggering Nobody can possiblybegin to understandall that is required 3 4 of CIOs say managingperformance is nowimpossible due tothe IT complexity Security risks have seen asimilar exponential expansion It seems like every day we hearabout another major breach The historical approachto enterprise architecturedoesn t work in the digital age If we don t dramaticallychange our approach and soon  it will be too late Today I m here to sharewith you what the path is We need to modernize ourenterprise applications and a lot of folkshave this wrong They have part of thepicture  but haven tgrasped that truemodernization requiressimplifying our approach not simply containerizing it Who am I I ve spent the last decade anda half leading industry changingorganizations  focusedon bringing simplicityto the chaos As senior vicepresident at MongoDB I led the development of thefirst non relational databaseto enter mainstreamuse in 45 years As chief operator atDocker  I led the effortto simplify containers As a side projectI created Hugo the most secure and highestperformance web engine Today I m one of the leadsof the Go Team  wherewe are building a modern simple Cloud native programminglanguage Since you might notbe familiar with Go I ll first provide a shortoverview of what Go is Then I ll share with youwhat a modern enterpriseapplication requires Lastly  I ll share withyou how Go and GoogleCloud will help you withmigration to modern enterprise Along the way I ll sharequotes from enterprises and how Go helped them Go is a modern programminglanguage designed by Googlefor Google s problemsof building and servingserver side software at scale Google uses Go extensivelyfor a wide range of things from our indexing platformthat powers GoogleSearch to the infrastructurethat Google Cloud is built on Launched a decade ago  Gohas seen explosive growth Today  Go is a top 10language  with broad adoptionamong the world sleading enterprises Go follows Einstein s Maxim  be as simple aspossible  but no simpler As Go is designed formaximum simplicity it is very easy tolearn and to work with Go drives the balance betweenproductivity and performance sharing the friendliness ofdynamic languages like Pythonand the productivity of compiledlanguages like C and Java As a modern language Go was designedfrom the very beginning withsecurity and safety in mind Over the past 10 years  Go hasdeveloped a large and matureecosystem Go is loved by both developersand employers alike ranked as thenumber one languageacross many industry surveys Go is widely loved bydevelopers because itmakes building modernsoftware simple and secure But what does it take to buildmodern enterprise applications First of all  theymust minimize risk They must be fully modern Adding modern components ontoor alongside legacy systemsintroduces significantlyhigher risk and complexity Instead of simplifying things they multiply the complexity You now need to worry aboutmany systems  languages and integration points They must be safe Modern applicationshave to be builtin a way that ensures safety According to researchersat Microsoft 70  of the security bugs thatthe company fixes each yearrelate to memory safety Google found a similar 70 of serious bugs in Chromeare memory managementand safety bugs Legacy programminglanguages were notdesigned with safety inmind  because at the timethere was no suchthing as a cyberattackand bits were at a premium Our environment today is vastlydifferent from the one legacylanguages were designed for Modern languages massivelyde risk applications They must be secure One of the largestrisks systems faceis in the form of lackingsecurity  which resultsin leaks and attack vectors In the constantbattle for security applications need toutilize the latestencryption and protocols They need to run inminimal environmentswith maximum isolation andminimal external dependencies They need to not dependon legacy librariesand environments Modern enterprise applicationsmust adapt to business needs One way they dothis is by utilizingservice oriented APIs Approaching applications witha communication first focusensures that the applicationhas the proper separation scalability  and flexibilityto adapt to new businessrequirements This approach also isolates eachfeature for greater flexibilityand security They must be architected tobe modular and composable Tech debt can be atechnologist s worst enemy This hidden tax slowsdown all development Traditional inheritance basedarchitecturescreate code that isvery tightly coupled and hard to adaptto new requirementsand very hard to maintain Which brings us toour next point  they must be easilymaintainable Teams and prioritiesare constantly changing We need code that is easyto understand and pick upby a constant influxof new engineers We want applications thatrequire minimal maintenancewithout constantruntime upgrades Our final theme is thatmodern enterprise appsmust sustain growth They must be scalableand efficient Data and usage has beenexpanding exponentiallyfor the past five decades With the end of Moore sLaw and increased pressureon computing costs  modernenterprise applicationsneed rapid deploymentand the abilityto scale up and down with ease using resources efficientlyand keeping operating andmanagement costs down They must have aCloud native architecture Modern enterpriseapplications musttake advantage of innovationsin Cloud computing With the Cloud nativearchitecture they must be designed withautomation  monitoring and repair from the beginning favoring managed services They must incorporateevent driven components also known as serverless Event driven featuresmust be properly utilizedto bring cost savings adaptability  and efficiencyto modern enterpriseapplications This requires softwarethat loads instantlywith minimalresource consumptionand minimal dependencies We re now going to gothrough these requirementsand highlight the waysthat Go and Google CloudPlatform will help you buildmodern enterprise applications Go and GCP helpyou minimize risk Go is a truly modern language requiring a completely cleanimplementation  keepingyour application decoupledfrom legacy libraries While there isrisk in a rewrite it pales in comparisonto the larger riskof sticking with legacysystems  or worse maintaining both a legacyand a modern system Go s high learnability productivity  and simplicityminimize the rewriterisk  and often paydividends in massivelyreducing both operatingcosts and technicaldebt  furtherminimizing business risk Zviad  a Senior PrincipalEngineer at Dropbox said   People become veryproductive in Go very fast Our infrastructureis built in Go today and all the new thingswe build in Go  Legacy softwarelanguages were designedfor a different era where safetywasn t prioritized  or worse often compromisedfor performance This trade off made sensein a pre network era but has created massiverisk in today s environment Modern languageslike Go are designedfor today s environment where safety is critical Go is memory safe type safe  and utilizeslow latency garbage collection These features alone avoidover 70  of critical bugsresulting from unsafelegacy languages Bala  a Senior Director ofEngineering at PayPal  wrote  In our tightly managedenvironments where we run Gocode  we have seen a CPUreduction of around 10  versusC    with cleaner andmore maintainable code  Go has been designed to createthe most secure applications minimizing risk asmuch as possible Go applications compiledown to a single binarywithout any local dependencies It s not uncommon tosee an application builtusing only the standardlibrary  or only a couplewell vetted Go dependencies Go s dependency managementuses an industry leading tamper evident transparencylog  with builtin tooling that ensuresthat your dependencies areexactly what you expect Go has nativeencryption  which isused to power muchof the internet including key componentsof Google and Cloudflare Adopting Go means thatthe next Heartbleed  whichimpacted virtually everyapplication writtenin any other language  won timpact your application Go even supportsdistroless containers where there are zero localdependencies to worry about GCP  CI CD  and ArtifactManager have direct accessto Go s VulnerabilityDatabase and can provide youinstant warnings aboutsecurity threats Finally  Go and GCP will helpyou adapt to business needs Service oriented APIsprovide enhanced agility scalability  and reliability allowing your businessto adapt to market andstrategy changes quickly Go is the only languageexplicitly designedto build service oriented APIs Go has native supportin the standard libraryfor all you need to buildperformant and secure APIs including ahigh performance web server This is Go s mostcommon use case with over 70  of Go developersusing Go for building APIs Matt  a Principal SoftwareEngineer at Monzo shared   Go is a perfectlanguage for creatingmicroservice architectures The concurrency features and the language in general has allowed the easy creationof small and simple networkservices at Monzo  A unique feature of Go isthat its application structuredirectly mirrorsthe architectureof service oriented systems Services are designedindependentlyin a share nothing architecturecommunicating across networks Go applications are composedof share nothing modulescommunicating across channels a native feature of Go This means that both yourservice and your applicationcomposed of manyservices are usingthe same architectural pattern The benefits ofagility  independence and easy maintenanceare twice as impactfulfor Go based services maximizing your abilityto adapt to business needs There s an additionalcognitive benefitfor developers of using asingle architectural patternat every level of your system John Graham Cumming the CTO at Cloudflare shared about Go   I camefor the easy concurrency I stayed for theeasy composition  Kubernetes creator  Joe Beda tweeted   Kubernetes successis in part due to Goand the Go community Go hits the sweet spot fordistributed system software  High maintenance costsensure that your technologycan t properly adapt orsupport business needs Go is unique among all languagesas it has been designedto minimize maintenance costs Rapidly evolving teamsare common at Googleand virtually all enterprises Go s simplicity  easyreadability  and single binarydeployment ensures thatGo applications haveminimal maintenanceand that it srelatively easy forsomeone new to a code baseto safely maintain it Martin  a softwareengineer at Trivago spoke of this hiddenbenefit of Go  Go s simplicity and itssophisticated toolinglet us scale not only ourservice  but more importantly the process of softwareengineering itself Reducing the friction ofonboarding and training someonehas a significant impact onthe company s productivity even more so at aconstantly movingenvironment like Trivago  And Benjamin Cane  VicePresident and PrincipalEngineer at American Express shared   After working on Go most of ourdevelopers don t wantto go back to other languages  Go and GCP will help yousustain business growth Historically  CTOs haveneeded to make an impossibletrade off   either choose todelay launch as the team buildsa scalable application first  orchoose to ship a slow prototypeearly  acknowledging thatdevelopment will be delayedsignificantly later as adoptiongrowth demands a rewrite  effectively havingto choose whento be unable to supportbusiness growth Go has eliminated this dilemma Go is designed for Google Scalewith built in concurrency thattakes full advantage of today smulticore machines and CloudServices Go enables programmersto quickly developapplications that performat scale  no rewrite needed Eric  a Software EngineeringManager at MercadoLibre wrote   With Go  the companyobviated 88  of their serversand cut CPU on theremaining ones in half producing a tremendouscost savings  Cloud computing bringscountless benefitsfor flexibility and efficiency critical to sustainingbusiness growth Go is the only modernlanguage that has first classsupport on all Cloud platforms Go is designed forCloud native applications Over 3 4 of the projectsin the Cloud NativeComputing Foundationare written in Go And Go is designed forcontainer deployment with a very smallfootprint  comparedto the  ship the world  legacylanguages like Java  Python and Note Mariano  a SoftwareEngineer at Movio blogged   Our Gomicroservices currently buildin 5 seconds or less  they testin one or two seconds whichincludes integrationtests  and theycan deploy  viaKubernetes  new containersin 10 seconds or less The key factor hereis small images  Among all Cloudbenefits  serverlesshas the largest potential forsustaining business growth with its low cost instant scalability and expanded flexibility Go is an ideal fitfor serverless with broad support onall serverless platforms Go has very lowresource use and nearlyinstant startup timescompared to applicationswritten in legacy languagesthat utilize interpreters Google s Cloud Run is the idealplatform for Go applicationswith seamless deployment Go and GCP help yousimplify the chaos  The biggest impediment toa company s future successis its past success  according to Dan Schulman the CEO of PayPal Today s business environment isdefined by modern enterpriseswith a  move fast andbreak things  culture Enterprises will needto embrace the processof constant transformationto remain relevant Go provides theflexibility and performanceto assist organizations withapplication modernizationefforts  and GCP providesflexible managed servicesto guide your organizationinto the modern era To learn more about Go its primary use case and to read casestudies of enterpriseswho have transformed theirorganizations  visit Go dev Thanks for spendingtime with me today I hope that you velearned something I hope you enjoyedit  and I hopeto see you at a futureconference or oneof the many Go conferences Thank you BRYAN ZIMMERMAN  Welcome  andthank you for joining us today We re going to bespeaking about howto build event drivenapplications on GoogleCloud using Eventarc My name is Bryan Zimmerman product manager at GoogleCloud  focusing on Eventarc METE ATAMEL  And myname is Mete Atamel I m a developer advocateat Google Cloud BRYAN ZIMMERMAN  Beforegetting into the details let s look at ourorchestration suite of productsat a high level Today  as we retalking about buildingevent drivenapplications  Eventarcis the product that helpsyou accomplish this easily Along with Workflows Tasks  and Scheduler Eventarc is part ofa suite of productsthat are fullymanaged  flexible and enterprise ready tohelp you accomplish allof your orchestration needs Eventarc connects toover 60 GCP servicesthrough its integrationwith cloud audit logs In addition  our integrationwith Pub Sub as a sourceallows you to useEventarc for events raisedfrom your own application As far as targets  we currentlysupport Cloud Run in GAand Cloud Run for Anthos and functions in preview Our other productssupport even more targetsthrough HTTP execution Together  these fullymanaged and flexible productshelp developers connectthe cloud with less code In our demo today you will see manyof these products in action  specifically  Eventarc Workflows  Cloud Run and Cloud Functions So what do we mean byevent driven architecture First  let s talk aboutthe modern application Here s an example ofa simple applicationwith a complex web of serviceteams working togethertowards a common goal This application issomewhat complicatedwith interactionsand dependenciesthat can become adrag on velocity But in reality  todayyour applicationis more likely to look likethis  an extremely complex webof point to pointinteractions creatinga significant numberof dependenciesand points of failure As applicationsexpand horizontally connections becomeeven more complexand difficult to understand While microservicepatterns providea great deal ofadvantages  they alsocome with a greatdeal of complexity By contrast event driven architectureproduces a solutionfor this problem It provides the abilityto reduce dependenciesand complexity inyour applicationwhile still allowingthe creation of moreand more microservices Event driven architecture allowstrue separation of concern A producer need notknow how an event willbe consumed or takeon any dependencyfrom a downstream service Similarly  theconsumer only needsto know that the event willbe raised and understandhow to utilize it none of the detailsof the upstream service With this looselycoupled approach we eliminate harddependencies between servicesand eliminate thatdrag on velocity In a traditionalapplication  you can onlyscale one way  vertically With event drivenarchitecture  a developeris easily able toscale an applicationas performance needs increase Furthermore  imaginea scenario whereyou re adding a new serviceto an existing application In a point to pointsignaling model upstream servicesmust be updatedin order to signal thedownstream services However  in anevent driven pattern users are easily ableto add a new servicefrom existing eventswithout updatingthe upstream application at all This helps toeliminate that spiderweb of concerns andpoint to point interactionssubstantially Eventarc is Google ssolution to this problem It is a managedeventing system thatmakes building event drivenserverless applications easy Simply select the GoogleCloud service or Pub Subtopic you re interestedin as a source define the filterparameters  and choosethe target you wish to invoke We do use Pub Sub in thebackground as a transport layerto benefit from its superiorreliability and observability but we take care of allthe details of setup Simply create thattrigger  and we lltake care of the eventingress  the filtering and the delivery of eventsto your desired target Lastly  we use the CNCFStandard cloud events format And because of this  it seasy to understand and marshalthe details of yourevents in orderto more intelligentlydrive application behavior Now I d like to sharean important customerstory from one of ourcustomers  Vivint Vivint is anexciting IoT company They currently powerthe smart home forover 1 8 million customers Their portfolio of devices isimpressive  offering smart homebundles that covereverything from camerasto locks to important sensorssuch as flood sensors As these devices areimportant for ensuringyour home is runningsmoothly and safely latency and reliabilityof the backend systemis extremely important forcreating the right customerexperience Let s talk a bit about thechallenge they were facing In 2020  Vivint was presentedwith an exciting businessopportunity They intended to offer astandalone doorbell cameraoutside of theirsmart home bundle This was an interestingtechnical challenge Their core system you see  was builtaround the concept of bundles so this standalone offeringwas a significantdeparture from that model Lastly  time to marketreally mattered There was a limitedwindow they had inwhich to make thishappen in orderto address thisfast moving market segment Here s what theybuilt  By leveragingevents and serverlesstechnology they were able to separatethe components neededto run the camera applicationfrom the core platformwithout having torewrite everything and without adding any riskor instability to the userexperience of eitherthe new applicationor their core offering Let s examine in detail thearchitecture used for their usecase  or one of theiruse cases specifically an image processing pipelinefor their camera application Images were loaded fromthe camera to a GCS bucket This invoked a CloudFunction or CloudRun for Anthos Service  whichanalyzed the images usingthe Computer Vision API inorder to categorize and annotateappropriately The results wereupdated in a BigQuerydata set for use later And the frontend platform wassignaled via an integrationwith Pub Sub Firebase FCM was used inorder to send notificationsto the mobile app  andtheir core platformreceived the eventfor historical reasonsand tracking The result  Eventarc andGCP surveillance technologyallowed them to accelerate timeto value and time to market Not only did these productshelp solve their problembut did so in a waythat would scale and by achievingseparation of concernwithout risking theircore technology To dig into some ofthe secondary benefits this provided a numberof things that werereally interesting to them First  it decreased the timeto market  as I mentioned The backend system wasdeveloped and ready to goearlier than expected They were also able toimplement continuous deployment They deployed new codeafter every two week sprint This is an improvement over theprevious quarterly OTA updates By using event processingin the same location they were also able to  sorry  in the same locationas the notificationservice  theywere also able todecrease latency by 78  And finally  built entirelyon managed serverlessand orchestrationproducts  the new solutionwas scalable even in themost successful scenarios Vivint didn t have toworry about scalability outof the gate  as theywere able to restassured in the confidenceof Google Cloud s abilityto scale Hopefully  Vivint sstory helps exemplifyhow Eventarc and Google Cloudcan help accelerate timeto value for your organization To help understand how this canwork for you in the real world Mete has created a demo ofa similar image processingpipeline So with this said  I willhand things over to Mete METE ATAMEL  Thanks  Bryan I ll show you a simplifiedimage processing pipeline In this pipeline  users dropimages in a Cloud Storagebucket These images will be processedby a chain of Cloud Runand Cloud Function services We use Eventarc to triggerthe pipeline via Cloud Run and Workflows to orchestratethe services thatprocess the images Let s have a look OK Before running thedemo  let s lookat the architecture diagram First  we have end users savingsome images to an input bucket Once the image is saved this generates an event and this event goes to aFilter Cloud Run service Now  this event is routedusing Eventarc s Cloud Storagetrigger Now the filter servicewill receive the image and it will use Vision API todetermine if it s a safe imageor not If the image issafe  then the filterstarts a Workflow execution This Workflow execution called image processing will call a numberof Cloud Functions The first Cloud Functionis called Labeler Labeler will receivethe image and again  itwill use Vision API to extractthe labels out of the image And it will save themto an output bucket Then Workflows will calla Resizer Cloud Function This will take the image  andresize it into 400 by 400 and save it to thatbucket as well And then there willbe another functioncalled Watermarker thatwill receive the labelsfrom the first function And it will also receivethe resized image And it will add awatermark using the labelson the resized image And then it will saveit to the output bucket As you can see  these arethree separate functions but they re orchestratedtogether using Workflows So how do you setsomething like this We ll take a look But before I show you the codeand the setup  all of the codeis on GitHub  and all theinstructions are on GitHub So feel free to checkit out yourself as well Now first  let s take alook at what we need to do So this is the GitHub page And before you begin  youneed to set up the project ID You need to enable the APIsthat we need with Eventarcand Workflows and others Then we need to pick a region And we need to setour Cloud Run regionand our Eventarclocation to that region And then we need to configuresome service accounts because we re going to usethem in Eventarc triggers So we want to make surethat we have a serviceaccount witheventarc eventReceiver role And we also need apubsub publisher rolefor the Cloud Storage trigger And we also want to makesure that our Pub Sub serviceaccount has theserviceAccountTokenCreatorrole Now  once we have all of those  and I already set all of this  we need to create storagebuckets for our images So we create one bucketfor the input imagesand another bucketfor output images So this is where the imageswill be saved by the users and this is where theprocessing will happenand the output imageswill be saved as well All right Then we can startdeploying our services If you remember  we hadthree Cloud Functions Watermarker Resizer  and Labeler that are orchestratedtogether in a workflow So let s look atWatermarker first Watermarker is a C  function but it can be in any language And to look at the codequickly  what it doesis that first  we read thebucket name as an environmentvariable This is the outputbucket where we regoing to save the imageonce we process it Then we receive HTTPrequest in this method And from the HTTP request  weextract a couple of things First  we extract the bucketand the file informationso we can download the image And then we alsoextract the labels thatwill be passed in by Workflows And we re going to use theselabels to add a watermark Then we download theimage from the bucket Then in this piece of code  weuse an image processing libraryto use the labels and add awatermark on top of the image And then finally we save the imagewith the watermarksto the output bucket All right Now  to deploy this servicewe just use gcloud deploy We specify theruntime as dotnet3 We make sure that this isan HTTP triggered function And we also pass in theenvironment variablefor the bucket so that thefunction knows how to save  where to save the image So once this is deployed we can move on to Resizer Resizer is very similar tothe previous Watermarker But instead of adding awatermark  it resizes And we deploy it prettymuch the same way You can check outthe code if you like And then Labeler is  again very similar function Instead of resizing the image it extracts the labels outof the image And it uses VisionAPI to do that So if we take a lookat the code quickly you can see that the Labelerwill receive the bucketand file information then it willcall this ExtractLabelsAsyncmethod with the storageURL of the image And if you lookat this method  itbasically uses theVision API clientand calls the DetectLabelsAsyncto extract the labels Then we order thelabels by score And we return them ina list that we use So once we return thelabels  what we dois that we first savethese labels in a textfile to the output bucket And then we also extract thetop three of these labelsand return them asHTTP response data And we re going to use thisfrom Workflows as well OK So that s Labeler And now we need to putthis into an orchestration And we do that with Workflows And to define the workflow  wecreate a workflow yaml file And then we ll define whathappens in that YAML file So let s take a lookat that quickly here So in workflow yaml file  firstit receives some arguments This will be coming fromour Cloud Run servicethat I m going to show you next And in these arguments we have the informationabout the bucket and the file And we also pass in theURLs or our services Because we don twant to hardcodethese URLs of thesefunctions  so insteadwe passed them in as arguments Then we have our steps In the first steps  we log ourbucket and file informationjust to make sure that weare on the right track Than in the labelstep  we make a callto the Labeler URL  which isthe URL of the Labeler function And we pass in thebucket and file And then this callsthe Labeler function And then we get theresponse from the functionand save it tolabelResponse variable Then we do the samewith resize step So in the resize step you just call the Resizerand capture the responsein resizeResponse Then in watermark step we call the Watermarker but we pass in the bucketand file informationfrom the resize step This is because we wantto add the watermarkfor the resized image  right So because of that  we passin the resized image s bucketand file information And we also pass in the labelsfrom the label response Because these are thelabels that we extractedfrom the Labeler function  sowe are passing them in as well And the Watermarkerwill use those labelsto add the watermark And then in the final step we return the HTTP statuscode from all thesteps  just to make surethat our workflowcompleted successfully OK So that s workflow And to deploy ourworkflow  all we need to dois just use gcloudworkflows deploy So let s just copythis  and go back here and deploy our workflow So this will look atour workflow yaml And now it s deployingour workflow  whichis very quick  as you can see Great Now the next step isthe filter function So if you go back to ourarchitecture diagram we deployed our functions we deployed our workflows Now we need to deploy theFilter Cloud Run service And we need tomake sure that it sconnected to CloudStorage eventsusing a Cloud Storage trigger Now  to deploy the filter this is a Cloud Run service So it s   we need a Docker file and we need to build and pushthe image So that s what weare doing here And then we also deploythe Cloud Run serviceusing gcloud run deploy and point to our image and pass in someenvironment variables These variables  like theproject ID  region  workflowname  this is the informationneeded for the workflow But we are alsopassing in the URLsor our functions  theLabeler URL  the Resizer URL That way  we don t need tohardcode them in our workflow Now  this  I already did this But now we need to connect CloudRun service to Cloud Storageevents And we do that withtriggers in Eventarc So we create atrigger name  and thenwe just create a triggerwith this command Here we are saying that thistrigger connects this Cloud Runservice in this region And it s filtering these events These events  as you can see it s the Cloud Storage objectfinalize event This is the event that getstriggered when you save a fileand we re alsofiltering on the bucket So let s copy this aswell  and go back hereand create our trigger So this will createthe Eventarc trigger And now everythingpretty much set We have our workflow We have our Cloud Run We have our trigger Now  to test  we need to dropan image and see what happens So let s make surethat our trigger iscreated  that it s created And if I do gcloudeventarc triggers list we can see that thetriggers are running Now  to test this out  let sgo to our Cloud Storage bucket And you can see that Ihave an input bucket So let s upload acouple of pictures here So I will upload onepicture of myself And then I ll upload anotherbeach kind of picture So let s upload these And now this startsup the whole chain And if we go tothe output bucket we should startseeing some images So now I will wait a little bit So you can see that we have somelabels for the first picture So if you click on the labelsand look at the actual file these are the labels that wereextracted from my picture Now the next one we see isthe picture that s resized This is the 400 to 400 picture so it s resized to 400 to 400 And finally  the thirdpicture is the resized imagewith the watermark And the watermark isjust the top three labelsfrom the image And just to show theother image as well if you look atthe beach  you seethat it has water  cloud and sky as the watermark So this concludes the demo I hope you enjoyed it Thanks very much BRYAN ZIMMERMAN Thank you  Mete It is great to see ourproducts work togetherto help customersconnect to cloud with less code of course Today  we vediscussed what we haveto offer in our orchestrationand serverless products  howour customer Vivint hasused this to accelerate timeto value  and even shown ademo that you can try yourselfvia the GitHub link that Metehas provided to gain experiencewith the concepts To learn even more  thereare several other sessionsthat can teach you about otherrelated serverless technology And of course  pleasevisit us at google com Thank you again forspending us time today I hope you enjoyed itas much as we have JENNIFER SMITH  Dear everyone I hope you re enjoying Next  21 Welcome to oursession on extendingAnthos to manage VM workloadsin on premises environments I m Jennifer Smith  Directorof Product Managementwithin Google Cloud Anthos I m also joined today by mycolleague Amr  Senior ProductManager for Google Cloud Anthos Amr  want to sayhi to everybody AMR ABDELRAZIK  Hi  everyone Thank you for joining us We re excited to show yousome of the cool things we vebeen working on over thelast couple of months JENNIFER SMITH  Thanks  Amr We ll talk to you again soon And like Amr said  we re reallyexcited to share with you someof the recent work we vebeen doing to help enterprisecustomers furtherwhen they navigatetheir hybrid andmulticloud applications I know virtual presentations andconferences have been our normfor longer than we allmight have imagined so we ve constructed oursession today as part talkand part do via a product demo So let s get going Customers areincreasingly lookingto modernize  whether thatmodernization is in place  liftand shift  or viacontainerization An overwhelmingmajority of enterpriseshave a hybrid andmulticloud strategy Wanting to modernizewith consistencyacross all environments isn tjust a nicety  it s crucial And yet even with thisincreasing desire to modernize only about one sixthof enterprise workloadsare containerized That leaves many millionsof non containerized VMbased workloads stillin the data center How can we include these VMsin modernization efforts We ll come back to that soon Let me start by providinga brief overview of Anthos Anthos is Google s hybridand multicloud platform providing a consistentoperating system for Cloudnative deploymentsacross all site types whether that s on premises at the edge  or public Cloud It gives you all the tools youneed for consistent operations service and policymanagement  so that you canbuild once  and run anywhere Starting withcontainer management it s a reliableand efficient wayto deploy and run containers leveraging our experiencewith Google Kubernetes Engine the industry leading containermanagement platform On top of this sits theservice management layer It s a managed service meshoffering to connect  manage and secure VMs and containers This is based on Istio And on top of that is theapplication development layer comprising a wide selection ofproduction ready developmentsolutions and servicesfrom both Googleand our partners all availablethrough the Google Cloudmarketplace The layers are flanked on twosides by policy management offering a central placeto manage configurationand security policiesacross your fleet using the power ofKubernetes along with getopsto implement an infrastructureas code approach to your Cloudnative network  and on theother by operations management offering an integratedlogging and monitoringservice for containers services  and applications There were a few reallyimportant attributeswe felt were essential tobuilding this new platformto help customers withtheir hybrid and multicloudapplications We wanted it to be opensource  declarative in nature developer friendly and extensible Because of this we chose Kubernetesas the basis of ourAnthos platform And as we ve been workingwith many Anthos customerson their modernizationjourneys  we vereceived some veryimportant feedback Customers have shared with ussome of their key challengeswith modernization  startingwith the simple fact that thereare many workloads that cannotbe easily or immediately movedto containers orVMs in the Cloud They also haveorganizational  skills and process gaps betweenCloud native and traditionaloperations Their infra teams lackunified management  securityand monitoring forVMs and containers And their dev teams areconstrained and usingmodern application developmenttechniques and toolswith traditional VM workloads Delving a little bit deeper  theroot causes of these challengesare long  multi stepprovisioning fragmented observability manual compliance enforcement and an expensiveproprietary ecosystem So what can we do Currently  there arethree Cloud centricand one on premises optionfor modernizing existing VMapplications You can migrate virtualmachines to Google CloudVMs with Migratefor Compute Engine We can lift and shift yourentire virtualization stackwith Google Cloud VMware Engine You can migrateyour VM workloadsto containers on Cloudwith Migrate for GKE Or you could decide to modernizeto containers on premiseswith Migrate for Anthos We support all of theseincremental VM modernizationstrategies  but our customerswant another option They want a way to leaveapps in VMs until and if theycan be migrated  while gainingthe same benefits of runningthose VMs on Kubernetesand mapping those VMsusing a consistentKubernetes style approachto provisioning  configuration monitoring and logging security  and optimization So today we re adding anew on premises option Leveraging Google sleadership in Kubernetes you can modernizeVM applicationswithout needing to containerizeor move to Cloud right away and you can do this nowwith Anthos for VMs Anthos for VMs letscustomers apply the expertisethey re building and running inmanaging containers to their VMenvironments to build bridgesof automation between containersand VMs and to reducetheir spending on VMplatforms and management tools With this new option  we providethe flexibility of choiceto modernize in a way thatbest meets your business needs So how does itactually come alive We ve created twoadoption models  Shifableand Attachable In the Shiftable case VMs run on Anthos In the Attachable scenario we manage other hypervisorsVMs with Anthos You can choose betweenthe two approachesor use them both simultaneously You get unified management declarative deployment and policy enforcement foryour VMs and your containers You bring a Cloud likeexperience to your VMsand can leverage Cloud nativeservices for your VM workloads Moving to a bit of a deeperlook at the technologiesthat enable the Shiftable andthe Attachable approaches First  we see the runtimelayer with Anthos VM runtimebased on Kubevirt inthe Shiftable scenario In the Attachable track we use VMware vSphere Next we have network securityand policy via Anthos ServiceMesh based on Istio And moving up the stack  we haveKubernetes APIs and an Anthosconfig and policy management At the top of the stack  wehave Cloud native capabilitiesand services  such as patchmanagement  databases and AIML So with Anthos forVMs  we help easemany of themodernization challengesour customers haveshared with us Using Kubernetes  we reable to provide fast  secureinfrastructure provisioning unified operational visibility automated compliance and cost reduction I m gonna pass it over toAmr now  ask him to come backand join us and help explainhow we can get started And you can get startedwith answers for VMsand run us through whatthe demo will look like Amr AMR ABDELRAZIK Thank you  Jennifer So the first step isto do a fit assessment This is a tool that actuallyunderstands your infrastructureand tries to understandwhat is the right migrationor what is theright modernizationjourney for your application It runs as astandalone tool or youcan run it on top of anexisting Anthos clusterif you re an Anthos customer And it does an assessmentof the VM workloads And it decides on ascore that tells youwhat is the right journey whether it s Shiftable you want to be able to migrateit to the Anthos VM runtime if it s Attachable  connectingthis VM to Anthos ManagementLayer  or if it sContainerizeable whether this VM iscontainerized or not And if a VM has multipleoptions  it lays that out too And there is a fit score thatalso gives you the effort and if there isany data that youneed to address  for example in order to move from one pathto the other From a workload perspective  youdownload the standalone tool you connect to existing vCenter It generates a local reportthat  optionally  youcan upload to the Cloud fora detailed visualizationor historical view We ll go to thenext slide  please This is how the fitassessment report looks like It provides you a data centerwide readiness  as I mentioned Actionable VM level suggestions ROI analysis  and risk metricsto every migration And now next we have a demo  and we re very excitedto show you that demo We ve been working onsome of these technologiesfor a couple of months So just to describe the demo the first part of the demotalks about the discovery andassessment using the mfit toolthat we just mentioned And then we re going to showcasetwo scenarios of ShiftableVM and Attachable VM that havebeen migrated or attached And then for day two we regoing to talk about managementfrom Cloud Operations  AnthosConfiguration Management and Anthos Service Mesh So with that  let smove on to the demo SPEAKER  All right  we renow ready for a quick demoof Anthos for VMs Here we see theAnthos landing page This is the centralinterface whereyou can view yourregistered Anthos clusters alter your services withAnthos Service Mesh or manage legacy workloadswith Anthos for VMs  new VMManagement Capabilities The critically important firststep when using Anthos for VMsis to run a fit assessment onyour legacy VMware environment determine which VMs can beeasily containerised  shifted or attached Anthos for VMs  fitassessment toolingquickly scans yourVMware environmentand collects application hardware  networking and storage metadatarelated to each VMware VMto systematically determinewhich category it belongs to Once its assessmentis completed the results are displayedin a report that explainseach recommendation in detail OK  now with thatexplanation out of the way let s run a fit assessmentand see the results Here we go in Terminal  it hasAnthos for VMs fit assessmenttooling installed on it After we run a single commandand authenticate our VMwareenvironment  we ll seethe assessment tool startto rapidly scanthrough over 2 000 VMs and then upload it s resultsto the Google Cloud console Once a report hasbeen generated we see that we veassessed 2 077 VMs 15 of them were categorizedas being excellent candidatesto attach to Anthosfor VMs with an agent and the rest weregreat candidatesto be hosted onKubernetes using Kubevirt In particular  wesee detailed resultsfor three legacy VMs  afront end  a back end and a transaction service We ll examine a few ofthese results in more depth And later on in the demo we ll use Anthos for VMsto both shift andattach these legacyVMs to Anthos  modernKubernetes based control point Now opening the result forlegacy front end service we ll see that it has beencategorized a shift candidate due to its Kubevirt compatibleapplication profile Opening the result for thelegacy transaction service we see it as being categorizedan attach candidatedue to the fact that it requiresmultiple network interfaces OK  now it s time to generatethe migration artifactsthat Anthos for VMs will useto take control of these legacyVMs Here we see amigration job calleddemo1 is already underway Clicking on CreateMigration will show usthe prompt that wasused to create this job By filling in just a few values such as migration source operating system typefor the source VMs and the desired output frommigration  like containerizeor convert to a KubevirtVM  one can easilycreate a VM migration job Examining our in progressmigration job more closely we can see that it s currentlypreparing some VM managementartifacts Switching back to theGoogle Cloud console we see an empty table After clicking theRefresh button we see that our Anthos clusternow contains the new Kubevirtand Istio custom resourcesthat are just createdby Anthos Config Manager We navigate tothe Workloads tab we can see at the Kubernetespods associated with the twolegacy VMs that we shift toKubevirt are now ready as well Click on of theseworkloads takes usto an overview page that showsreal time metrics and events Clicking on theOperations buttonopens a side panel  displaystons of additional information like regular metrics andlogs specifically designedto assess applicationreliability Next we shift our focus to theAnthos for VMs Service Mesh Here we see the entireservice topologyof our example application Our example appgenerates client trafficthat is sent to a moderncontainerized front endservice This front end servicein turn sends a requestto a few other modernAPIs and our three shiftedand attached legacy VMs The front end serviceis highly available It sits behind a loadbalancer  which spreads trafficacross five service replicas Request to the modern front endmade to a few particular endpoints are automatically routedto the legacy shifted from it Similarly  themodern front end alsoroutes a few specific APIcalls to the legacy shiftedbackend service The legacy attachedtransaction serviceis called both by the shiftedback end service and a numberof other modern APIs Double clicking on theshifted back end servicetakes us to a detailed viewof its local service topology where we can see info onits current performanceand resource utilization and lots of other stuff More importantly  now that ourlegacy VM is managed by Anthos we can now use Anthosobservability toolingto make the old app awesome To demonstrate this we re goingto go ahead and create an SLO AKA a Service Level Objective First  we select anavailability based SLI Here we ll set our complianceperiod to one week and a performance goalto 99 5  availability Finally  we click Create Next we ll add analert to our SLO Here we ll just select allthe default settings  and thena short message Awesome We ve just sprinkledsome Google SRA magicon our legacy application And now we ve reachedthe end of our demo We ve seen how Anthos or VMs canbe used to consistently managelegacy applications usingKubernetes awesomeness and slick Google tooling Enjoy AMR ABDELRAZIK  We hopeyou liked the demo And with that  I llhand it over to Jenniferto tell us how we can learnmore about Anthos for VMs JENNIFER SMITH  Thanks Amr Thanks for sharing howour customers can getstarted with Anthos for VMs Like Amr said  we would loveto work with you as well Anthos for VMs iscurrently in preview and we are working with a groupof customers as our designpartners If you re interested in ourpreview  or learning more please contact us at this email We would really loveto talk some more Thank you very much Have a great day And enjoy the rest of Next  21  MUSIC PLAYING APARNA SINHA  Hi I m Aparna Sinha Welcome to day twoof Google Cloud Next I ll be co hosting todayalongside Urs Holzle And we ve got a really greatlineup of new technologyand demos from Google Cloud But before we getstarted  I wantto thank you all for takingtime out of your busy weekto be here with us Yesterday was an incrediblefirst day at Next Thomas and Sundar madesome amazing announcements This week  we re releasing over100 new products  services and programs for you We re kicking off day twonow with this keynote A Cloud Built for Developers After this  we ll jump rightinto a live developer Q Awhere you can ask us anything And then tomorrowis Community Day And that ll be totallydedicated to youfor learning discussions  networking and all kinds of fun All right  let sget things startedby welcoming Urs  Hi  Urs URS HOLZLE  Hi  Aparna Hi  everyone I hope you are as excited asI am to get started today And since this is adeveloper keynote we ll kick it offwith our first demoright away from Google Researchand our DeepMind colleagues who have been working on someamazing voice technology SPEAKER  Welcome to Next  21  NON ENGLISH SPEECH URS HOLZLE  Now  beforeyou get too impressedby my language skills  I didnot actually speak these words What you just heard is acustom text to speech modelthat has trained onmy voice and thatcan generate synthetic speechin different languages And our teams havealready startedusing this technologyto improve our voiceexperience for all users APARNA SINHA  That s right Google s Project Euphonia isalready using custom voiceto help people withatypical speechto communicate andbe better understood Technology like this createsa more inclusive world To find out moreabout this  check outthe Project Euphonia website Custom voice is available todayfor select Cloud customers We re very mindful ofthe potential misusesof this technology  andwe re taking great careto prevent them by reviewingeach use case uniquely URS HOLZLE  Now look around you Every day  you seeinnovation that sbrought to you bydevelopers like yourselfwith persistence and skill And Google has a long traditionof supporting developersin open source and elsewhere For years  you ve beenusing technologieslike Kubernetes  Firebase TensorFlow  Go  Angular gRPC  and many others And when we built Google Cloud we built it for developers And we were inspired by all thethings you ve created with it And our job is  quitesimply  to make it easierfor you to do what you love So we focus on making you asproductive as possible withthe least amount of effort And so in everything we design we take all the feedbackthat you re sharing with usand build a cloud platformthat just works  whetherthat s by natively embeddingkey security or sustainabilityfeatures into the platformitself  or by featuringpartner solutions rightwithin our own console We re reallyfocused on one goal giving you the best developerexperience of any cloudprovider APARNA SINHA  Google Cloudhas had tremendous tractionwith digital native customerssince our very early days How have you seencustomer and partneradoption evolve since then URS HOLZLE  Well  manyof our biggest customersare cloud natives But we ve seentremendous adoptionby a broad segment ofenterprise customersin traditionalindustries as well like entertainment orfinancial services For example  MajorLeague Baseball which is North America soldest and most attendedprofessional sports league is using Google Cloudto modernize fanengagement and to increaseoperational efficiency And Equifax  whichwas founded in 1899and is one of theworld s largest consumercredit reporting agencies is transforming itselffrom a credit bureau to anext generation data analyticsand technology companybuilt on Google Cloud APARNA SINHA  We veseen a huge shift Essentially every companyis becoming a tech companyto increase theircompetitivenessand establish leadershipin their industries Developer talentand cloud servicesare at the heart of this shift Whether you call that digitaltransformation or somethingelse  companies ofall sizes are findingthat Google Cloud isoptimized to help makeyour data  yourapplications  and your talentmore useful and relevantto your business URS HOLZLE  Exactly And as Thomasmentioned yesterday everyone needs to bethinking through how they llfundamentally shift intoa technology companyto serve their customers in themost meaningful ways 10 yearsfrom now And you as developers arekey to making this happen So organizations askthemselves  do wehave the most cutting edgetechnology to becomea leader in our industry And you  the developers  arewell equipped to answer this And we are superfocused on making youand your company successful And there s twoareas we focus onto support your growth  first  of course making it easier for developersto get their job done  second investing in thedeveloper communityso that everyone can learnand grow from each other So let s talk abouthow Google Cloud ismaking it easier for developersto get their job done From our transformationalinfrastructurestack to our deep innovationsin data  security  ML every feature we release startswith simplifying the developerexperience Take our open cloudexperience  for example We recently expandedour compute stackto include Tau VMs that deliver42  better price performanceover any other comparablesolution in the marketwith no recompile And thanks to our zero trustapproach to security Google Cloud was rankeda leader in IaaS platformnative security by Forrester well ahead of the competition And of course we re focusing alsoon managed services that make iteasy for you to deploy  scale and manage Kubernetes clusterson the edge or in the cloud And Google Cloud has themost complete and most securecontainer experiencefor developers In fact  the 2021 GartnerSolution Scorecardfor Google KubernetesEngine gave GKEan overall score of 92  again well ahead of the competition And of course  withGKE and Anthos you can run thesecontainers anywhere  on premise  other clouds on the edge  anywhere So it s fair to say that inthe years since Google inventedKubernetes  containershave really completelyrevolutionized IT operations Now  recently  Europeanfilmmakers from Honeypot iocreated a documentary onthe history of Kubernetes And it will be outin January 2022 And you are thevery first audienceto have a look at thetrailer right now So let s roll that  VIDEO PLAYBACK   Do I have to lookat you or the camera  MUSIC PLAYING   2013  it was clearthat cloud was a thing but most folks were focusedon infrastructure cloud   The dirty secretfor a long timewas people who were eitherbuilding their own data centersor using colos There s a huge resource waste   And so at thatpoint  automation toolsare all the rave People are now trying toabstract away the servers   Google was lookingfor ways to applyits internal infrastructureexpertise to the cloud   As we started looking attechnologies like Docker we were impressedby the strengthof what they daccomplished in solvinga very specific problem   This is going to happenwith us or without us   Google had to make a boldmove in the cloud space to bethe long term winner   Every big startup I felt like had a containerorchestration project And half of them wereannounced at DockerCon 2014   Open source is mostsuccessful when it s playedas a positive sum game  MUSIC PLAYING  END PLAYBACK APARNA SINHA  This issuch a great community URS HOLZLE  Yeah  absolutely I recognize a lotof the faces  and Ican t wait to see the film Now let s get back to howwe re making it easierfor the developer  foryou  to build reallythe leading technologycompanies of tomorrow Now  Kubernetes deploymentscan involve a fair bitof manual configuration clusters  nodes  loadbalancers  YAMLs  et cetera  butnot on Google Cloud  because weoffer you the most automatedand secure Kubernetesexperience available With GKE Autopilot  Googleprovisions and managesthe cluster s entireunderlying infrastructure including control plane node pools   INAUDIBLE   And that lets you focus onthe higher level servicesand applicationsthat you re building Nobody else offersanything like this Because beyondmanaging node upgrades GKE Autopilot also automaticallyconfigure security featureslike Shielded GKE nodes  secureboot  and workload identity And it also implementssecurity best practicesby blocking less safe features like external IPs or legacyauthorization So you don t get atoy Kubernetes clusterwith GKE Autopilot You get a sophisticatedcluster thatuses the best practicesbrought to you by the team thatbrought to youKubernetes itself so you re always up to date And you get the sameresults as the expertswithout having to be anexpert  too  yourself APARNA SINHA  Thepandemic put developersin the driver s seat  andyou all drove GKE usageto all time highs At the same time  wesaw explosive growthin the use of Google Cloudserverless offerings especially Cloud Runand Cloud Functions It s mainlyenterprise developerswho have driven this growth Cloud Run excels atdeveloper experience It s earned the highestcustomer satisfaction ratingamong developers as measuredby user research international Cloud Run combines thebest of both worlds bringing you serverlessand containers There s no cluster toset up or configure so developers are able toscale seamlessly and securely Under the hood  Cloud Runscales container instancesin isolated sandboxes Any access outside asandbox is mediatedby network controls  or Identityand Access Management  or both And it isn t just for new apps Cloud Run supportstraditional workloads like Java SpringBoot and asp net We also recently introducedcommitted use discountsto lower the cost at scale and we ve introduced alwayson CPU  which enablesasynchronous and backgroundprocesses to beused on Cloud Run So you have all thebenefits of serverlesswithout the restrictions The theme here is easier  moresecure development  especiallywith remote work URS HOLZLE  You reabsolutely right We ve been focusing on remotedevelopment for some time now but the pandemic has certainlyaccelerated the shift Now  what would be moreessential to remote developmentthan to be able to usethe full power of GCPright from your laptopwith zero local setup Cloud Shell Editor is a contextaware remote developmentenvironment that lets youdevelop and manage applicationssecurely from any browser It supports languages  like Go Java  node  Python  C sharp And it comes with an integrateddebugger source control APIExplorer And if you want to testlocally on your laptop it also comes with localemulators for Kubernetesand serverless APIs APARNA SINHA  Thanks Urs  Next  letme introduce you to Abby Carey She is going to show us howGoogle Cloud makes it easyfor you to securely buildmodern applications  again right from your laptop Hi  Abby ABBY CAREY  Hi  Aparna We developers havehad a hard timewriting  extending  deploying and operating applications But it doesn t haveto be difficult Let s start withCloud Shell Editor It comes with current versionsof your favorite DevTools like Docker  Minikube Scaffold  and more There s nothing to downloador install locally Tutorials are built intoCloud Shell Editor  whichmakes it easy to come upto speed on complex topics like GKE APARNA SINHA  So no moreswitching between tabs  Docs your terminal  and your code This integratedexperience is highlydifferentiatedfrom other clouds You can even offeryour own tutorials and that allowsyour organizationto share best practices andon board new hires faster ABBY CAREY  Anotherpopular featureis Kubernetes YAMLauthoring assistance Let s say  I want to add YAMLfor a service to this project I compress ControlSpace  and then findthe Kubernetes service snippet Now  I can tap throughand fill everything in I also get auto completes And if I happen to makea formatting mistake I am notified that there san issue in real time APARNA SINHA  Now many of you preferto work locally in an IDE This same YAMLauthoring assistance it s also available for VS codeand IntelliJ via the Cloud Codeplugin Cloud Code has built insupport for both Cloud Runand Kubernetes ABBY CAREY  In fact  if you reusing Cloud Run or functions you don t need to know Docker You can build and deploy yourapp with just one command because Cloud Code  because Cloud Build isintegrated under the hood This is an applicationwith no Docker file With the new gcloud run deploycommand  all I have to dois provide a namefor my service and then let it know wheremy source code lives  whichis this current directory And we re deploying APARNA SINHA  So nice And thanks to this easeof use  98  of usersdeploy an application toCloud Run on their first tryin less than five minutes ABBY CAREY  I just showed SourceCode deploys the Cloud Run but there are more ways GoogleCloud has made deploymenteasier and more secure First  I can scan mybuild container imagesto check for vulnerabilities I ve already run an on demandscan on one of my imagesusing gcloud artifactsDocker images scan Now  I can copythe ID of my scan and then view my imagesvulnerabilities with the listvulnerabilities command And then  once that sfinished  a severity levelis assigned toeach vulnerabilityto help you prioritize APARNA SINHA  That ssuper important It s really helpful inaddressing security concernsearlier in the softwaredevelopment lifecycle But now  what if your buildpipeline is compromised ABBY CAREY  Forthat  I can enableBinary Authorization on mydeployed Cloud Run services This way  only trustedcontainer imagesare deployed to production APARNA SINHA Binary Authorizationis truly unique in the industry It enables you to put proactivesecurity measures in placeto reduce softwaresupply chain attackrisk by blocking deploymentsthat violate policy And speaking ofdeploying  we re makingit seamless for youto do CI CD securely You can take advantageof serverlessbuild environments within yourown private network with CloudBuild private pools ABBY CAREY  Andfor advanced CD  wehave Google CloudDeploy  which allowsyou to create custom deliverypipelines for your specific usecase and needs APARNA SINHA  That is so cool Well  a real applicationconnects to many supportingCloud Services So  Abby  can youshow us an exampleof how we make theseintegrations easier ABBY CAREY  Sure  whencreating a Cloud Function it s easy to integratewith Secret Manager First  create a secretthat stores your APIkey  which I ve already done Now  I can eithermount it  as a volume or expose it as anenvironment variable I ll mount it  as a volume  andthen I ll name my mount path This will always point to thelatest version of my secret and now  I can securelyreference this API keyfrom my source code This abstraction enablesportability and a betterlocal development experience Cloud Run also integrateswith Secret Managerto make it easier todo the right thingand not put sensitivedata in source APARNA SINHA  Ilove that so much OK  so now  you vewritten your app You ve deployedyour app  and you veconnected your app to otherGoogle Cloud resources What s next ABBY CAREY  Operatingyour app in production With Cloud Ops  youget one integrated viewfor your alerts  events metrics  and logs No more jumpingaround multiple toolsas you try to understandwhat went wrong APARNA SINHA  Thatwas so awesome  Abby Thank you forsharing this with us ABBY CAREY  Thanks  Aparna APARNA SINHA  In eachof these instances we ve done theintegration work for you Because the more work weput into this  the lesswork you have to do and this principleapplies to security as well We ve put a lot of energy intobuilding security nativelyinto everythingwe do  so that youcan innovate with assurance Both GKE and Cloud Runbenefit from the securityfixes we implement beforevulnerabilities are exposed Just think about thefamous vulnerabilityuncovered in how Kuberneteswas handling proxy requests We found it We coordinated andcommunicated the disclosure We fixed it for the entireKubernetes community and we patched all ourproducts before any customerswere impacted More recently cyber threats haveshifted the focus towardsthe software supply chain URS HOLZLE  That s right Malicious actors are trying tocompromise the software supplychain from bad codesubmission to bypassingthe CI CD pipeline altogether And to help solvethese problems we proposed an industrystandard called SLSA It s a security frameworkthat provides common criteriafor increasinglevels of softwaresecurity through automation andthrough cryptographic signingat each stage of thesoftware supply chain And that makes it possible but not necessarily easy So making it easy fordevelopers to ensure securityis super important And that s why we re focusingon building this securityright into thedeveloper tool chain anticipating and preventingissues ahead of time not when you re most at risk So  for example  CloudBuild  our servicethat lets you build test  and deployacross multipleenvironments  such as VMs serverless Kubernetes or Firebase  nowoffers Salsa level onecompliance by default Because Cloud Build gives youa verifiable build provenance So this provenance lets youtrace a binary to the sourcecode that it was builtfrom to prevent tamperingand to prove thatthe code that youthink you re running actuallyis the code you are running Cloud Build is the firstand only CI CD serviceto offer this capability but we go beyond that As you ve seen  buildintegrity automaticallygenerates digitalsignatures  whichcan then be validatedbefore deployment by BinaryAuthorization It s another Google Cloud first So without you needingto do anything we prevent anyonein your organizationfrom deploying code that has notbeen built by your legitimatebuild system Now  ensuring securitypost deploymentis equally critical On GCP  you can enablecontinuous scanning And you can use our servicemesh to embrace a zero trustsecurity model  andautomatically and declarativelysecure your services and the communication So you can manageauthentication  authorization and encryption between serviceswith little or no changesto the applications themselves Let me say that again With little to no changes tothe applications themselves So that means that thesesecurity improvementshelp secure not just new code but also  existing binaries So you can use themfor any applicationthat you re migratingto the Cloud Both Anthos Service Mesh and now  Cloud Build Hybridare available acrossGoogle Cloud and your onpremise environment And they work with VPC ServiceControls and VPC peeringto automate the developersecurity for your enterprise No other Cloud provider protectsyour software supply chainto this level because we startedworking on softwaresupply chain securitylong before it wasin the headlines So by choosing GCP  youbenefit from this leading edgefocus on security APARNA SINHA  Whether we rebuilding foundational OpenSource technologies like Kubernetes or Istio or turning them intofully managed services like GKE and AnthosService Mesh our goal is always to reducecomplexity for our users By helping create theseindustry standards we can provide saferand simpler servicesfor you  the developer And that s exactly our approachto securing the software supplychain We ve co founded the OpenSource Security Foundationwith other technology leadersto create security standardsfor Open Source software And we re starting tobring products to market like Open Source Insights which provides youa complete transitive dependencygraph for many Open Sourcepackages Now  let s turnback to Urs to hearwhy Google Cloudis best positionedto support you inbecoming a technologyleader in your industryusing data as a core asset URS HOLZLE  Thanks Yes  so far  we ve been talkingabout developing and managingcode But data is at the heartof many enterprises So we also have theleading data Cloud productsin the industry designedfor optimal performanceand reliability forapplications of all sizes while scaling toimmense capacity Now  let s start with databases When it comes to databases every Cloud gives you choices They offer SQL databases  whichare great  but unfortunately don t scale And  of course no SQL databases which do scale  butunfortunately  are not SQL Only Google Cloud gives youa third choice with Spanner because Spanner is SQL And  in fact  it justgot a Postgres interface but it scales horizontally And it can literally handle abillion requests per second Nobody else has ascalable SQL system so it s no wonder we reseeing huge adoption Now  on the datawarehouse side  wehave  of course  theleading Cloud data warehousewith BigQuery Hundreds of customers are usingBigQuery at petabyte scaletoday  petabyte each  and youcan run  of course  BigQueryon AWS or Azure On top of that Open Source systemsfor data lake processing like Flank  Spark  and Beam run natively on Google Cloudin a simpler and more costeffective way than inother environments In fact  you canrealize a 57  lower TCOcompared to on premisedata lakes for data scienceprojects Now  on top of thatsavings  our data Cloudalso includes theworld s first and onlyautoscaling andserverless Spark service And finally  Googlehas deep partnershipswith leading data drivencompanies  includingDatabricks  Confluent  MongoDB Redis Labs  and many others So together  we help customersaccess an open platformthat powers analytics atscale  yet  is easy to use APARNA SINHA  Ourpartner communityis central to the healthof our Cloud business and we re especially excitedabout the innovation comingfrom our dataCloud partnerships Together  we ve optimizedour infrastructurefor performance and efficiencyto give our partnersthat extra edge whenthey run on Google Cloud One of our leading datapartners is MongoDB and we have their CEO DevIttycheria here with us today Welcome  Dev DEV ITTYCHERIA  Hi  everyone Happy to be here APARNA SINHA  Dev one of the trendswe re seeing in ourenterprise customer baseis that they re nowcompeting for leadershippositions in their industry bybecoming technology companies How would you say Google Cloudand MongoDB working togethercan help these customersachieve that transition DEV ITTYCHERIA  Well Aparna  the companieswho are in the leadershippositions in their industriesare those who have built theircompetitive advantage usingsoftware and data totransform their business And the key word here is build You can t buy acompetitive advantage You have to build it This means you need to enableyour developers to innovateas quickly aspossible  whether it sbuilding new software toseize new opportunitiesor to respond to new threats MongoDB and Google Clouddeeply understand this Developers chooseMongoDB on Google Cloud because we givethem the tools theyneed to be asproductive as possible including having our servicesavailable in the Google Cloudconsole for easydiscovery and deployment Today  MongoDB Atlasruns in 24 Google regionsacross the world with deeptechnical integrationswith Google sanalytic and AI tools This enables our customers toinnovate quickly and emergeas leaders in their industries As a result  we reseeing explosive growth our customers embracing thetrue value of our partnership APARNA SINHA  That s incredible So when you think aboutthe development teamsat these new customers what s the biggest challengethat you re helping them solve DEV ITTYCHERIA  Yeah  whenyou talk to developing teams you find that theyspend the mostamount of their time tryingto work with data as servingrelevant data at the righttime to the right audienceis critical to buildingany application Unfortunately relational databasesare not designed for the waydevelopers think or code Nor are they designed for scale fault tolerance  or resilience Consequently development teams findit hard to move fast usingrelational databases MongoDB is designed toaddress this problem We make it very easy fordevelopers to work with data And we re able to address themost demanding requirementsfor performance  scale and fault tolerance The partnership withMongoDB and Google Cloudenables developersaround the worldto easily build modern softwareapplications to addresstheir needs oftoday and tomorrow APARNA SINHA  That s terrific Thank you so much forbeing here with us today DEV ITTYCHERIA  Thankyou for having me URS HOLZLE  Yes  thanks Dave  for joining us Now  there s lotsof ways developerscan improve their productivity automate tasks thatare repetitive  masterthe command line use the best tools thatmake your life easier or reuse other people scode just to name a few And another great way toaccelerate your productivityis with buildingblocks  or templates or fully managed services inareas  like machine learning Because on GoogleCloud  you don thave to be an expert tobuild smart applications With new services  like VertexAI  you can build  deploy and scale more effectiveAI models quickly So that lets youdeliver the insightsto your organization thatwill help them create morepersonalizedcustomer experiences run more efficient processes and take that leadershipposition in your industry APARNA SINHA  So with that let s go to our next live demo Joining me todayis Anu Srivastava She s going to show us howthese breakthroughs in AIare advancing Cloud adoptionand redefining the worldof document processing Hi  Anu ANU SRIVASTAVA  Hey  Aparna We all know how towork with data whenit s in a structured format like in a database  some JSON CSV files  or justvariables in my code  right But  what aboutunstructured data Many of the world s businessprocesses start  include or end with a document But these documents canbe difficult to process Think about all the ways youcould enhance your applicationif you could justunlock that data This is where Google CloudDocument AI comes in Doc AI is a platform thathas solutions and toolingfor automating your workflowsbacked by machine learning We ve bundled together someof Google s flagship AItechnology  such as ComputerVision  OCR  Natural LanguageUnderstanding  and evenGoogle s expertise and buildingknowledge graphs All to provide you with asimple  yet  powerful wayto build applicationsthat betterunderstand unstructured data Let s go see a demoof Doc AI in action So here  we have a receipt I was buying someoffice suppliessince we are  unfortunately not back in the office yet What I m going to do is I mgoing to actually upload thisinto the Doc AI platform So in our Cloud console  wehave the Doc AI platform where we have builtin preview mechanisms So you can testout your documents So this is going toan endpoint  whichhas a specialized model wehave specifically trainedon a variety of expenses Google maintains andimproves the models for you APARNA SINHA  Wait a minute Ihope this is not with my data ANU SRIVASTAVA  Absolutely not We never use your datato train our models Your data is only usedto serve your request so let s take a lookat the data extracted APARNA SINHA  I veseen this before Next  you re going totell me that you regoing to automate my expenses ANU SRIVASTAVA  I knewyou would say that This is the canonicaldemo use case  right But  have you everseen it  like this Take a look at this fieldthat I m highlighting the supplier address This address isn t presentanywhere in the document APARNA SINHA  Wow  wheredid that come from ANU SRIVASTAVA  This is onlypossible with Google s DocumentAI The secret sauce here isthat the knowledge graphis able to not only giveyou back the original textfrom document But it s going toenrich your responseakin to what you d see in asearch  but as part of your APIresponse APARNA SINHA  Wow that s really great ANU SRIVASTAVA  Andit s not just this We have severalspecialized modelsfor many more document typesof much higher complexity Let s take a lookat this payslip So I ran this earlier  and we relooking at the preview outputagain You can see thatwe have some keys We have some fields You can see enrichment on theemployer name and the address Once your data is ina schematized format meaning that we know for everydocument of a certain doc type there are common importantpieces of information So what we did is wepredefined a set of keys So what we do is  withyour extracted data we merge your data tothese pre defined keys So it s much easier towork with than raw OCR So once it s in aschematized format it s easier to pass onto a downstream service Or maybe you re usingsomething for analytics like BigQuery or Looker APARNA SINHA  Thatmakes sense  but whatabout ensuring accuracy And  also  do you havemulti language support ANU SRIVASTAVA  We know thatwith important documents such as these  you can tafford any missteps whenit comes to accuracy So that s why Doc AIprovides a human in the loopconfiguration to triggeron confidence scores so either for specific keys oron the entire document itself And as for atranslation  we supportover 100 languages  such asSpanish  Japanese  Arabic No other solution on the marketsupports such a wide arrayof languages APARNA SINHA  Human inthe loop  translation and knowledge graphcapabilities thatcan be applied to a widevariety of documents This seems super useful Of course  the nextbig question is  can itbe applied to big bulky  complex documents like business contracts ANU SRIVASTAVA Let s take a look So here  I read a contractearlier this morning You can see that thereare typical thingsyou d find in any contract There are some documentnames  the parties involved some dates And like with everyeasy to read contract  I m being sarcastic here   there is an expiration term So this expiration dateactually isn t presentanywhere in the document and it s actually noteasy to figure out It s not in an easilyparsable format Shocker Google s contract processor isable to figure out this datevalue by understandingsignals foundacross the entire document APARNA SINHA  Wow Well  Anu  beforeyou go  can youtell our awesomedevelopers how theycan get started with Doc AI ANU SRIVASTAVA  Absolutely so I know we covereda lot at just breakneck speed So check out thebreakout sessionson Doc AI to dive deeper You can also checkout the documentationfor code labs and quick starts We have client libraries andall of your favorite languages such as Python  node js My personal favorite isJava  but it s an API So you can really use this withwhatever platform or frameworkyou re already using We are thrilled and look forwardto see how you use Google DocAI to power your applications APARNA SINHA  That was amazing Can I have a high five ANU SRIVASTAVA  Yeah APARNA SINHA  Thank you  Anu I loved every part of it ANU SRIVASTAVA  Thankyou for having me URS HOLZLE  Another area thatGoogle has invested in deeplyand that s becoming moreimportant to more companiesis sustainability Many Cloud providers havea vision for a sustainablefuture  and many aim to matchtheir electricity consumptionwith 100  renewableenergy by 2025 or 2030 We accomplished 100 renewable energy in 2017 so we re the only hyperscaleCloud to do this today And all of thatwith data centersthat are twice as efficientas the average data center APARNA SINHA  Thispast week  Sundartalked about Google s goal toenable over a billion usersto live and work moresustainably by next year To reach goals  likethis  and those outlinedin climate pledges made bymore organizations every day we rely on developers  likeyou  to do something about it But we also knowthat it s difficult URS HOLZLE  That s right One of the biggest challengesthat companies faceis that they lackthe tools to accountfor environmental costs  andto help developers address thisfor their organizations  webuilt sustainability toolsdirectly into Google Cloud With Google CloudCarbon Footprint you have access to theenergy related emissions datathat you need for externalcarbon disclosures in just oneclick Now  you won t needthis calculatorif you just want to reportthe net carbon footprintof your workload on GCP Because on GCP it s always zero We also have ourRegion Picker  whereyou can choose the datacenter region with the lowestgross carbon cost right now Of course  again your net impactis zero  no matterwhat region you pick But this tool lets youhelp go one step furtherto become carbon free not just carbon neutral Now  that s actually atool that I can t waitto deprecate in 2030 or so Because Google Cloud hascommitted to be 100  carbonfree by 2030 everyhour of every day Now  we also realizethere s stilla lot to learn when it comesto building sustainably and to help  we justreleased a master classcalled SustainabilityIT Decoded with someof the world s top experts So check it out forguidance on how we can allbuild more sustainably Now  while we re proudto run the cleanest Cloudin the industry  were evenmore inspired by the workthat our customers aredoing with Google Cloudto solve climatechange challenges thatare unique to their business And today  webring you a previewof Google Earth Engine and itsintegration with Google Cloud With over 700 data setsand 50 petabytes of datatoday  Earth Engine givesscientists and developersaccess to the world s largestcatalog of satellite imageryand to tools for drivingsustainable impact APARNA SINHA  So let s lookat this in a bit more detailwith an example of how GoogleEarth Engine and GoogleCloud enable customersto assess risksarising from climate change But instead of metelling you about it we ve invited JoelConkling to show you JOEL CONKLING  Thanks  Aparna The world isconstantly changing and that createsopportunities and risks Helping uncover criticalinsights about the changingworld is why we re integratingEarth Engine into Google Cloud and that integration isnow in Private Preview Today  I ll demo a workflowthat combines Vertex AI  EarthEngine  BigQuery  andGoogle Maps platformto show how Google Cloud makesit incredibly easy for youto innovate  and deliverinsights  and do it quickly So here s a scenario You work at aninsurance company and you need to analyzeyour company s exposureto flood risk You think new buildingsmay be a strong contributorto that risk  and you wantto test your hypothesis To do that  we first need tounderstand where the builtenvironment is expanding In other words  we just needto categorize the surfaceof the entire planet That could behard  but Vertex AIoffers the tooling to developa best in class ML model And Earth Engine providesconstantly updated data Let s fast forward a bit We finished training ourmodel  and now  Earth Engineis sending satelliteimagery to be categorized So your understandingof the worldcan update in near real time Here s the Earth Engine script I m showing theresults of that model This area in red iswhere the model estimatesthe locations of buildings That s your currentbuilt environment To find the change over time  weneed a few more lines of code These lines of code give usa built environment in 2016 and here  we calculatethe differencebetween 2016 and today When there s a change  itshows up in purple on the map This is where thereare new buildings So next  we re going to export sample  and export the data So we can do additionalanalysis in BigQuery Over in the BigQueryconsole  this scriptclusters those data pointshere and then outputspolygons that show the areaswith the biggest changesin the built environment So at this point  youhave a few options You could combine thisdata with flood locationsyou identify around the world also  with Earth Engine Maybe you want to enhanceyour model with weatherdata and physical terrain data That s available onEarth Engine too You could also include dataon your company s insuranceportfolio to gain additionalinsight into critical risks We ll wrap up this demoby visualizing our resultsin a new feature availableon Google Maps platform the Open Source DataViz Library DeckGL with a BigQueryconnector provided by CARTO So we now have a clear pictureof where the built environmentis changing  and where tofocus next for our workon flood risks In summary  nowrangling data  no needto manage infrastructure just actionable insightsincredibly quickly We can t wait to see whatyou ll do with Earth Engine snew integrationwith Google Cloud and with that  I llpass it back to Aparna APARNA SINHA  Thank you  Joel It s incredible to seehow our customers canuse our sustainable technologiesto address climate change now I m really inspiredby all the thingswe ve talked about today And thinking abouthow you re goingto lead your companies into thefuture  that s super exciting No pressure  but it sreally up to you We ve invested millionsin the developer communityover the last fiveyears  and we ll continueto invest in the coming years And  Urs  as proofof that  I understandyou have some additionalnews to share today URS HOLZLE  Absolutely I m really excitedto announce todayour new developer communityprogram called Google CloudInnovators I want to welcome and introduceour first group of leaderswho are driving meaningfulimpact in the industryand their communities So take a look  VIDEO PLAYBACK  TYPING  MUSIC PLAYING  END PLAYBACK APARNA SINHA  Thisis so exciting URS HOLZLE  Yeah through this program we ll give developers accessto early technology previewsand Google engineers We ll recognize the expertiseof our community influencersby promoting theircontributions and we will workclosely with themto solve the toughest problems So we re excitedto come togetherwith this group of innovators Join us atcloud google com innovators APARNA SINHA  So cool I ve been waitingfor all this time Community is extremelyimportant for companiesto create that much needed humanconnection with developers and we hope that thisgives you a windowinto the motivationthat you all giveGoogle to build Cloud productsand services that developerslove URS HOLZLE  And we lookforward to partnering with youto become the greatest techcompanies in your industries APARNA SINHA Remember to join usnext at the live developer Q Asession  and also  tomorrowat Community Day Enjoy the rest of the show URS HOLZLE  Thanks  everyone  MUSIC PLAYING  MUSIC PLAYING APARNA SINHA  Hi I m Aparna Sinha Welcome to day twoof Google Cloud Next I ll be co hosting todayalongside Urz Holzle and we ve got a really greatlineup of new technologyand demos from Google Cloud But before we getstarted  I wantto thank you all for takingtime out of your busy weekto be here with us Yesterday was an incrediblefirst day at Next Thomas and Sundar madesome amazing announcements This week we re releasing over100 new products  services and programs for you We re kicking off day twonow with this keynote a Cloud built for developers After this  we ll jump rightinto a live developer Q Awhere you can ask us anything And then tomorrowis community day and that ll be totallydedicated to youfor learning discussions  networking and all kinds of fun All right  let s get thingsstarted by welcoming Urz Hi  Urz URS HOLZLE  Hi  Aparna Hi  everyone I hope you are as excited asI am to get started today And since this is adeveloper keynote we ll kick it offwith our first demoright away from GoogleResearch and our Deep Mindcolleagues who ve beenworking on some amazing voicetechnology SPEECH MODEL Welcome to Next 21  SPEAKING FRENCH URS HOLZLE  Now beforeyou get too impressedby my language skills  I didnot actually speak these words What you just heard is acustom text to speech modelthat has trained onmy voice and thatcan generate synthetic speechin different languages And our teams havealready startedusing this technologyto improve our voiceexperience for all users APARNA SINHA  That s right Google s project Euphonia isalready using custom voiceto help people withatypical speechto communicate andbe better understood Technology like this createsa more inclusive world To find out more aboutthis  check out the projectEuphonia website Custom Voice is available todayfor select Cloud customers We re very mindful ofthe potential misusesof this technology  andwe re taking great careto prevent them by reviewingeach use case uniquely URS HOLZLE  Now lookaround you  every dayyou see innovation that sbrought to you by developerslike yourself withpersistence and skill and Google has a long traditionof supporting developersin open source and elsewhere For years  you ve beenusing technologieslike Kubernetes  Firebase Tensorflow  Go  Angular GRPC  and many others And when we built Google Cloud we built it for developers and we were inspired by all thethings you ve created with it And our job is quitesimply to make it easierfor you to do what you love So we focus on making you asproductive as possible withthe least amount of effort And so in everything we design we take all the feedbackthat you re sharingwith us and builda platform that just works Whether that s by nativelyembedding key securityor sustainability featuresinto the platform itselfor by featuringpartner solutions rightwithin our own console  we rereally focused on one goal giving you the best developerexperience of any Cloudprovider APARNA SINHA  Google Cloudhas had tremendous tractionwith digital native customerssince our very early days How have you seencustomer and partneradoption evolve since then URS HOLZLE  Well  manyof our biggest customersare Cloud natives  but we veseen tremendous adoptionby a broad segment ofenterprise customersin traditional industriesas well like entertainmentor financial services For example  MajorLeague Baseball which is North America soldest and most attendanceprofessional sports leagueis using Google Cloudto modernize fanengagement and to increaseoperational efficiency And Equifax  whichwas founded in 1899and is one of theworld s largest consumercredit reporting agencies is transforming itselffrom a credit bureau to anext generation data analyticsand technology companybuilt on Google Cloud APARNA SINHA  We veseen a huge shift Essentially every companyis becoming a tech companyto increase theircompetitivenessand establish leadershipin their industries Developer talentand Cloud Servicesare at the heart of this shift Whether you call that digitaltransformation or somethingelse  companies ofall sizes are findingthat Google Cloud isoptimized to help makeyour data  yourapplications  and your talentmore useful and relevantto your business URS HOLZLE  Exactly And as Thomasmentioned yesterday everyone needs to bethinking through how they llfundamentally shift intoa technology companyto serve their customers in themost meaningful ways 10 yearsfrom now  and you as developersare key to making this happen So organizationsask themselves  dowe have the most cuttingedge technology to becomea leader in our industry And you  the developers  arewell equipped to answer this And we are superfocused on making youand your company successful And there s twoareas we focus onto support your growth first of course making it easier for developersto get their job done  second investing in adeveloper communityso that everyone can learnand grow from each other So let s talk abouthow Google Cloud ismaking it easier for developersto get their job done From our transformationalinfrastructurestack to our deep innovationsin data  security  ML every feature we release startswith simplifying the developerexperience Take our open Cloudexperience  for example We recently expandedour compute stackto include  INAUDIBLE  VMsthat deliver 42  better priceperformance over anyother comparable solutionin the market with no recompile And thanks to our zerotrust approach to security Google Cloud was ranked a leaderin IS platform native securityby Forrester  well aheadof the competition And of course we re focusing alsoon managed servicesthat make it easyfor you to deploy scale andmanage Kubernetes clusterson the edge or in the Cloud And Google Cloud has themost complete and most securecontainer experiencefor developers In fact  the 2021 Gartnersolution scorecardfor Google AnalyticsEngine gave GKEan overall score of 92  again well ahead of the competition And of course withGKE and Anthos you can run these containersanywhere on premise other clouds  onthe edge  anywhere So it s fair to say that inthe years since Google inventedKubernetes  containershave really completelyrevolutionized IT operations Now recently Europeanfilmmakers from Honeypot iocreated a documentary onthe history of Kubernetes and it will be outin January 2022 And you are thevery first audienceto have a look at thetrailer right now So let s roll that  VIDEO PLAYBACK   Sounds good   Do I look at you look at camera   2013  it was clearthat Cloud was a thing but most folks were focusedon infrastructure Cloud   The dirty secret for a longtime is like people who areeither building their owndata centers using co los there s a huge resource waste   And so at thatpoint  automation toolsare all the rave People are now trying toabstract away the servers   Google was lookingfor ways to applyits internal infrastructureexpertise to the Cloud   As we started looking attechnologies like Docker we were like impressedby the strengthof what they daccomplished in solvinga very specific problem   This is going to happenwith us or without us   Google had to make a boldmove in the Cloud space to bethe long term winner   Every big startup  I felt had a container orchestrationproject  and half of them wereannounced at Docker Con 2014   Open source is mostsuccessful when it splayed as a positive sum game  MUSIC PLAYING  END PLAYBACK APARNA SINHA  This issuch a great community URS HOLZLE  Yeah  absolutely I recognize a lotof the faces  and Ican t wait to see the film Now let s get back to how we remaking it easier for developersfor you to build reallythe leading technologycompanies of tomorrow Now Kubernetesdeployments can involvea fair bit of manualconfiguration  clusters  nodes load balancers animals  et ceterabut not on GoogleCloud  because weoffer you the most automatedand secure Kubernetesexperience available With GKE Autopilot Google provisions andmanages the clusters entireunderlying infrastructure including control plane node pools  working nodes And that lets youfocus on the higherlevel of servicesand applicationsthat you re building Nobody else offersanything like this because beyondmanaging node upgrades GKE Autopilot also automaticallyconfigure security featureslike shielded GKE nodes  secureboots  and workload identity And it also implementssecurity best practicesby blocking less safe featureslike external IPS or legacyauthorization So you don t geta toy Kubernetescluster  which GKE Autopilot You get a sophisticatedcluster thatuses the best practices broughtto you by the team that broughtto you Kubernetes itself So you re alwaysup to date  and youget the same resultsas the expertswithout having to bean expert yourself APARNA SINHA  Thepandemic put developersin the driver s seat  andyou all drove GKE usageto all time highs At the same time  wesaw explosive growthin the use of Google Cloudserverless offerings especially Cloud Runand Cloud Functions It s mainlyenterprise developerswho have driven this growth Cloud Run excels atdeveloper experience It s earned the highestcustomer satisfaction ratingamong developers as measuredby user research international Cloud Run combines the bestof both worlds bringing youserverless and containers There s no cluster toset up or configure so developers are able toscale seamlessly and securely Under the hood  Cloud Runscales container instancesin isolated sandboxes Any access outside asandbox is mediatedby network controls or identityand access management or both And it isn t just for new apps Cloud Run supportstraditional workloadslike Java SpringBoot and asp net We also recently introducedcommitted use discountsto lower the cost at scale And we ve introducedAlways on CPU which enables asynchronousand background processes to beused on Cloud Run So you have all thebenefits of serverlesswithout the restrictions The theme here is easier  moresecure development  especiallywith remote work URS HOLZLE  You reabsolutely right We ve been focusing on remotedevelopment for some time now but the pandemic has certainlyaccelerated the shift Now what would be moreessential to remote developmentthan to be able to usethe full power of TCPright from your laptopwith zero local setup Cloud Shell Editor is a contextaware remote developmentenvironment that lets youdevelop and manage applicationssecurely from any browser It supports languages like Go java  node  Python  C Sharp and comes with an integrateddebugger source control APIExplorer And if you want to testlocally on your laptop it also comes with localemulators for Kubernetesand serverless APIs APARNA SINHA Thanks  Urs  Next letme introduce you to Abby Carey She is going to show us howGoogle Cloud makes it easyfor you to securely buildmodern applications  again right from your laptop Hi  Abby ABBY CAREY  Hi  Aparna We developers have had ahard time writing  extending deploying  andoperating applications but it doesn t haveto be difficult Let s start withCloud Shell editor It comes with current versionsof your favorite DevToolslike Docker  Minikube Scaffold  and more So it s nothing to downloador install locally Tutorials are builtinto Cloud ShellEd  which makes it easyto come up to speedon complex topics like GKE APARNA SINHA  So no moreswitching between tabs  docs your terminal  and your code This integratedexperience is highlydifferentiatedfrom other clouds You can even offeryour own tutorials and that allowsyour organizationto share best practices andon board new hires faster ABBY CAREY  Anotherpopular featureis Kubernetes YAMLauthoring assistance Let s say I want to add YAMLfor a service to this project I can press ControlSpace and then findthe Kubernetes service snippet Now I can tap throughand fill everything in I also get auto completes And if I happen to makea formatting mistake I am notified that there san issue in real time APARNA SINHA  Nowmany of you preferto work locally in an IDE This same YAMLauthoring assistance it s also available for VS codeand IntelliJ via the Cloud Codeplugin Cloud Code has built insupport for both Cloud Runand Kubernetes ABBY CAREY  In fact  if you reusing Cloud Run or functions you don t need to know Docker You can build and deploy yourapp with just one command because Cloud Code  because Cloud Build isintegrated under the hood This is an applicationwith no Docker file With the new GCloud run deploycommand  all I have to dois provide a name formy service and thenlet it know where mysource code lives which is this currentdirectory  and we re deploying APARNA SINHA  So nice And thanks to this easeof use  98  of usersdeploy an application toCloud Run on their first tryin less than five minutes ABBY CAREY  I just showed sourcecode deploys the Cloud Runbut there are more ways GoogleCloud has made deploymenteasier and more secure First  I can scan mybuild container imagesto check for vulnerabilities I ve already run an on demandscan on one of my imagesusing GCloud ArtifactsDocker images scan Now I can copy the IDof my scan and thenview my images vulnerabilitieswith the list vulnerabilitiescommand And once that s finished a severity levelis assigned toeach vulnerabilityto help you prioritize APARNA SINHA  That ssuper important It s really helpful inaddressing security concernsearlier in the softwaredevelopment lifecycle But now what if your buildpipeline is compromised ABBY CAREY  Forthat  I can enableBinary Authorization on mydeployed Cloud Run services This way only trustedcontainer imagesare deployed to production APARNA SINHA Binary Authorizationis truly unique in the industry It enables you to put proactivesecurity measures in placeto reduce softwaresupply chain attackrisk by blocking deploymentsthat violate policy And speaking ofdeploying  we re makingit seamless for youto do CI CD securely You can take advantageof serverlessbuild environments within yourown private network with CloudBuild private pools ABBY CAREY  Andfor advanced CD  wehave Google Clouddeploy  which allowsyou to create custom deliverypipelines for your specific usecase and needs APARNA SINHA  That is so cool Well  a real applicationconnects to many supportingCloud Services So Abby can youshow us an exampleof how we make theseintegrations easier ABBY CAREY  Sure When creating aCloud Function  it seasy to integratewith Secret Manager First  create a secretthat stores your APIkey  which I ve already done Now I can eithermount it as volumeor expose it as anenvironment variable I ll mounted it as a volume  andthen I ll name my Mount path This will always point to thelatest version of my secret And now I can securelyreference this API keyfrom my source code This abstraction enablesportability and a betterlocal development experience Cloud Run also integrateswith Secret Managerto make it easier todo the right thingand not put sensitivedata in source APARNA SINHA  Love that so much OK  so now you ve written yourapp  you ve deployed your app and you ve connected your appto other Google Cloud resources What s next ABBY CAREY  Operatingyour app in production With Cloud ops  youget one integrated viewfor your alerts  events metrics  and logs No more jumpingaround multiple toolsas you try to understandwhat went wrong APARNA SINHA  Thatwas so awesome  Abby Thank you forsharing this with us ABBY CAREY  Thanks  Aparna APARNA SINHA  In eachof these instances we ve done theintegration work for you Because the more workwe put into this the less work you have to do And this principle appliesto security as well We ve put a lot of energy intobuilding security nativelyinto everythingwe do so that youcan innovate with assurance Both GKE and Cloud Runbenefit from the securityfixes we implement beforevulnerabilities are exposed Just think about thefamous vulnerabilityuncovered in how Kuberneteswas handling proxy requests We found it we coordinated andcommunicated the disclosure we fixed it for the entireKubernetes community and we patched all ourproducts before any customerswere impacted More recently cyber threats haveshifted the focus towardsthe software supply chain URS HOLZLE  That s rightmalicious actors are tryingto compromise the softwaresupply chain from bad codeemission to bypassing thisCI CD pipeline altogether And to help solvethis problems  weproposed an industrystandard called Salsa It s a security frameworkthat provides common criteriafor increasinglevels of softwaresecurity through automation andthrough cryptographic signingat each stage of thesoftware supply chain And that makes it possiblebut not necessarily easy And so making it easy fordevelopers to ensure securityis super important and that s whywe re focusing onbuilding the securityright into thedeveloper tool chainanticipating and preventingissues ahead of time not when you re most at risk So for example  CloudBuild our servicethat lets you build test  and deployacross multiple environments such as VMs  Serverless Kubernetes  or Firebasenow offers Salsa levelone compliance by default Because Cloud Build gives youa verifiable build provenance So this provenance lets youtrace a binary to the sourcecode that it was builtfrom to prevent tamperingand to prove thatthe code that youthink you re running actuallyis the code you are running Cloud Build is the firstand only CI CD serviceto offer this capability but we go beyond that As you ve seen  build theintegrity automaticallygenerates digitalsignatures  whichcan then be validatedbefore deployment by BinaryAuthorization That s anotherGoogle Cloud first And so without youneeding to do anythingwe prevent anyonein your organizationfrom deploying code that has notbeen built by your legitimatebuild system Now ensuring securitypost deploymentis equally critical On GCP  you can enablecontinuous scanning and you can use our servicemesh to embrace a zero trustsecurity model and automaticallyand declaratively secureyour services andthe communication So you can manageauthentication  authorization and encryption between serviceswith little or no changesto the applications themselves Let me say that again  with little to no changes tothe applications themselves So that means that thesesecurity improvementshelp secure not just new codebut also existing binariesso you can use themfor any applicationthat you re migratingto the Cloud Both Anthos Service Meshand now Cloud Build hybridare availableacross Google Cloudand your on premiseenvironment  and theywork with VPC ServiceControls and VPC appearingto automate the developmentsecurity for your enterprise No other Cloud provider protectsyour software supply chainto this level because we startedworking on softwaresupply chain securitylong before it wasin the headlines And so by choosing GCP  youbenefit from this leading edgefocus on security APARNA SINHA  Whetherwe re buildingfoundational open sourcetechnologies like Kubernetesor Istio or turning them intofully managed services like GKEand Anthos ServiceMesh  our goalis always to reducecomplexity for our usersby helping create theseindustry standardswe can provide safer and simplerservices for you  the developerand that s exactly our approachto securing the software supplychain We ve co founded the OpenSource Security Foundationwith other technology leadersto create security standardsfor open source software And we re starting tobring products to marketlike Open Source Insights which provides youa complete transitive dependencygraph for many Open Sourcepackages Now let s turnback to Urs to hearwhy Google Cloudis best positionedto support you inbecoming a technologyleader in your industryusing data as a core asset URS HOLZLE  Thanks Yes  so far we ve been talkingabout developing and managingcode  but data is at theheart of many enterprises So we also have theleading data cloud productsin the industry designedfor optimal performanceand reliability for applicationsof all sizes while scalingto immense capacity Now let s start with databases When it comes to databases every Cloud gives you choices They offer SQL databases  whichare great but unfortunatelydon t scale  and ofcourse NoSQL databases which do scale but unfortunately  are not SQL Only Google Cloud gives youa third choice with Spanner Because Spanner isSQL  and  in fact it just got apost credits interface but it scales horizontally and it can literally handlea billion requests per second Nobody else has isscalable SQL system so it s no wonder we reseeing huge adoption Now on that datawarehouse side  wehave  of course  theleading Cloud data warehousewith BigQuery Hundreds of customers are usingBigQuery at petabyte scaletoday  petabyte each And you can run of courseBigQuery on AWS or Azure On top of thatopen source systemsfor data like processinglike flank  spark and beam run nativelyon Google Cloudin a simpler and morecost effective way thanin other environments In fact  you canrealize a 57  lower TCOcompared to on premisedata lakes for data scienceprojects Now on top of thatsavings  our data cloudalso includes theworld s first and onlyautoscaling andserverless Spark service And finally  Googlehas deep partnershipswith leading data drivencompanies  includingData Flint  Confluent  MongoDB Reddy s labs  and many others And so togetherwe help customersaccess an open platform thatpowers analytics at scaleyet is easy to use APARNA SINHA  Ourpartner communityis central to the healthof our Cloud business and we re especially excitedabout the innovation comingfrom our dataCloud partnerships Together  we ve optimizedour infrastructurefor performance and efficiencyto give our partnersthat extra edge whenthey run on Google Cloud One of our leading datapartners is MongoDB And we have their CEO DevIttycheria here with us today Welcome  Dev DEV ITTYCHERIA  Hi  everyone Happy to be here APARNA SINHA  Dev one of the trendswe re seeing in ourenterprise customer baseis that they re nowcompeting for leadershippositions in their industry bybecoming technology companies How would you say Google Cloudand MongoDB working togethercan help these customersachieve that transition DEV ITTYCHERIA  Well Aparna  the companieswho are in the leadershippositions in their industriesare those who have built theircompetitive advantage usingsoftware and data totransform their business And the key word hereis built  You can tbuy a competitive advantage You have to build it This means you need to enableyour developers to innovateas quickly aspossible  whether it sbuilding new software toseize new opportunitiesor to respond to new threats MongoDB and Google Clouddeeply understand this Developers chooseMongoDB on Google Cloud because we givethem the tools theyneed to be asproductive as possible including having our servicesavailable in the Google Cloudconsole for easydiscovery and deployment Today MongoDB Atlas runsin 24 Google regionsacross the world with deeptechnical integrationswith Google sanalytic and AI tools This enables our customers toinnovate quickly and emergeas leaders in their industries As a result  we re seeingexplosive growth or customersembracing the true valueof our partnership APARNA SINHA  That s incredible So when you think aboutthe development teamsthat these new customers what s the biggest challengethat you re helping them solve DEV ITTYCHERIA  Yeah  whenyou talk to development teams you find that theyspend the mostamount of their time tryingto work with data  as servingrelevant data at the righttime to the right audienceis critical to buildingany application Unfortunately relational databasesare not designed for the waydevelopers think or code nor are they designed for scale fault tolerance  or resilience Consequently development teams findit hard to move fast usingrelational databases MongoDB is designed toaddress this problem We make it very easy fordevelopers to work with data and we re able to address themost demanding requirementsfor performance scaleand fall tolerance The partnership withMongoDB and Google Cloudenables developersaround the worldto easily build modern softwareapplications to addresstheir needs oftoday and tomorrow APARNA SINHA  That s terrific Thank you so much forbeing here with us today DEV ITTYCHERIA  Thankyou for having me URS HOLZLE  Yes Thanks  Dev  for joining us Now there s lotsof ways developerscan improve their productivity automate tasks thatare repetitive  masterthe command line use the best tools thatmake your life easier or reuse other people scode just to name a few And another great way toaccelerate your productivityis with buildingblocks or templatesor fully managed services inareas like machine learning Because on GoogleCloud  you don thave to be an expert tobuild smart applications With new services like VertixAI  you can build  deploy and scale more effectiveAI models quickly So that lets youdeliver the insightsto your organization thatwill help them create morepersonalizedcustomer experiences run more efficient processes and take that leadershipposition in your industry APARNA SINHA  So with that let s go to our next live demo Joining me todayis Anu Srivastava She s going to show us howthese breakthroughs in AIare advancing Cloud adoptionand redefining the worldof document processing Hi  Anu ANU SRIVASTAVA  Hey  Aparna We all know how towork with data whenit s in a structured format likein a database  some JSON  CSVfiles  or just variablesin my code  right But what aboutunstructured data Many of the world s businessprocesses start  include or end with a document but these documentscan be difficult to process Think about all the ways youcould enhance your applicationif you could justunlock that data This is where Google Clouddocument AI comes in Doc AI is a platform thathas solutions and toolingfor automating your workflowsbacked by machine learning We ve bundled together some ofGoogle s flagship AI technologysuch as computer vision  OCR natural language understanding and even Google s expertisein building knowledge graphsall to provide you witha simple yet powerful wayto build applicationsthat better understandunstructured data Let s go see a demoof DocAI in action So here we have a receipt I was buying someoffice supplies since we are unfortunatelynot back in the office yet What I m going to do is I mgoing to actually upload thisinto the DocAI platform So in our Cloud console we have the DocAI platformwhere we have builtin preview mechanismsso you can testout your documents So this is going toan endpoint  whichhas a specialized model wehave specifically trainedon a variety of expenses Google maintains andimproves the models for you APARNA SINHA  Wait a minute I hope this is not with my data ANU SRIVASTAVA  Absolutely not We never use your datato train our models Your data is only usedto serve your request So let s take a lookat the data extracted APARNA SINHA  I veseen this before Next you re going totell me that you regoing to automate my expenses ANU SRIVASTAVA  I knewyou would say that This is the canonicaldemo use case  right But have you everseen it like this Take a look at this fieldthat I m highlighting the supplier address This address isn t presentanywhere in the document APARNA SINHA  Wow Where did that come from ANU SRIVASTAVA  This is onlypossible with Google s DocumentAI The secret sauce here isthat the knowledge graphis able to not only giveyou back the original textfrom document  butit s going to enrichyour response akin to what you dsee in a search but as partof your API response APARNA SINHA  Wow That s really great ANU SRIVASTAVA  Andit s not just this We have severalspecialized modelsfor many more document typesof much higher complexity Let s take a lookat this payslip So I ran this earlier  and we relooking at the preview outputagain You can see thatwe have some keys We have some fields You can see enrichment on theemployer name and the address Once your data is ina schematized format meaning that we know for everydocument of a certain doc typethere are common importantpieces of information so what we did is wepredefined a set of keys So what we do is withyour extracted data we merge your data tothese pre defined keys So it s much easier towork with than raw OCR So once it s in aschematized format it s easier to pass onto a downstream service or maybe you re usingsomething for analyticslike BigQuery or Looker APARNA SINHA  That makes sense But what about ensuringaccuracy  and alsodo you havemulti language support ANU SRIVASTAVA  We know thatwith important documentssuch as these  you can tafford any missteps whenit comes to accuracy So that s why DocAIprovides a human in the loopconfiguration to triggeron confidence scores so either for specific keys oron the entire document itself And as for atranslation  we supportover 100 languages  such asSpanish  Japanese  Arabic No other solution on the marketsupports such a wide arrayof languages APARNA SINHA  Human in theloop  translation  and knowledgegraph capabilitiesthat can be appliedto a wide variety of documents this seems super useful Of course  the nextbig question is can itbe applied to big bulky  complex documentslike business contracts ANU SRIVASTAVA Let s take a look So here I read a contractearlier this morning You can see that thereare typical thingsyou d find in any contract There are some documentnames  the parties involved some dates And like with everyeasy to read contract  being sarcastic here  there is an expiration term So this expiration dateactually isn t presentanywhere in the document and it s actually noteasy to figure out It s not in an easilyparsable format  shocker Google s contract processor isable to figure out this datevalue by understandingsignals foundacross the entire document APARNA SINHA  Wow Well  Anu  beforeyou go  can youtell our awesomedevelopers how they can getstarted with DocAI ANU SRIVASTAVA  Absolutely So I know we covered a lotat just breakneck speed So check out thebreakout sessionson DocAI to dive deeper You can also checkout the documentationfor code labs and quick starts We have client libraries inall of your favorite languages such as Python  node js My personal favorite isJava  but it s an APIso you can really use this withwhatever platform or frameworkyou re already using We are thrilled and look forwardto see how you use Google DocAI to power your applications APARNA SINHA  That was amazing Can I have a high five Yeah Thank you  Anu I loved every part of it ANU SRIVASTAVA  Thankyou for having me URS HOLZLE  Another area thatGoogle has invested in deeplyand that s becoming moreimportant to more companiesis sustainability Many Cloud providers havea vision for a sustainablefuture  and many aim to matchtheir electricity consumptionwith 100  renewableenergy by 2025 or 2030 We accomplished 100 renewable energy in 2017 So we re the only hyperscaleCloud to do this todayand all of thatwith data centersthat are twice as efficientas the average data center APARNA SINHA  Thispast week Sundartalked about Google s goal toenable over a billion usersto live and work moresustainably by next year To reach goals likethis and those outlinedin climate pledges made bymore organizations every day we rely on developers likeyou to do something about it but we also knowthat it s difficult URS HOLZLE  That s right One of the biggest challengesthat companies faceis that they lackthe tools to accountfor environmental costs And to help developers addressthis for their organizations we built sustainability toolsdirectly into Google Cloud With Google Cloudcarbon footprint you have access to theenergy related emissions datathat you need for externalcarbon disclosures in just oneclick Now you won t needthis calculatorif you just want toreport the net carbonfootprint of your workloadon GCP  because on GCP it salways zero We also have ourregion picker whereyou can choose that datacenter region with the lowestgross carbon cost right now Of course  again your net impactis zero no matterwhat region you pick but this tool lets youhelp go one step furtherto become carbon free not just carbon neutral Now that s actuallya tool that Ican t wait to deprecatein 2030 or so because GoogleCloud has committedto be 100  carbon free by2030 every hour of every day Now we also realizedthere s stilla lot to learn when it comesto building sustainably And to help we justreleased a master classcalled SustainableIT Decoded with someof the world s top experts So check it out forguidance on how we can allbuild more sustainably Now while we re proud to run thecleanest Cloud in the industry we re even moreinspired by the workthat our customers aredoing with Google Cloudto solve climatechange challenges thatare unique to their business And today we bring you apreview of Google EarthEngine and its integrationwith Google Cloud With over 700 data setsand 50 petabytes of datatoday  Earth Engine givesscientists and developersaccess to the world s largestcatalog of satellite imageryand the tools for drivingsustainable impact APARNA SINHA  So let s lookat this in a bit more detailwith an example of how GoogleEarth Engine and GoogleCloud enable customersto assess risksarising from climate change But instead of metelling you about it we ve invited JoelConkling to show you JOEL CONKLING  Thanks  Aparna The world isconstantly changing and that createsopportunities and risks Helping uncover criticalinsights about the changingworld is why we re integratingEarth Engine into Google Cloud and that integration isnow in private preview Today I ll demo aworkflow that combinesVertex AI  Earth Engine BigQuery  andGoogle Maps platformto show how Google Cloud makesit incredibly easy for youto innovate and deliverinsights and do it quickly So here s a scenario You work at aninsurance company and you need to analyzeyour company s exposureto flood risk You think New buildingsmay be a strong contributorto that risk  and you wantto test your hypothesis To do that  we first need tounderstand where the builtenvironment is expanding In other words  we just needto categorize the surfaceof the entire planet That could be hard  butVertex AI offers the toolingto develop a bestin class ML model and Earth Engine providesconstantly updated data Let s fast forward a bit We finished training ourmodel  and now Earth Engineis sending satelliteimagery to be categorized so your understandingof the worldcan update in near real time Here s the EarthEngine script showingthe results of that model This area in red iswhere the model estimatesthe locations of buildings That s your currentbuilt environment To find the change over time  weneed a few more lines of code These lines of code give usa built environment in 2016 And here  we calculatethe differencebetween 2016 and today When there s a change  itshows up in purple on the map This is where thereare new buildings So next we re going toexport sample and exportthe data so we can doadditional analysis in BigQuery Over in the BigQueryconsole  this scriptclusters those data pointshere and then outputspolygons that show the areaswith the biggest changesin the built environment So at this point  youhave a few options You could combine thisdata with flood locationsyou identify around theworld also with Earth Engine Maybe you want to enhanceyour model with weatherdata and physical terrain data That s available inEarth Engine too You could also include dataon your company s insuranceportfolio to gain additionalinsight into critical risks We ll wrap up thisdemo by visualizingour results and a new featureavailable on Google Mapsplatform  the open sourcedata viz library deck glwith a BigQuery connectorprovided by  INAUDIBLE   So we now have a clear pictureof where the built environmentis changing and where tofocus next for our workon flood risks In summary  nowrangling data  no needto manage infrastructure just actionable insightsincredibly quickly We can t wait to see whatyou ll do with Earth Engine snew integrationwith Google Cloud and with that  I llpass it back to Aparna APARNA SINHA  Thank you  Joel It s incredible to seehow our customers canuse our sustainable technologiesto address climate change now I m really inspiredby all the thingswe ve talked about today And thinking abouthow you re goingto lead your companies into thefuture  that s super exciting No pressure  but it sreally up to you We ve invested millionsin the developer communityover the last fiveyears and we ll continueto invest in the coming years And Urs  as proof ofthat  I understandyou have some additionalnews to share today URS HOLZLE  Absolutely I m really excitedto announce todayour new developer communityprogram called Google Cloudinnovators I want to welcome and introduceour first group of leaderswho are driving meaningfulimpact in the industryand their communities So take a look  VIDEO PLAYBACK  KEYBOARD CLICKING  MUSIC PLAYING  END PLAYBACK APARNA SINHA  Thisis so exciting URS HOLZLE  Yeah through this program We ll give developers accessto early technology previewsand Google engineers We ll recognize the expertiseof our community influencersby promoting theircontributions and we will workclosely with themto solve the toughest problems So we re excitedto come togetherwith this group of innovators Join us atcloud google com innovators APARNA SINHA  So cool I ve been waitingfor all this time Community is extremelyimportant for companiesto create that much needed humanconnection with developers and we hope that thisgives you a windowinto the motivationthat you all giveGoogle to build Cloud productsand services that developerslove URS HOLZLE  And we lookforward to partnering with youto become the greatest techcompanies in your industries APARNA SINHA Remember to join usnext at the livedeveloper Q A sessionand also tomorrowat community day Enjoy the rest of the show URS HOLZLE  Thanks  everyone  MUSIC PLAYING  MUSIC PLAYING ALISON WAGONFELD  Hi  everyone I m Alison Wagonfeldwith Google Cloud Welcome to Google Cloud Next And thank you to ourGoogle orchestra composed of Bay Area Googlers We are coming to you livefrom our Google Cloudcampus in Silicon Valley where I m here with SundarPichai and Thomas Kurian We have an incredible lineupover the next three dayswith live keynotes demos  and Q Awith our leadershipin over 140 sessionscovering all ourcloud solutions Our customers andpartners will sharehow they re using Google Cloudand Google Workspace technologyto tackle their greatestchallenges and opportunities We are humbled to support theirwork and share their stories Thank you forbeing here with us And now  please join me inwelcoming Google and AlphabetCEO  Sundar Pichai SUNDAR PICHAI  Hi  everyone Welcome to GoogleCloud Next 2021 We are happy you re here Of course  I d be happierif we could actuallybe here in person Don t get me wrong Thomas is great company But he still doesn t laughat my container jokes So while many of us are stillwaiting for a return to normal it does feel likewe are on our way The question is  willwe recognize normalwhen we get there The pandemic hassped up digitizationin all aspects of our lives It s changed howwe visit a doctor how our kids learnin school  and howwe connect with one another And it s made the futureof work our present giving us renewed opportunityto transform our organizations From our work withcustomers  Thomas and Ihave learned there are threethings that position us to bemost helpful to businesses  first  our Cloudplatform  it s designedto help enterprises transformthrough digitization built on the deepinvestments we vemade in technical infrastructureover the past 23 years Looking at our networking anddata center investments alone we have the largest network withthe lowest latency of any cloudprovider With our expansions into Warsaw Delhi  Melbourne  and Torontothis year  we now have 28regions with plans for 10 more And it s all connectedby 19 sub sea cables Our Grace Hoppertransatlantic cablewill be the first to incorporatenovel optical fiber switching Second  our enterprise customersbenefit from our consumer scaleand innovation We have a deep andbroad consumer ecosystemof devices and services YouTube connectsa global communityof viewers and creators Android  Nest Assistant  and Mapsare loved by peoplearound the world We have the unique ability toconnect consumer ecosystemswith enterprise ecosystems For example  the realisticvoices and languageunderstanding thatpowers Google Assistantcan improve enterprisecustomer experiencesand increase satisfaction And the underlyingvisual technologythat enables Google Lens tohelp people shop or learnabout a new flowerin their gardencan help manufacturersidentify defects Third  we offercomprehensive securitythat helps organizations including ours protect what matters Security is foundationalto everything we do We have a longhistory of building itinto every layer of thecomputing environment We are a pioneer ofzero trust computingand have deep experiencerunning this model at scale On its own  any oneof these pillarswould be helpful to customers Put together  they canbe transformative  whichis the mission of Google Cloud We continue to drive innovationthrough our products Over the past year  we havehad more than 1 500 productand feature releases That s about four a day They re all designedto be helpful from Smart Canvas andGoogle Workspace thatmakes collaborationricher  to new AI poweredindustry solutions and BigQueryOmni for data analyticsacross any cloud We ve also made significantpartnership announcements For example  we teamed upwith global telecom leaders such as Ericsson  Nokia  andT Systems to deliver 5G edgeand hosted computing solutions What I m mostexcited about is howour strengths in cloud consumer  and securityare helping topenterprises acceleratetheir transformation Ford chose Google as theirtransformation partnerbecause of our strengthin consumer and cloud Together  we are innovating inelectrification  connectivity and self driving technologies Ford is harnessing GoogleMaps to help drivers navigate Google Assistant to helpdrivers concentrate and Google Play to keeppassengers learningand entertained L Oreal partneredacross Google to builda virtual try on experience They are harnessingthe best AI models as well as using YouTube tobuild a community of fans and Google Shopping to makeit easy to discover and buytheir products We are humbled that so manyorganizations continue to puttheir trust in Google Cloud And we are committed tohelping you solve the biggestchallenges you face I ll be back a little later totalk more about how we do that But first  I llturn the stage overto Thomas Kurian CEO of Google Cloudto talk more about our mission THOMAS KURIAN Thank you  Sundar Hello  everyone It s a pleasure to be here onGoogle s Silicon Valley campustoday for the moment we veall looked forward to all yearlong  the opportunityto connect with you our customers  partners and developers and to share all theamazing  exciting stuff we vebeen working on A special thank youto all our sponsors especially our luminarysponsors  Accenture  Atos Maven Wave  and Deloitte Let s first hear fromsome of our customersabout the amazingways in which theyhave accelerated transformationwith Google Cloud  VIDEO PLAYBACK  MUSIC PLAYING   The pandemicreally did acceleratethe digital wave of change byeasily three to five years   We transitionedto all virtual care   We re betterpositioned to accelerate   Fast forward to today we have a truly profound permanent structuraltransformation   Now  our team can lookat hundreds of callsin minutes instead of weeks   Our business hasreally been transformedwith the help of Google Cloud We have spikes in demand We can scale four timesin less than five minutes   Schrodinger is transformingthe way scientistsare developing drugs Chemical space isalmost infinite We can do things wecouldn t do before   Can t I justtrack my shipment We think that we can help solvethat for the supply chain   How can we help workerswith artificial intelligence   Suddenly  lots ofideas become interesting   We re very excited to developour artificial intelligenceand machine learning   Access to innovation the speed of innovation   Seconds  millisecondsall matter   We re rethinking routineand pioneering simplicity   The partnershipwith Google Cloudprovides us with technologythat will drive us to be better  END PLAYBACK Now  it s not a secret thatmany of the biggest and mostinnovative technology companies leaders such as Spotify Twitter  Shopify  MercadoLibre ShareChat  Dapper Labs and so many other excitingstartups and tech leaderschoose Google Cloud to helpthem build  scale  and innovate Still  other customersare transformingusing our purpose builtindustry solutions The Home Depot for instance  usedcontact center AI toreduce customer resolutiontime by 91 million minutes Procter   Gamblecreates personalizedomnichannel journeys from over275 million consumer records IKEA Retail userecommendations AIto increase e commerceclick through rates by 30 and increase order value by 2  FIH  a Foxconn company used visual inspectionto identify defects 10times more accurately Mr  Cooper  a leadingmortgage services provider used document AI to processmortgage documents 400  moreefficiently And Bank of New York Mellondeveloped a liquidity solutionthat predicts nearly 40  ofsecurities settlement failureswith 90  accuracy We re helping many businessesdrive durable innovationthrough long term partnershipswith Google s transformationcloud Now  to help them transform we focus on five key themes One  are we the best atunderstanding and usingdata in the industry Two  do we have theindustry s leading technologyinfrastructure Three  are we creatingthe best hybrid workplacefor our passionate andtalented employees Four  do we know that our data systems  and users are securenow and that they willbe secure in the future And five  are weworking togetherto address the mostimportant challenges facingour world today We help organizations addressthese important questions Let s Start With the first question Are we the best at understandingand using data in the industry We help organizationsunify their dataacross multipleclouds and silos combining structuredand unstructured data and making data everyemployee s superpower Using data analyticsand AI together Schrodinger acceleratedclinical drug discovery by 60  finding newbreakthroughs faster Like many states aroundthe United States the Wisconsin Departmentof Workforce Developmentconfronted a wave ofnew unemployment claims Using Google s data cloud  theyclear over 750 000 unemploymentclaims  reducing claim timesfrom weeks to just daysand dispersing over  2 billionin unemployment benefits Walmart is transformingthe experiencefor the approximately220 million shoppersaround the globe who visita Walmart store  club or e commerce website each week I m honored today to welcomeSuresh Kumar  executive vicepresident  Walmart s globalchief technology officer and chief development officer SURESH KUMAR  Hi  Thomas And hello  everyone I m excited to behere today to shareabout how we at Walmartare innovating on behalfof our customers  ourassociates  and the rolethat our partners likeyou  Google  have playedin our innovation journey Now  I came to Walmart at atime of incredible digitaltransformation and jointly with my team created a robust executionplan to acceleratethe transformationacross three areas  building greatcustomer experiences optimizing our business and modernizing our platformand our infrastructure Each of these three prioritiesare individually important But when you bring themtogether at the scaleof a business like Walmart it has a massive impact If you move with speed you can completelytransform global operationsand disrupt entire industries Some of our most data intensive our critical decisioningprocesses are gettingthe BigQuery treatment Through this  not only are weseeing significant savings thatwill continue as we migratemore and more data over we are also seeing theability to use the datain interesting ways  includingenabling analytics at scaleand turning data intoactionable insights From a data migrationto BigQuery standpoint 97  of the tables usedfor data warehousehave already migratedto the cloud And 30  of the big data havebeen migrated with plansto almost double that bythe end of this fiscal year And the cherry on thetop is that BigQueryhas allowed us to integratewith pretty much any datavisualization tool and analyticstools that s out there thereby improving our processingtime by 23  and  of course needless to say  amuch better experience Using BigQuery has had adirect impact on our business An excellent exampleis our abilityto close our financialbooks in three days insteadof five days And that s a pretty big deal Leveraging ourcloud has enabled usto unleash the potential ofAI across our entire business This goes frompredicting demand to managing in stock levels to optimizing supply chain to freeing up timefor our associatesto serve our customers As one example Express Delivery which is our twohour delivery servicewhich we launchedlast year  activelyuses it behind thescenes as our customeris creating their order Multiple algorithmswork in unisonto optimize the deliveriesout and determineif the customer is eligiblefor Express Delivery We are building our ownAI and ML capabilitiesto power multipleareas of our business including classification natural language processing forecasting  regression computer vision  predictions process automations and a whole lot more So to wrap up  I m reallyexcited about the opportunityWalmart has to driveinnovation and to furtherdisrupt the retail industry I hope you ll agreethat our work togetheris making a huge differencefor our customersand our associates Thank you for having me THOMAS KURIAN Thank you  Suresh Tens of thousands of incredibleorganizations like Walmartchoose Google s datacloud for four reasons First  Google s data cloud isthe most complete and unifieddata and AI platform to help youmanage every stage of the datalifecycle From runningoperational transactionsto developinganalytical applications we help customers unifydata lakes and warehousesas data lake houses to reduce complexity and to combine structuredand unstructured data Built in data science and AIwith support for MapReduce Spark  and Presto enablescontinuous learningand experimentation And we supportreal time streamingthat natively uses open sourcestandards  like Beam and Flink Google s data Cloud is alsounmatched for speed  scale security  and reliability BigQuery  one of the mostsuccessful and beloved datawarehouse solutions  with itsunique serverless approach during a typical weekhelps more than 3 000different organizationsanalyze more than 200 petabytesof data And thousands ofcustomers use Dataprocto run Spark in Hadoopclusters easily Spanner  a fully managedrelational databasewith unlimited scale andmulti zone and multi regionconsistency  serves more than1 billion requests per secondand provides a 5 9 SLA Google data Cloud sVertex AI offersthe AI pioneered at GoogleResearch and Deepmindso that every data scientistand ML engineer can nowbuild  deploy  and scale AImodels faster and with 80  lesscode using AI thesame way Google does For instance  GE Appliances  anappliance manufacturing leaderfor more than a century  isusing Google Cloud includingAI The Edge to buildinnovative digital productswith over the air updates Finally  Google s data cloudis more open standards based You can choose from Postgres MySQL  Redis  MongoDB or migrate fromOracle or SQL server And we have strategicpartners with leadingdata driven applications For instance  C3 AI hasmade its entire lineupof AI based applicationsavailable on Google Cloud Our customers can analyze govern  and visualize datafrom many databases andstorage systems on Googleor other clouds  includingall of Google s databases Google Sheets and BigQuery usingLooker  our premier BI andembedded analytics solution Today  we rethrilled and excitedto announce that we llbe integrating Tableau a leader in datavisualization  with Looker Google Sheets  and BigQuery Tableau customers willsoon be able to useLooker s semantic model enabling new levels of datagovernance and access to data Let s take a look at this Alex  over to you ALEKSANDRA ALEKSIC Thank you  Thomas In this demo  we re going toshow you how you can seamlesslyuse Looker and Tableautogether to answeryour criticalbusiness questions Let s say I work atan e commerce company And our Looker modelprovides a consistent viewacross the organization forour most important businessmetrics Here  for example  youcan see the definitionfor gross margin Our Looker dashboard is basedon that same semantic model Since it provides trustedinsights at a glance I can immediately seethat sales are up However  did I miss anything Let s find out Say I m more comfortableworking in spreadsheets I can now take advantage of thenew integration between Lookerand Google Sheets With a couple of clicks  Ihave access to my governedand trusted Lookerdata in Sheets And I can easily connectto my data models To understand which factorsare impacting sales I ll drag and drop fields frommy model into a pivot tableto break out sales bycategory over time Since it s pullingfrom Looker  Ican trust that all the datadefinitions are consistent I can use familiar Sheetsformulas and formattingto get insights fasterand understand howspecific products are selling We see a notabledownward trend monthover month for activewear I want to share these findingswith my marketing team  who susing Tableau Now  I can use the newTableau and Looker integrationto combine data from my Lookerdata model with marketing datato build a more comprehensiveview of my business Next  I m going toplot sales by categoryand then add marketingspend to the view Tableau makes it easy for meto drill down on relevant data Now  I can see thatsocial marketingspend was significantlylower for activewear And this is why there s adownward trend in activewearsales I m going to publish thisresult on Tableau Onlineand tag my marketing team so they can take action Leveraging integrations betweenTableau  Sheets  and Lookerallows our teams to buildpowerful visualizationsusing trusted data We can then sharethese visualizationsthroughout our organizationto drive informed decisions action  and impact Back over to you  Thomas THOMAS KURIAN  Thank you  Alex The second questioncustomers ask  do we have industry leadingtechnology infrastructure We re experiencinggreat momentumwith our openinfrastructure cloud Many of the most demandingand technically advancedcustomers in the worlddisproportionately choose us Take  for example  leadingmedia companies and streamingservices  such asUnivision and Global the largest mediagroup in Latin America They use our high performancenetworking  computing and storage to broadcastmedia  includinglive events  such as therecent Tokyo Olympics Leading telecommunicationscompaniesare also choosing Google Cloud Reliance Jio  the world ssecond largest mobile carrier is automating its 5G network Vodafone is processingover a billion networkevents each day AT T is delivering newmulti access edge computingsolutions And Tellus is migrating criticalIT and network workloadsall to Google Cloud Leading datamanagement companies including Databricks  Redis Cockroach Labs  Couchbase Elastic  InfluxData  MariaDB MongoDB  Neo4j  and SingleStoreare all growing faster on GoogleCloud than on any other cloud And they love ourstorage scalabilityand advanced infrastructure MongoDB  for instance has more net new customersvia Google Cloud Marketplacethan on any other cloud Leading cybersecurity companies such as Palo Alto Networks  INAUDIBLE  Splunk Broadcom  and ForgeRockare also choosing GoogleCloud for its performance global scale  andunderlying security Customers are migrating andtransforming SAP and VMwareworkloads to Google Cloud In just the last year  we veadded three times the numberof SAP customersas the prior year And we re accelerating PayPal  for instance  hasan SAP HANA scale out systemsupporting 40 millionbusiness transactions daily 200 billion recordstouched in just 30 seconds Mitel in Canada migrated over1 000 VMware virtual machinesin less than 90 days toGoogle Cloud VMware Engine Nylas  an API platform company chose our Tau virtual machinesalong with GoogleKubernetes Engine as they saw over 40 better price performancethan in a competitor Customers choose ouropen cloud Infrastructurefor three main reasons First  we make your pathto migrate and modernizeusing cloud easy withmigration tools  new serverlessand container capabilities and managed servicesas your developersspend more time buildingexperiences thatyour customers love Second  we offertransformative capabilitieswith cutting edgeperformance and security And our networkoffers three timesthe throughput ofother cloud providers Third  we remain  and were the very first and only cloudprovider with a clearmulti cloud strategy We provide you with aconsistent developer experiencebuilt in open source to writeonce and deploy anywhere Many customers  like Plaid a financial services leader use Anthos to deploy  operate and manage applicationsacross multiple clouds Recently  we introducedAnthos for virtual machines standardizing theway that you manageapplications across bothvirtual machines and containers For multi clouddata  BigQuery Omnilets customers analyze dataacross Google Cloud  AWS and Azure With it  Johnson  Johnson  for instance was able to combine datain Google Cloud and AWS 3using BigQuery Omni withoutneeding to migrate data Customers also wantto expand wherethey use Google Cloud  whetherin their private data centersor out on the edge For instance  some workloadscannot move to the public cloudentirely due to the need forlarge amounts of local dataprocessing  extremely lowlatency  or strict regulations To solve these challenges we re announcing todayGoogle Distributed Cloud a portfolio of hardwareand software solutions thatextend our infrastructureto the edge and even intoyour own data center To tell you more aboutit  please welcome Inesfrom our Google CloudDistributed cloud team INES ENVI  Thank you  Thomas Google DistributedCloud is a setof fully managed servicesrunning on Google s managedhardware Based on open APIsand built on Anthos it gives customers greaterdeployment flexibilityand ensures moreconsistent operationsacross hybrid and multi cloudenvironments  morethan any other Cloud provider There are fourdeployment scenarios depending on yourspecific needs Google s network edge this allows customersto leverage over140 network edgelocations around the world Operator edge  thisscenario is designedto accommodate emergingservices and applicationswith stringent latency andreliability requirementsfor a specific operator Customer edge  this scenariosupports customer owned edgelocations  such as retailstores  factory floors or branch offices  whichrequire localized computeand processes And finally  customerdata centers  this deployment option isspecific to customer owned datacenters and call facilities Google Distributed Cloudalso includes a hosted modeto run sensitiveworkloads and addressdata sovereignty  security and privacy requirements It does not require connectivityto Google Cloud at any timeto manage infrastructure services  APIs  or tooling You have the choiceto manage it yourselfor host through adesignated trusted partner Back to you  Thomas THOMAS KURIAN  Thank you  Ines Partners like HPE Dell  Cisco  and NetAppare key as we deploy GoogleDistributed Cloud globallyon best in class hardware I m pleased to announce theexpansion of our partnershipwith Network Appliancein two important ways First  NetApp isour primary partnersupporting Google s DistributedCloud storage infrastructure Second  for customers usingGoogle Cloud VMware Engine they now have preview accessfor NetApp Cloud Volumes We are the only hyperscaleto provide customerswith this choice Deep partnerships withT Systems systems and Telushave also enabled us to addressthe evolving sovereigntyrequirements for cloudtechnology in Germanyand France The next question we hear is how can we provide the bestenvironment to helpemployees create and innovatetogether  especiallyin a hybrid workplace Hybrid is redefininghow we all work making it lessabout where we work but more about thequality of our experience Studies estimate thatmore than 48  of employeeswill work remotely post COVID Google Workspace isthe best platformfor hybrid work with simple powerful  and secure toolsto help people communicate andcollaborate with one another no matter where they are We introduced Smart Canvas tohelp content creators removefriction fromworkflows and enableteams to stay connectedoutside of meetings to foster well being  andreduce video call fatigue We made communicationand collaborationcompletely seamless For instance  users caneasily present and collaboratein meetings  create meetingnotes from their calendar present in Meet directlyfrom Docs  Sheets  or Slides and collaborate directlyfrom Chat and Spaces We optimized collaborationfrom all mobile devicesto provide great experiences forfrontline workers who make up80  of the global workforce One of America s largesthospital systems CommonSpirit Health  isadvancing its missionduring the pandemic bymaking its frontline doctorsand nurses productiveat Google Workspace Since hybrid work hasremoved the notionof a physical locationfor work  Google Workspacecreated spaces  a unifieddigital location for work Today  more than 3 billion usersmove security and seamlesslybetween Mail  Chat  Audio Video  and now Spaces And you too can choose amongthe over 5 300 public appsin our marketplace And you can also addthousands of private appsto enhance your workspace And I want to stepaside for just a minuteand tell you about a hiddensecret in our portfolioof products  AppSheet our notebook platform thatmakes it easy for everyone  notjust professional developers to build amazingapps and workflows For instance  property managersat  INAUDIBLE  Propertiesused AppSheet tobuild a mobile app to log  track  and resolveproblems with headquarters replacing a multi stepmanual process We re also partneringwith Citrixbecause employees sometimesneed access to Windows and Mac OS applications and desktops directlyserved from the Cloud We re expanding ourpartnership with Citrixtoday to deliver an integrateddesktop as a service solutionpurpose built on Google Cloud Now  customers ask us  do weknow that our data  systems and users are secure And how do we keep themsecure in the future As more digital users accessmore data from more locations the risk of cybersecuritybreaches is accelerating By pioneering newapproaches to security Google keeps morepeople safe onlinethan anyone else in the world Take  for instance  JetBlue It s keeping its systemsecure  protectingthe data of all its travelers and modernizing its securityoperations using Chronicleto detect threatsacross petabytes ofsecurity telemetry eliminating data blind spots Commerzbank  one ofGermany s largest banks is using a new certificateauthority serviceto verify machine andworkload identity improving security  simplifyingoperations  eliminatingentire classes of threats Secure by defaultdrives customersto choose Google Cloud First  we veimplemented zero trustat the core of our servicesand our operations enabling you to trust nothing Second  we ve built cloud scalethreat detection and response allowing you todetect everything And third  we know it snot just about tools We provide theadvisory services youneed to transformsecurity operations We keep customers safe withBeyondCorp Enterprise  whichenables zero trustaccess for all users Today  we reexpanding BeyondCorpto all your applications  modern and legacy  weband desktop  and even toproduction environments We keep customers safe byeliminating software supplychain vulnerabilities Over the past two decades we ve pioneered an approachto secure our ownsoftware supply chain We are now makingthat technologyavailable through theSLSA Open Source Frameworkand as managedservices in our Cloudto help you secure yourown software supply chain We keep customers safe byintegrating Chronicle s threatdetection capability withour Security Command Center allowing you to respondfaster to potential risks And we keep customers safe bybringing our security expertiseto you with our GoogleCybersecurity Action Team a team of our leadingsecurity expertswho can help shape yoursecurity transformationfrom your veryfirst implementationthrough respondingto a major incidentand engineering newsolutions as needs change Finally  we continue toexpand our security ecosystemwith companies like Fortinet Palo Alto Networks   INAUDIBLE and so many more Today  I d like towelcome your new securitypartner  Cybereason  withwhom we are collaboratingto deliver a new  extendeddetection and responseoffering that willcombine Chronicle scapability withCybereason s MalOps Enginefor faster  easier threatdetection and response Our products  our newCybersecurity Action Team and our partnershipsall make the world saferfrom cyber threats To talk more about how we helpkeep each and every one of yousecure and how we readdressing someof the most importantchallenges facing our world I d like to invite Sundar back Thank you SUNDAR PICHAI Thank you  Thomas Security is top of mind forbusinesses of all sizes as well as the public sector And despite the progress thathas been made in cybersecurity large scale breachescontinue to make headlines That s why  in August Google announcedwe will invest  10 billionover the next five yearsto strengthen cybersecurity That includes expandingzero trust programs helping secure thesoftware supply chain and enhancing opensource security A new challenge iskeeping collaborationsecure and private  as we moveto hybrid work environments To help  we are introducinga new Work Safer program It provides the highest levelof security for your email meetings  messaging  andmore  and brings togetherGoogle Workspace with TitanSecurity Keys  BeyondCorp and other secure technologies products  and partners Now  every kind oforganization in businesscan access the samesecurity protectionsthat make work safer at Google Security is an issuethat affects us all And this brings us to thelast question Thomas posed Are we addressing themost important challengesfacing our world Climate change is one ofthe most profound challengeswe face It ll take all of us workingtogether to solve it Sustainability has beena core value for usfor more than two decades Google has been carbonneutral since 2007 And we ve matched our operationswith 100  renewable energyover four consecutive years Last year  we set out to makeour third decade of climateaction our biggest yet That included a boldcommitment to operateon 24 7 carbon free energyacross our offices and datacenters by 2030 And last week  weannounced new wayswe are helping 1 billion peoplemake more sustainable choices When it comes toenterprises  oneof the most importantsustainability choicesis where to run your technology IDC predicts cloud migrationsover the next four yearscould reduce carbon emissionsby over 1 billion metric tonsor the equivalent ofremoving 200 million carsoff the road for a year We are proud that GoogleCloud is the cleanest cloudin the industry Earlier this year  weintroduced a way for customersto choose the cleanest regionsto run their workloads Today  we areexpanding our portfolioof solutions and partnershipsto help reduce your carbonfootprint First  we have focused ongiving you greater transparencyaround carbon emissions data We are announcing CarbonFootprint  a new serviceto measure  track  andreport the gross carbonemissions associated withyour Google Cloud usage Having this data at yourfingertips is really important And we know that many customerstrack sustainability datain other places That s why I m happy to announcethat Google Cloud EmissionData will integrate directlyinto Salesforce SustainabilityCloud We also want to give you thetools to act on the data So today  we areintroducing a new toolthat will alert you whenyou have idle workloadsand make recommendationsto reduce carbon emissions It s all part ofour goal to helpmake your Cloud transformationsecure and sustainableand solve for what s next So on behalf ofThomas and myself let me thank you onceagain for trustingus to be a partner withyou on this journey And we hope you enjoythe next three days Thank you  MUSIC PLAYING Google Cloud NEXT  21 Google Cloud NEXT  21 Google Cloud NEXT  21 Google Cloud NEXT  21 Google Cloud NEXT  21 Google Cloud NEXT  21 Google Cloud NEXT  21 Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google CGoogle Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT next 21Google CGoogle Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT n 21Google Cloud NEXT  21Google Cloud NEXT  21Google CGoogle Cloud NEXT  21Google Cloud NEXT  21Google CGoogle Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21 Google Cloud NEXT  21 Google Cloud NEXT  21 Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21  JONATHAN CHAM  Hey there We are about six minutes fromthekickoff of Google Cloud Next and I m glad you re here I m Jonathan Cham  coming to youlive from our Sunnyvale campuswith a few of my Cloud Nextcolleagues Over the next three days  you llhave complete access to ourkeynotes  demos  Q As andsessions where you can engagewith our Google experts live And when you can t be with us inreal time  customize youron demand playlist to watch yourfavorite content any time anywhere In the next few minutes  we regoing to share some of ourGoogle Cloud Partner andCustomer Award Winners  andyou ll meet a few of oursponsors who created videointroductions just for you First up  check out howAccenture combines its industryexpertise with Google Cloud stechnology to delivertransformation At scale   JONATHAN CHAM  It s awardsseason around here  and we wantyou to recognize our partnerswho are on the front lines ofdigital innovation and drivingexceptional results forcustomers Today  I m excited to share theGoogle Cloud Partner Awards Here are our global winners innine industry categories Our reseller partner of the yearis DoiT International Services Partner of the YearDeloitte In the Industry SolutionsCategory  Capgemini The Breakthrough Award  TCS And expansion  Wix Our Technology Partner of theYear  Palo Alto Networks Our puPublic Sector  Deloitte Diversity  Equity and InclusionPartner of the Year is GoPomelo And finally  our socialSocial IPartner of the Year award goesto Atos To see all of our winnersrecognized across regions  checkout our Web site Congratulations to our globalwinners  and a big thank you toall of our Google Cloud partnersthat are enabling andaccelerating Cloudtransformation for our customersevery day Next up  meetAtos Maven Wave  a leadingGoogle Cloud digitaltransformation partner with morethan 11 partner specializationsand four time Google ServicesPartner of the Year awardwinner Most recently  the company wasnamed Global Social ImpactPartner of the Year Here s more    In the digital first world innovation has no limits Seamless digital transformationthat modernizes business  hybridwork environments for on demandcollaboration  and Cloudintegration that optimizes workflow The fastest  smoothest path todigital transformation engineered by Google Cloud  madeeasier by Maven Wave and Atos   JONATHAN CHAM  And now  I mthrilled to announce theInaugural Google Cloud CustomerAwards These awards recognize customersthat innovate and createindustry leading solutions andexceptional customer experienceswith Google Cloud There are some truly outstandingsuccess stories impacting ourworld in countless ways  whichis why it s my honor to presentto you our Customer Awardwinners of 2021 in the followingcategories Social impact Diversity  Equity   Inclusion Cross industry We have multiple winners in thiscategory Retail Again  lots of winners in thiscategory as well Manufacturing Communications and ServiceProviders Moving on to financial services Again  many winners in financialservices Gaming Healthcare and Life Sciences Media and Entertainment Government And our final customer award inthe category of education Congratulations to all ourwinners Google Cloud is proud to be apart of your story And finally  we want to thankall our sponsors for being greatpartners and continuing tosupport us  including Deloitte our four time Global ServicesPartner of the Year winner Here s how Deloitte can helpwith your digital transformationjourney  Music    JONATHAN CHAM  Thanks forwatching our Cloud Next  21pre show We d like to thank our sponsors Accenture  Atos Maven Wave  andDeloitte Thanks to our partners and ourcustomers  and congratulationsto all our award winners We hope you enjoy these threedays of Next Be sure to build your customplaylist  share it on social and join the conversation ontwitter and Linked in with GoogleCloudNext Now  stay right here The live keynote is about tobegin   ALISON WAGONFELD  Hi everyone I m Alison Wagonfeld with GoogleCloud Welcome to Google Cloud NEXT and thank you to our Googleorchestra composed of bay areaGooglers orchestra  composed of Bay AreaWe re coming to you live fromour Google Cloud campus inSilicon Valley  where I m herewith Sundar Pichai and ThomasKurian We have an incredible lineupover the next three days withlive keynotes  demos and Q Awith our leadership  and over140 sessions covering all ourCloud solutions Our customers and partners willshare how they re using GoogleCloud and Google Workspacetechnology to tackle theirgreatest challenges andopportunities We re humbled to support theirwork and share their stories Thank you for being here withus Now please join me in welcomingGoogle and Alphabet CEO  SundarPichai   SUNDAR PICHAI  Hi  everyone Welcome to Google Cloud Next2021 We re happy you re here Of course  I d be happier if youcould actually be here  inperson Don t get me wrong  Thomas isGreat company  but he stilldoesn t laugh at my containerjokes So while many of us are stillwaiting for a return to normal it does feel like we re on ourway The question is  will werecognize normal when we getthere The pandemic has sped updigitization in all aspects ofour lives It s changed how we visit thedoctor  how our kids learn inschool  and how we connect withone another And it s made the future of workour present  giving us renewedopportunity to transform ourorganizations From our work with customers Thomas and I have learned thereare three things that positionus to be most helpful tobusinesses  First  our Cloudplatform It s designed to helpenterprises transform throughdigitization  built on the deepinvestments we ve made intechnical infrastructure overthe past 23 years Looking at our networking anddata center investments alone we have the largest network withthe lowest latency of any Cloudprovider With our expansions into Warsaw Delhi  Melbourne and Torontothis year  we now have 28regions  with plans for tenmore And it s all connected by 19subsea cables Our Grace Hopper trans Atlanticcable will be the first toincorporate novel optical fiberswitching Second  our enterprise customersBenefit from our consumer scaleand innovation We have a deep and broadconsumer ecosystem of devicesand services YouTube connects a globalcommunity of viewers andcreators Android  Nest  Assistant  andMaps are loved by people aroundthe world We have the unique ability toconnect consumer ecosystems withenterprise ecosystems For example  the realisticvoices and languageunderstanding that powers GoogleAssistant can improve enterprisecustomer experiences andincrease satisfaction And the underlying visualtechnology that enables GoogleLens to help people shop orLearn about a new flower intheir garden can helpmanufacturers identify defects Third  we offer comprehensivesecurity that helpsorganizations  including ours protect what matters Security is foundational toeverything we do We have a long history ofbuilding it into every layer ofthe computing environment We re a pioneer of zero trustcomputing  and have deepexperience running this model atscale On its own  any one of thesepillars would be helpful tocustomers Put together  they can betransformative  which is themission of Google Cloud We continue to drive innovationthrough our products Over the past year  we ve hadmore than 1 500 product andfeature releases That s about four a day They re all designed to behelpful  from Smart Canvas inGoogle Workspace that makescollaboration richer  to newAI powered industry solutions and Big Query Omni for dataanalytics across any Cloud We ve also made significantpartnership announcements For example  we teamed up withglobal telecom leaders such asEricsson  Nokia and T Systems todeliver 5G edge and hostedcomputing solutions What I m most excited about ishow our strengths in Cloud Consumer and Security arehelping top enterprisesaccelerate their transformation Ford chose Google as theirtransformation partner becauseof our strength in consumer andcloud Together  we re innovating inelectrification  connectivityand self driving technologies Ford is harnessing Google Mapsto help drivers navigate  GoogleAssistant to help driversconcentrate  and Google Play tokeep passengers learning andentertained L Oreal partnered across Googleto build a virtual try onexperience They are harnessing the best AImodels as well as YouTube tobuild a community of fans  andGoogle Shopping to make it easyto discover and buy theirproducts We are humbled that so manyorganizations continue to puttheir trust in Google Cloud And we are committed to helpingyou solve the biggest challengesyou face I ll be back a little later totalk more about how we do that But first  I ll turn the stageover to Thomas Kurian  CEO ofGoogle Cloud  to talk more aboutour vision   THOMAS KURIAN  Thank you Sundar Hello  everyone It is a pleasure to be here onGoogle s Silicon Valley campustoday for the moment we all lookforward to all yearlong  theopportunity to connect with you our customers  partners anddevelopers  and to share all theamazing  exciting stuff we vebeen working on A special thank you to all oursponsors  especially ourLuminary Sponsors  Accenture Atos Maven Wave  and Deloitte Let s first hear from some ofour customers about the amazingways in which they haveaccelerated transformation withGoogle Cloud    The pandemic really didaccelerate the digital wave ofchange by easily three to fiveyears    We transitioned to allvirtual care    We re better positioned toaccelerate    Cross over to today  we havea truly profound permanentstructural transformation    Now our team with look athundreds of calls in minutesinstead of weeks    Our business has really beentransformed with the help ofGoogle Cloud    We have spikes in demand We can scale four times in lessthan five minutes    Transforming the wayscientists are developing drugs chemical spaces almost inspint We can do things we couldn t dobefore    We think we can help solvethat for the supply chain    How can we help workers withartificial intelligence    Suddenly  lots of ideasbecome interesting    We re very excited to developour artificial intelligence andmachine learning    Access to innovation  thespeed of innovation    We re rethinking  retainingand pioneering simplicity    The partnership with GoogleCloud provides us withtechnology that will drive us tobe better   THOMAS KURIAN  Now  it is nota secret that many of thebiggest and most innovative techcompanies  leaders such asSpotify  twitter  Shopify Mercado Libre  ShareChat  DapperLabs  and so many other excitingstartups and tech leaders chooseGoogle Cloud to help them build scale and innovate Still  other customers aretransformingusing our purpose built industrysolutions The Home Depot  for instance used contact center AI to reducecustomerresolution time by 91 millionminutes Procter and Gamble createspersonalized omni channeljourneys from over 275 millionconsumer records IKEA Retail used RecommendationsAI to increase eCommerceclick thru rates by 30 percent And increase order value by 2percent FIH  a Foxconn company  usedVisual Inspection to identifydefects ten times moreaccurately Mr  Cooper  a leading mortgageservices provider  used DocumentAI to process mortgage documents400 percent more efficiently And bank of network Mellonliquidity solution that predictsnearly 40 percent of securitiessettlement failures with 90percent accuracy We are helping MANY businessesdrive durable innovation throughlong term partnerships withGoogle s Transformation Cloud To help them transform  we focuson five key themes One  are we the best atunderstanding and using data inour industry Two  do we have theIndustry s leading technologyinfrastructure Three  are we creating the besthybrid workplace for ourpassionate and talentedemployees Four  do we know that our data systems and users are securenow  and that they will besecure in the future And five  are we workingtogether to address the mostimportant challenges facing ourworld We help organizations addressthese important questions Let s start with the firstquestion Are we the best at understandingand using data in our industry We help organizations unifytheir data across multipleclouds and silos  combineunstructured and structuredData and making data everyemployee s super power Using Data Analytics and AItogether  Schrodingeraccelerated clinical drugdiscovery by 60 percent findingnew breakthroughs faster Like many states around theUnited States  the WisconsinDepartment of WorkforceDevelopment confronted a wave ofnew unemployment claims Using Google s data cloud  theyclear over 750 000 unemploymentclaims  reducing claim time fromweeks to days  and disbursingover  2 billion in unemploymentbenefits Wal Mart is transforming theexperience for the approximately220 million shoppers around theglobe who visit a Wal Martstore  club or eCommerce Website each week I m honored today to welcomeSuresh Kumar  executive vicepresidentPresident  Wal Mart s GlobalChief Technology Officer  andChief Development Officer    Hi  Thomas  and hello everyone I m excited to be here today toshare about how we at Wal Martare innovating on behalf of ourcustomers  our associates  andthe role that our partners likeyou  Google  have played in ourinnovation journey Now  I came to Wal Mart at atime of incredible digitaltransformation  and jointly withmy team  created a robustexecution plan to accelerate thetransformation across threeareas  Building great customerexperiences  optimizing ourbusiness  and modernizing ourplatform and our infrastructure Each of these three prioritiesare individually important  butwhen you bring them together  inthe scale of a business likeWal Mart  it has a massiveimpact If you move at speed  you cancompletely transform globaloperations and disrupt entireindustries Some of our most data intensiveare critical decisioning processare getting the big credittreatment Through this  not only are weseeing significant savings likewe ll continue as we migratemore and more data over  we arealso seeing the ability to usethe data in interesting ways including enabling analytics atscale  and turning data intoactionable insights From a data migration to bigQuery standpoint  97 percent ofthe tables used for datawarehouse have already migratedto the cloud  and 30 percent ofwith plans to almost double thatby the end of this fiscal year And the cherry on the top isthat BigQuery has allowed us tointegrate with pretty much anydata visualization tool andanalytics tool that s out there Thereby  improving ourprocessing time by 23 percent and  of course  needless to say a much better user experience Using BigQuery has had a directimpact on our business And a current example is ourability to close our financialbooks in three days instead offive days  and that s a prettybig deal Leveraging our Cloud has enabledus to unleash the potential ofAI across our entire business This goes from predicting demandto managing in stock levels tooptimizing supply chain  tofreeing up time for ourassociates to serve ourcustomers As one example  express deliverywhich is our two hour deliveryservice  which we launched lastyear  actively uses AI behindthe scenes as the customer iscreating their order MuMultiple    work in unison tooptimize the delivery route anddetermine if the customer iseligible for express delivery We are building our own AIcapabilities to power multipleareas of our business includingclassification  natural languageprocessing  forecasting regression  computer vision predictions  process automationsand a whole lot more So to wrap up  I m reallyexcited about the opportunityWal Mart has to drive innovationand to further disrupt theretail industry I hope you will agree that ourwork together is making a hugedifference for our customers andour associates Thank you for having me   THOMAS KURIAN  Thank you Suresh Tens of thousands of incredibleorganizations  like Wal Mart choose Google s Data Cloud forfour reasons  First  Google sData Cloud is the most completeand unified data and AI platformto help you manage every stageof the data life cycle  fromrunning operational transactionsto developing analyticalapplications We help customers unify datalakes and warehouses as datalakehouses to reduce complexityand to combine structured andunstructured data Built in Data Science and AIwith support for MapReduce Spark and Presto enablecontinuous learning andexperimentation And  we support real timestreaming that natively usesopen source standards like Beamand Flink Google s Data Cloud is unmatchedfor speed  scale  security andreliability BigQuery  one of the mostsuccessful and beloved datawarehouse solutions with itsunique serverless approach during a typical week helps morethan 3 000 differentorganizations analyze more than200 petabytes of data And thousands of customers useDataproc to run Spark andHadoop clusters easily Spanner  a fully managedrelational database withunlimited scale and multi zoneand multi region consistencyserves more than one billionrequests per second and providesa five 9 SLA Google Data Cloud s Vertex AIoffers the AI pioneered atGoogle Research and DeepMind  sothat every data scientist and MLengineers can build  deploy andscale AI Models faster and with80 percent less code  using AIthe same way Google does For instance  GE Appliances  anappliance manufacturing leaderfor more than a century  isusing Google Cloud  including AIat the edge  to build innovativedigital products withover the air updates And finally  Google s Data Cloudis more open standards based You can choose from Postgres  MySQL  Redis  MongoDB  or migratefrom Oracle or SQL Server And we have strategicPartners with leadingdata driven applications For instance  C3 AI has made itsentire lineup of AI softwareBased applications available onGoogle Cloud Our customers can analyze govern and visualize data frommany databases and storagesystems on Google or otherClouds  including all ofGoogle s databases  GoogleSheets  BigQuery  using LOOKER our premier BI and embeddedanalytics solution You know  today  we are thrilledand excited to announce that wewill beintegrating Tableau  a leader indata visualization  with Looker Google Sheets  and BigQuery Tableau customers will soon beable to use Looker s semanticmodel  enabling new levels ofdata governance and access todata Let s take a look Aleks  over to you In this demo  we re going toshow you how you can seamlesslyuse Looker and Tableau togetherto answer your critical businessquestions Let s say I work at an eCommercecompany and our Looker modelprovides a consistent viewacross the organization for ourmost important business metrics Here  for example  you can seethe definition for gross margin Our Looker dashboard is based onthat same semantic model Since it provides trustedinsights at a glance  I canimmediately see that sales areup However  did I miss anything Let s find out Say I m more comfortable workingin spreadsheets I can now take advantage of thenew integration between Lookerand Google Sheets With a couple of clicks  I haveaccess to my governed andtrusted Looker data in Sheets and I can easily connect to mydata models To understand which factors areimpacting sales  I ll drag anddrop fields from my model into apivot table to break out salesby category over time Since it s pulling from Looker I can trust that all the datadefinitions are consistent I can use familiar Sheetsformulas and formatting to getinsights faster and understandhow specific products areselling We see a notable downward trendmonth over month for activewear I want to share these findingswith my marketing team  who susing Tableau Now I can use the new Tableauand Looker integration tocombine data from my Looker datamodel with marketing data tobuild a more comprehensive viewof my business Next  I m going to plot sales bycategory  and then add marketingspend to the view Tableau makes it easy for me todrill down on relevant data Now I can see that socialmarketing spend wassignificantly lower foractivewear  and this is whythere s a downward trend inactivewear sales I m going to publish this resulton Tableau online and tag mymarketing team so they can takeaction Leveraging integrations betweenTableau  Sheets  and Looker allows our teams to buildpowerful visualizations usingtrusted data We can then share thesevisualizations throughout ourorganization to drive informeddecisions  action  and impact Back over to you  Thomas   THOMAS KURIAN  The secondquestion customers ask is  do wehave industry leading technologyinfrastructure We re experiencing greatmomentum with our OpenInfrastructure Cloud Many of the most demanding andtechnically advanced customersin the world disproportionatelychoose us Take  for example  leading mediacompanies and streamingservices  such asUnivision and Globo  the largestmedia group in Latin America use our very high performancenetworking  computing andstorage to broadcast media including live events such asthe recent Tokyo Olympics Leading telecommunicationscompanies are also choosingGoogle Cloud Reliance Jio  the world s secondlargest mobile carrier  isautomating its 5G network Vodafone is processing over abillion network events each day AT T is delivering newmulti access edge computingsolutions  and Telus ismigrating critical IT andnetwork workloads  all to GoogleCloud You know  leading datamanagementcompanies  including Databricks Redis  Cockroach DB  Couchbase Elastic  Influx Data  MariaDB MongoDB  Neo4J  and Singlestoreare all growing faster on GoogleCloud than on any other CloudAnd they love our storagescaleability and advancedinfrastructure MongoDB  for instance  has morenet new customers via GoogleCloud Marketplace than any otherCloud Leading cybersecurity companies such as Palo Alto Networks Exabeam  F5 Shape  Splunk Broadcom  and Forgerock are alsochoosing Google Cloud for itsperformance  global scale andunderlying security Customers are migrating andtransforming SAP and VMwareworkloads to Google Cloud In the last year  we ve addedthree times the number of SAPcustomers as the prior year  andwe re accelerating Paypal  for instance  has anSAP HANA scale out supporting 40million business transactionsdaily and 200 billion recordstouched in 30 seconds Mitel  in Canada  migrated over1 001 000VM wear virtual machine inless than 90 daysto Google Cloud VMware Engine Nylas  an API platform company chose our Tau VM s with GoogleKubernetes Engine  as they sawover 40 percent better priceperformance than on acompetitor Customers choose our open Cloudinfrastructure for three mainreasons  First  we make yourpath to migrate and modernizeusing the Cloud easy withmigration tools  new serverlessand container capabilities  andManaged services as yourdevelopers spend more timebuilding experiences that yourcustomers love Second  we offer transformativecapabilities with cutting edgeperformance and security And our network offers threetimes the throughput of otherCloud providers Third  we remain  and were  thevery first and only Cloudprovider with a clear multi cloud strategy We provide you with a consistentdeveloper experience built onopen source to write once anddeploy anywhere Many customers like Plaid  afinancial services leader  useAnthos to deploy  operate andmanage applications acrossmultiple clouds Recently  we introduced Anthosforvirtual machines  standardizingthe way you manage applicationsAcross both virtual machines andcontainers For multi cloud data  BigQueryOmni lets customers analyze dataacross Google Cloud  AWS andAzure With it  Johnson   Johnson wasable to combine data in GoogleCloud and AWS S3 with BigQueryOmni without needing to migratedata Customers also want to expandwhere they use Google Cloud whether in their private dataCenters or out on the edge For instance  some workloadscannot move to the public cloudentirely due to the need forlarge amounts of local dataProcessing  extremely lowlatency  or strict regulations To solve these challenges  we reAnnouncing today  Googledistributed Cloud  a portfolioof hardwareand software solutions thatextend our infrastructure to theEdge and even into your own datacenter To tell you more about it please welcome Ines from ourGoogle Cloud distrDistributed CTeam    Thank you  Thomas Google Distributed Cloud is aset of fully managed servicesrunning on Google managedhardware Based on open API s and built onAnthos  it gives customersgreater deployment flexibilityand ensures more consistentoperations across hybrid andmulti cloud environments More than any other Cloudprovider There are four deploymentscenarios depending on yourspecific needs Google s Network Edge This allows customers toleverage over 140 network edgelocations around the world Operator Edge This scenario is designed toaccommodate emergency servicesand applications with astringent latency andreliability requirements for aspecific operator Customer edge  this scenariosupports customer s own edgelocations such as retail stores factory floors or branchoffices  which require localizedcompute and processes And finally  customer datacenters This deployment option isspecific to customer owned datacenters and co facilities Google Distributed Cloud alsoincludes a hosted mode to runsensitive workloads and addressdata  sovereignty  security andprivacy requirements It does not require connectivityto Google Cloud at any time tomanage infrastructure  services APIs or tooling You have the choice to manage ityourself  or host through adesignated trusted partner Back to you  Thomas   THOMAS KURIAN  Thank you Ines Partners like HPE  Dell  Ciscoand NetApp are key as we deployGoogle Distributed Cloudglobally on best of classhardware I am pleased to announce theexpansion of our NetApppartnership in two ways First  NetApp is our primarypartner supporting Google sDistributed Cloud storageinfrastructure Second  for customers utilizingGoogle Cloud VMware Engine  theynow have preview access forNetApp Cloud Volumes We are the only hyperscaler toprovide customers with thischoice Deep partnerships with T Systemsand Thales have also enabled usto address the evolvingsovereignty requirements forCloud technology in Germany andFrance The next question we hear is how can we provide the bestenvironment to help employeescreate and innovate together especially in a hybridworkplace Hybrid is redefining how weAll work  making it less aboutwhere we work  but more aboutthe quality of our experience Studies estimate that more than48 percent of employees willwork remotely post COVID Google Workspace is the bestplatform for hybrid Work withSimple  powerful and securetools to help peoplecommunicate and collaborate withone another  no matter wherethey are We introduced Smart Canvas tohelp content creators removefriction from work flows andenable teams to stay connectedoutside of meetings to fosterwell being and reduce video callfatigue We made communication andCollaboration completelyseamless For instance  users can easilypresent andcollaborate in meetings  createmeeting notes from Calendar present in Meet directly fromDocs  Sheets or Slides  andcollaborate directly from Chatand Spaces We optimize collaboration fromall mobile devices to providegreat experiences for frontlineworkers  who make up 80 percentof the global workforce One of America s largesthospital systems  Common SpiritHealth  is advancing its patientcare mission during the pandemicby making its front line doctorsand nurses productive withGoogle Workspace Since hybrid work has removedthe notion of a physicallocation for work  Google wWorkspace created spaces  aunified digital location forwork Today  more than 3 billion usersmove securely and seamlesslybetween Mail  Chat  Audio  Videoand now spaces And you  too  can choose amongthe over 5 300 public apps inour marketplace and you can alsoadd thousands of private apps toenhance your work space Now  I want to step aside forjust a minute and tell you abouta hidden secret in our portfolioproducts Appsheet Our local platform that makes iteasy for everyone  not justprofessional developers  tobuild amazing apps and workflows For instance  property managersat car for properties usedAppsheet to build a mobile appto log  track and resolveproblems with headquarters Replacing a multi step manualprocess We re also partnering withCitrix  because employeessometimes need access to windowsand Mac OS applications and desktops  directly served from thecloud We re expanding our partnershipwith Citrix today to deliver anintegrated desk top as a servicesolution  purpose built onGoogle Cloud Customers ask us  do we knowthat our data  systems and usersare secure And how do we keep them securein the future You know  as more digital usersaccess more data from morelocations  the risk of cybersecurity breaches isaccelerating By pioneering new approaches tosecurity  Google keeps morepeople safe online than anyoneelse in the world Take  for instance  JetBlue It s keeping its systems secure protecting the data of all itstravellers  and modernizing itssecurity operations useChronicle to detect threatsacross petabytes of securitytelemetry  and eliminating datablind spots Commerzbank  one of Germany slargest banks  is using our newCertificate Authority Service toverify machine and workloadidentity  improving security simplifying operations andeliminating entire classes ofthreats Secure by default drivescustomers to choose GoogleCloud First  we have implemented zerotrust at the core of ourservices and our operations enabling you to trust nothing Second  we have builtcloud scale threat detection andresponse  allowing you to detecteverything And third  we know it s not justabout tools We provide the advisor servicesyou need to transform securityoperations We keep customers safe withBeyondCorp Enterprise whichenables Zero Trust Access forall users Today  we re expanding BeyondCorp to all your applications modern and legacy  Web and desktop  and even to productionenvironments We keep customers safe byeliminating software supplychain vulnerabilities Over the past two decades  wepine neared an approach tosecure our own software supplychain We are now making thattechnology available through thesalsa open source framework  andas managed services in our cloudto help you secure your ownsoftware supply chain We keep customers safe byintegrating chronicle s threatdetection capability  with oursecurity Command Center allowing you to respond fasterto potential risks And we keep customers safe bybringing our security expertiseto you rs with our Googlecybersecurity Action Team  ateam of our leading securityexperts who can help shape yoursecurity transformation  fromyour very first implementation through responding to a majorincident  and engineering newsolutions as needs change Finally  we continue to expandour secure ecosystem  withcompanies like Fortnet  PaloAlto Networks  Thales  and somany more Today  I would like to welcome anew security partner cyberreason  with whom we arecollaborating to deliver a newextended Detection and Responseoffering that will combineChronicle s capability withCybereason s Mal Ops engine forfaster  easier threat detectionand response Our products  our newCybersecurity Action Team andour partnerships all make theworld safer from cyberthreats To talk more about how we helpkeep each and every one of yousecure  and how we re addressingsome of the most importantchallenges facing our world  I dlike to invite Sundar back Thank you   SUNDAR PICHAI  Thank you Thomas Security is top of mind forbusinesses of all sizes  as wellas the public sector And despite the progress thathas been made in cybersecurity large scale breaches continue tomake headlines That s why  in August  Googleannounced we will invest  10billion over the next five yearsto strengthen cybersecurity That investment includesexpanding zero trust programs helping secure the softwaresupply chain  and enhancingopen source security A new challenge is keepingcollaboration secure andprivate  as we move to hybridwork environments To help  we re introducing a newwork Safer Program It provides the highest level ofsecurity for your E mail meetings  messaging and more and brings together GoogleWorkspace with Titan SecurityKeys  BeyondCorp  and othersecure technologies  productsand partners Now every kind of organizationand business can access the samesecurity protections that makework safer at Google Security is an issue thataffects us all And this brings us to the lastquestion Thomas posed Are we addressing the mostimportant challenges facing ourworld Climate change is one of themost profound challenges weface  and it will take all of usworking together to solve it Sustainability has been a corevalue for us for more than twodecades Google has been carbon neutralsince 2007  and we ve matchedour operations with 100 percentrenewable energy over fourconsecutive years Last year  we set out to makeour third decade of climateaction our biggest yet That included a bold commitmentto operate on 24 7 carbon freeenergy across our offices anddata centers by 2030 And last week  we announced newways we re helping one billionpeople make more sustainablechoices When it comes to enterprises one of the most importantsustainability choices is whereto run your technology IDC predicts Cloud migrations over the next four years  couldreduce carbon emissions by over1 billion metric tons  or theequivalent of removing 200million cars off the road for ayear We are proud that Google Cloudis the cleanest cloud in theindustry Earlier this year  we introduceda way for customers to choosethe cleanest regions to runtheir workloads Today  we are expanding ourportfolio of solutions andpartnerships to help reduce yourcarbon footprint First  we are focused on givingyouyou greater transparency aroundcarbon emissions data We re announcing carbonfootprint  a new service tomeasure  track and report thegross carbon emissionsassociated with your GoogleCloud usage Having this data at yourfingertips is really important And we know that many customerstrack sustainability data inother places That s why I m happy to announcethat Google Cloud s emissiondata will integrate directlyinto Salesforce s SustainabilityCloud We also want to give you thetools to act on that data So today  we re introducing anew tool that will alert youwhen you have idle workloads andmake recommendations to reducecarbon emissions It s all part of our goal tohelp make your Cloudtransformation secure andsustainable  and solve forwhat s next So on behalf of Thomas andmyself  let me thank you onceagain for trusting us to be apartner with you on thisjourney And we hope you enjoy the nextthree days Thank you    Hello  and welcome to ourvery first spotlight I hope you enjoyed the openingkeynote For the next hour  we will bediving in to all things data I m Stephanie Wong  a developeradvocate  and I m here withGerrit Kazmaier  who leadsdatabase  analytics and BIsolutions at Google Cloud    Thanks  Stephanie I am very excited to be here and to share with everyone whatwe have been working on The team thought we start on thelight side of things here  andstart with a joke One of my favorite jokes actually So here it goes A SQL Query walks into a bar andapproaches two tables and asks can I join you  Drum beat     Okay I guess we roll the videoinstead    Innovation is a hugechallenge in baseball The current MLB fan expects usto be relevant    Customers call us Patients call us Members call us And you can imagine a teamthat s just buried under theweight of thousands of callrecordings    We look at so much data Everything you can imagine  weanalyze    Frankly  we spend 70 percentof our time just trying tocapture it  and trying to siftthrough it    We re expecting to transferor run 300 to 400 petabytes tothe cloud It s hard to even describe howbig that is    Google Cloud was veryimpressive to us It allowed us flexibility instorage and compute scaling    Google Cloud provides us withtechnology that will drive us tobe better    Now our team can look athundreds of calls a day  inminutes instead of weeks    With Google Cloud  we regoing to be able to spend ourtime analyzing real time andusing the data that we ve beencollecting    We thought Google Cloud wasthe best partner for us    I think this is very muchgoing to be an open andevolving  long term relationshipbetween Google and twitter    It s a perfect fit withGoogle Cloud as we go forward    Google s made it possible forus to reach exceptional customerexperience in a way that hasn tbeen done before    And welcome back So what are we solving fortoday This spotlight session is abouthow to leverage data as yourmost strategic business asset especially as you adapt to andanticipate change Data has the power to transformstagnant businesses intoprofitable ones  streamlineoperations to improve people slives  and identify areas ofwaste to radically reduceunnecessary spending It can be used to solvebusinesses and world problems both the simple ones and theones that keep people up atnight It has the potential toaccelerate progress faster thana think tank or world summit And it s largely untapped As you learn more about GoogleCloud capabilities and makeprogress on your data Cloudjourney  you can unlock your own10X mindset as external factorsaccelerate the pace of change Never before have we faced thismuch uncertainty  and now is thetime when every company can turndata into an ability The reality today is that toomany companies struggle toremove the barriers to turn datainto intelligence    This is exactly what we areseeing with our customers turning data into intelligenceand delivering value is nolonger a nice to have It is essential A recent study shows that only32 percent of the companiessurveyed gain value from theirdata investments  and this isthe result ofdifficult to access  slow unreliable  complex andfragmented systems Many companies just feeloverwhelmed with the demand forthe various types of workingwith data  and of doing it atscale They feel walled in with a lackof integration  a lack ofopenness  a lack of portability and very poor APIs It is time to re imagine ourdata cloud strategy that isunlocking your data advantage Build great applications thatunderstand and interact withyour customer  and operationaldatabases that scale to billionsof queries per second  react onany event in real times andprocess millions of datastreams  analyze petabytes ofdata without sizing or tuningyour data warehouse anymore  andultimately  optimize yourprocesses with hundreds ofmachine learning models inproduction This is how you unlock your datacloud and unlock your dataadvantage To build your data cloud  youneed your systems to be simple And this requires a partner thatmeets you where you are today and helps you to mature whereyou want to be  but at your ownpace Making the complex simple is ourbusiness It has been for decades building systems that mask thecomplexity of the entireinternet behind a simple  singleinterface  and buildingapplications with billions ofusers It all runs on data andanalytics So you can say that data is ourDNA  and we can t predict thefuture  but we can plan for it And we are deeply grateful towork with tens of thousands oforganizations who built theirdata clouds on Google Cloud andunlocked ultimately their dataadvantage from databases toanalytics to streams to lakes toAI  and built a unified datacloud together with us    It s leaders like you who areaccelerating their use of datato transform complexity intosimplicity This week at Next  you ll hearfrom leading organizations thathave chosen Google Cloud fortheir data Cloud strategy overmarket alternatives Verizon Media is one of thesecustomers Verizon uses BigQuery and Lookieto scale their massiveenterprise media warehouse supporting monthly scans withmore than one exabyte of data The performance improvementsinclude 90 percent plusproductivity improvements acrossthe board  and lower overall TCOcompared to other Cloudproviders Almost overnight  traffic toChess com tripled The pandemic and successfulseries   The Queen s Gambit created a perfect storm Ten years worth of growthoccurred in just six months  anddaily active users rose from280 000 to more than onemillion There s no way to handle thatsurge without the move to GoogleCloud Using BigQuery s near infiniteability to scale in response todemand  this meant thatChess com didn t have toforecast site traffic and worryabout unexpected surges or wastemoney by over provisioningservers that go unused    Take Auto Trader It s U K  s largest onlineautomotive marketplace and theywere able to migrate more than60 percent of their currentOracle footprint to Cloud SQL Auto trader builds their datacloud by leveraging Cloud SQL BigQuery and Looker to deliveraccess to their data to all oftheir users  and with Cloud SQLas a fully managed service  AutoTrader release cadence hasimproved by over 140 percentyear over year  enabling animpressive peak of more than 450releases to production on asingle day And this is how you build yourdata advantage    When Cardinal Health migratedto Google Cloud  they enableddisaster recovery  increasedtheir ability to respond tobusiness challenges  andleveraged machine learning todrive business efficiencies  allwhile reducing their cost by 25percent We would like to welcome RayBajaj CTO of Cardinal Health toshare their data transformationstory Ray  thank you so much forjoining us here today    Thanks  Stephanie And I m really happy to be heretoday Cardinal Health is a pioneer inhealthcare industry forleveraging the Cloud as ourprimary source ofinfrastructure  achieving ourCloud transformation journeywith more than 80 percent of ourworkload fully migrated on tothe Cloud Our journey to Google Cloudstarted in 2018  leveraging thesecurity and flexibility ofGoogle Cloud allows us to scaleand innovate at lower cost which empowers the organizationto focus on building the futureof healthcare and driving betteroutcomes across the healthcareecosystem    So why Google Cloud    With Google  we can delivermodern data insightsCapabilities to our engineeringteams so that they can transformourwhole business  while alsoimproving the customerexperience  helping theorganization evolve in adjacentareas of healthcare We also selected Google Cloudbecause of its ability topartner with SAP and run massiveERP workloads reliably    Now  what outcomes have youbeen able to realize as a resultof this    We leverage Google s Lookerand BigQuery to develop episodeanalytics  a data driven predicted cost tracking toolthat enables community andquality practices to accuratelymeasure the cost of care andprovides clearer insights in tovalue based performance    Well  ray  Cardinal isclearly at the forefront ofthese shifts  and we can t waitto see what s in store for you So thank you so much for beinghere with us today    Thank you    Organizations like CardinalHealth are solving complexchallenges with Google Cloud But what is holding others backfrom using data to accelerateinnovation Companies are looking forsolutions that can simplifytheir data landscape to movefrom a state where everyscenario requires a new tool toan agile platform that combinesdata analytics  databases  dataengineering and science security and governance into asingle platform To move from data at rest to aplatform where data isaccessible in real time    Our vision is to provide aplatform that allows data tounfold its full potential Ultimately  enabling us to reachnew horizons ranging from newlybuilt digital experiences products and business models toadvanced machine learning andAI  to ultimately empoweringhumans to reason beyond theirown biological capacity And we have built the industry smost simple  intelligent andbest integrated cloud dataplatform that enables businessesto fuel their own digitaltransformation And this is how you unlock yourdata advantage  to seize newopportunities  improveoperational efficiencies anddeliver new use experiences thatdelight customers  partners andyour employees Thanks to you  our customers andpartners  Google s data platformis one of the fastest growingclouds fueled by the demand forproducts like BigQuery And here comes the most excitingpart Today  we are excited toannounce a number of new productinnovations And the first announcement goesout to all of the data engineersin the world who are using Sparkon Google Cloud Open standards are at the heartof our Data Cloud strategy  andat Google  we follow an openapproach that enablesflexibility and choice And today  I m excited toannounce Project Big Spark It is the world s firstautoscaling serverless Sparkservice in existence And regardless of whether youare a BigQuery user  a data useror a cloud AI user  we havecreated an integrated ecosystemaround spark that the factorstandard for data engineeringworkloads We want to make it easy for youto onboard on our platform ofSpark and never limit yourambition with data science anddata engineering    Our long history in the opensource community has shown usthat betting on the communityfirst is always the rightstrategy to give our customersportability and optionality We re excited to make Spark afirst class citizen on ourplatform  and to automate theproductionalization of our dataengineering and data scienceworkloads    The next announcement goes toall of the data scientists As enterprises consolidate theirdata platforms  the distinctionbetween advanced analytics anddata science and machinelearning is really diminishing Today  more than ever  datascientists are looking forunified interface to be able toseamlessly connect their entiredata estate  and also run allexplorations with SQL  Spark andauto machine learning libraries With Vertex AI  we havesimplified the experience fordata scientists with 80 percentless code required to train amodel compared to any otherplatform As a product of Google s VertexAI s credit sheet  I am happy toannounce the preview of AINotebook for Vertex AI  thatoffers a fully managed  scalableand enterprise grade IDEcombined with easily enforceablepolicies and user management    These AI Notebook innovationsare great examples of Googlebringing together the power ofdata and AI Google truly aims to fast trackinnovation by eliminating theneed for data scientists toshift context between aspectslike data analytics  datascience and machine learning    That is right on And here is an excitingannouncement to all of thedevelopers We are adding a Postgresinterface for Spanner  whichcontinues our commitment toopenness and reinforces ourefforts to democratize theaccess to Spanner With this new Postgresinterface  enterprises can takeadvantage of Spanner s unmatchedglobal scale  five 9 s ofavailability  and strongconsistency  using skills andtools from the popular Postgresecosystem  and starting today you can create a Spannerdatabase that uses Postgres SQLdialect and connect with itsopen wire protocol This interface supportsSpanner s rich feature set using the most popular Postgresdata types and SQL features  andthus  democratizing access toSpanner for millions ofdevelopers out there Our commitment to the ecosystemhas been longstanding  withongoing support for Cloud SQL for my SQL  Postgres  and SQLServer  so you can lift andshift your databases Our mission with Cloud SQL iscompatibility  and we were thefirst cloud provider to supportPostgres 13    These announcements are soexciting  because Postgres hasemerged as a popular choice foroperational databases Postgres is known and loved forits stability and feature setand has built an active andgrowing ecosystem of developersand tools Our investments in Postgres area step in the right direction tocontinue meeting developerswhere they are  with the toolsthey love    We are excited to announcenew ways to help organizationsgoing beyond traditionalbusiness intelligence toanalyze  access and act on data Looker continues to integratewith the other offerings in ourportfolio  as well as continuesto innovate with our coreproduct And to address the needs  we arecontinuing to simplify theexperience across BigQuery Looker and Data Studio  togetherwith Connected Sheets Today  we are announcing newenhancements which bring thereality of augmented analyticsto more people with Lookersolution for Contact Center AIto help businesses leverage AIand converse naturally withcustomers and deliveroutstanding experiences We are also announcing a newLooker block for healthcare NLPAI  which creates a userinterface to advance AI andnatural language functionality and this helps healthcareproviders to gain deeperunderstandings of their patientdata    Part of the secret sauce liesin Looker s in databasearchitecture and semantic layerwhich makes it possible forbusiness users to trust data  nomatter how they experience it via a dashboard  E mail or textalert  or even a customapplication We are excited for these recentinnovations  but we couldn t dothis alone Together with our community ofcustomers  partners anddevelopers  we are deliveringinnovation across a variety ofuse cases We are able to create newsolutions and accelerate yourjourney to transform data in tovalue    Next  I want to announce animportant partnership withInformatica We have partnered withInformatica to help hundreds ofcustomers succeed with theirjourney to the cloud Together  we are helpingcustomers move mission criticalworkloads to the cloud andintegrate with Google Cloudtechnologies  such as BigQuery Today  we are excited toannounce that we are deepeningour partnership with Informaticaeven more First  for existing Informaticapower center customers  we areso pleased to announce ourmigration factory  which willhelp to accelerate migration ofdata driven workloads to GoogleCloud with Informatica Cloud Second  we are pleased toannounce our new services orGoogle Cloud s Marketplace which will include Informatica sintegration and enterprise datacatalog offerings And we are continuing to investin key product integrations withdatabricks Our engineering and productteams continue to work togetherto deliver the joint road map and we have multiple customersusing our latest enterprisesecurity and performanceenhancements of databricks onGKE In Q4  we will also deliverdatabricks SQL  databricksphoton on Google Cloud  as wellas we are launching new datacenter regions together in EMEAand APEC Our AI teams are engaged toallow customers to experimentand develop models on databricksand Vertex AI together anddeploy models at scale in toproduction with Vertex ML Opssuite    So much goodness Okay Before we get into the livedemo  I recommend tuning in tomore sessions to hear from allour customers this week Join the chief data officerpanel discussion  and listen toour product strategy sessionsfor database  data analytics  BIand AI    We can t wait to hear fromyou Connect with you Join us in our community andshare your expertise and askyour questions But most importantly  start tobuild your data cloud and unlockyour data advantage Okay Time now to get hands on andcheck out our products in realaction Join us for live demo andinteract with our experts andone another  and then stickaround for the live Q Afollowing the demo I ll see you there    Hi  everybody I m Leah    And I m Nikita    And welcome to the Data Cloudlive demo If you re just joining us fromthis morning s keynote andspotlight  you heard from thelatest innovations we launched Earth Engine  Spark on GoogleCloud  Postgres secret interfacefor spanner  and workbench allto help organizations    andsimplify them Bringing together data on amassive scale to explore areal world scenario for climateand supply chain analytics    In order to do this  we regoing to need the whole datateam Say hi  everybody    Hello I m Brad    Hi I m Derrick    Now  a quick reminder This demo is truly live and youcan engage with us directly So ask questions  say hi in thechat window  and hit thoseemojis    I think I see some coming innow I see lots of hearts and fire A lot of people saying hello inthe chat Thanks  everyone  for joining ustoday So if you can t see this or thechat  just go back to the NextEvent Web site and click on theblue join the Join the InteractExperience button    Okay    Okay You re actually joining us forour weekly stand up at symbolsuper store  a fictionalU S  based grocery chain focusedon sourcing from localproducers Like many different industries we need to bring togetherdisparate data sources to buildresilience  specifically for us drought in the western U S  hasimpacted our producers  and oursupply chain has been disruptedwith decreased shipments weekover week    We need to better understandhow to manage inventory  and toprevent stock outs  and ensurewe can scale in both how weanalyze information and how werun our transactions    All right  team So to address this supply andscale changes that Nikita justmentioned  we have fourdifferent workloads First  I m going to evaluate thebest climate information toidentify at risk producers Then  Brad will mature our datapipelines to transform thatdata Ne  Derrick will dig in toscaling our transactionprocessing And finally  Nikita will showhow we re gaining new insightsby evolving our data sciencecapabilities    Let s get into it and let skeep hearing from all of you outthere  our data pros Leah  you re up first    Great Thanks  Nakita So in order to gain insight intoour supply chain  first I needto evaluate the risks So I m focusing on figuring outhow drought impacts the verysource of our grocery products And to do this  I turn toGoogle s platform for earthscience data and analytics Earth Engine  which has a hugecatalog of satellite images forthings like climate  weather crop lands and much more There s currently over  like  50petabytes of data in thecatalog Plus  more and more images areadded each day So let me just go ahead andbring up this cool littleapplication that I built usingthe Earth Engine Code Editor So I can walk you through thethree data sets we ll be usingtoday First is going to be crop datafrom the USDA NASS Program which I can grab right from thepublic data catalog bysearching This data set is going tocontain one image of the U S going back all the way to 1997 And each ten meter pixel in thisimage has actually been assigneda crop type Let me just close    the seconddata set contains the outline ofagricultural fields And we re going to use earthengine to assign each one ofthese fields the crop type thatis most prevalent among thesales that cover it So in this view that I m showingright here  the pink fields arealfalfa  and the dark greenfields almonds    Leah  these fields are superhelpful  but can we export thisdata for analysis in otherenvironments    Yep I m actually going to push thisin to cloud storage and BigQueryin just a second  but first Iwant to add in some additionalsignals on drought So the third data set that I musing is a collection of bothshort and long term krautdrought indicators  meaning thatwe can see what drought lookslike for each field Now  using data sets like thesedoes require some deep subjectmatter expertise So special thanks to ourreal world partners at ClimateEngine who build solutions ontop of eaEarth Engine s API helping organizations groundtheir analytics projects withscientifically acceptedmethodologies Okay So you just saw how we usedgeospatial data  and I want toknow how you use it in youranalysis So go ahead and answer that pollso we can see All right  everyone So with these layers in place we can start exploring droughtrisk Let me just go ahead and zoom tothis field And here  I m going to show analmond orchard northeast ofModesto  California And overlooking that is thelong term drought indicator So this orange tint that you reseeing means that this field hasexperienced pretty high levelsof drought over the last fiveyears Now  if I go ahead and just zoomout a bit  we can actually seethat the entire central valleyin California is pretty droughtstressed So my last step was to calculatedrought risk for each of thefields I showed before And this will tell us whichfarms and produce types arepotentially at risk for notmeeting customer demand  whichis just one consideration toinform symbols resilientstrategy Better yet  we can incorporatethe geography data with otherspatial stuff like store andcustomer locations that arealready inside of BigQuery And anyone who knows me knowsthat I love BigQuery If you agree  let me see in theemojis All right So the code editor has beengreat for interacting with EarthEngine  but making thesecalculations at scale requires adifferent approach So I went ahead and createdearth engine tasks for batchprocessing to run thesecalculations over all the datain five day increments for thisten years I also set up a cloud functionto run every five days goingforward so that we ll alwayshave fresh data Now  just as a quick recap  wejumped into earth engine toevaluate earth science data thatindicates climate risk for ourfarm fields Now we ll have fresh droughtdata available in Cloud storage So just think of the power ofintegrating huge amounts ofspecialized data  giving us acomplete picture of supply riskfor our business Oh  and I think I can see thepoll results coming in now It looks like lots of people areusing geospatial data for alldifferent kinds of things  andif your answer wasn t in thepoll  just go ahead and let usknow in the chat So next up  Brad will transformand push this data into BigQueryso geospatial analysis with ourexisting data So Brad  over to you    Okay That was pretty cool  Leah Thanks All right So to gain more insight in tohow drought is impactingproduction  my main focus hasbeen building out the datatransformation pipeline to pushfarm field drought indicatorsfrom cloud storage intoBigQuery Speaking of batch processing  asyou know  to assist withinventory management  priceoptimization and productassortment  we use product to   we use Google Cloud data as ourlong running clusters For you data pros out there  wewould love to know how you reusing Spark today so let us knowwhile I go ahead and pull up thecloud Console So here is a data cles siclustewe currently use This code is typical boilerplate code for basic datatransformations In this case  we are processingthe earth engine files fromcloud storage  performing typechecking and mapping to ensureproper ingestion in to BigQueryusing the Spark Bill ClintonqueryBigQuery connector    So this is pretty cool  but Ithink having a batch work flowthat s scheduled on acontinuously running clusterseems like it can be wasteful So is there any way we can makethis more efficient    That s a fair point  Leah There is still some overhead inmanagement here  and forhardened batch cases like this it would be preferred to havethese jobs run on their owndiscrete resources Good news  with the newserverless option  we can submita price job without submittingany infrastructure These automatic capabilities icspend more Dev time writing codeand less time on writinginfrastructure    All right  Brad First off  I m pretty sure thatyou just set me up for that  andsecondly  you definitely made upthat word  auto magic    Yeah  can I get some emojilove for that Anyway  I ll now go ahead andexport this notebook to a Pythonfile  nuings  using NNB convert Okay And next  let s go and use thecloud SDK We can use the data Proc API andcall batches to submit aserverless job So I m showing you here a pricepark job  but I can also submitjobs in scholar or SQL With auto provision and autoscale  no infrastructureconfiguration is needed This helps    powerfuldistributed data processingmodel With these capabilities  there sno need to manually create intoclusters So once I ve submitted theSo clearly it s been a busy weekfor me  using Spark serverlessto create pipelines to pushdrought data in to BigQuery  butthe efficiency equivalents we llgain by not needing to managecluster overhead will be worthit Now  you can let me know in thechat if you re as excited as Iam    Okay Let s see how everyone s usingSpark today Looks like notebooks are prettypopular Oh  and automation through airflow That s cool  too Okay Great Thanks so much  Brad So now that we have greaterinsight in to supply risks  weneed to scale our transactionsystems to handle more ordersand allow us to understand thepotential business impact ofthese climate risks HopHopefully while lowering costs So this brings me to Derrick our DBA Derrick  you ve been focusing onthis effort  so can you let usknow where you re at today    Yeah Absolutely  Leah We have a new grocery chain thathas struggled to maintain theiron prem databases as they vegrown There s an opportunity to solvetheir underlying scalingproblems with cloud spanner I spent the last few weeksevaluating spanner s ability toscale and the ease ofmaintenance while keeping strongconsistency for this point ofsale workload I migrated some of the chains tospanner already Using granular instant sizing  Ionly provisioned a portion of aspanner node to handle therequired load As we move the remainder ofstores  we can easily scale upwithout down time or appchanges    Quick question  Derrick We ve been really focused onreducing the cost and complexityof building scalableapplications  so I m wonderinghow does adding spanner impactthat effort    Yeah  that s a greatquestion One way we ve been doing thisinometer environments is tostandardize on postgres  awell established open databasethat our development and opsteams are already familiar with I tried to get the team to addan elephant emoji so you allcould show some love forPostgres  but anyway Spanner just announced aPostgres interface This gives us the benefits ofspanner without having tocomplete retrain our applicationteams Having    this data will help usmaintain velocity for newfeatures  even as our databaseneeds evolve Here Let me show you So I ve connected in to ourspanner  using the SQL commandline tool From here  I can explore theschema  just like I would anyother database  any otherPostgres database You can see that both thePostgres data type and thespanner data type is availableto the information schema So this will make it familiar toanyone who already knowsPostgres    Awesome Thanks  Derrick This is really great    Absolutely And I highlighted how spannerallows us to get started smalland scale as needed usinggranular instant sizing  and howthe new Postgres will interfacereduces friction for our teamsalready familiar with Postgres You can use existing data  andI ll keep onboarding morestores Nakita  I believe you need thisdata for some of the BigQueryanalysis you were working on You ve been busy figuring outhow to evolve our ability tomake sense of all this data Can you show us how that works    Yes Absolutely I was hoping that you d ask So to ensure that we re managingstock to meet customer demand my focus this week was onevolving our data science forexploratory analysis I combined the transactionaldata that Derrick s been workingon with the jot data that Leahand Brad pushed in to BigQuery This way  we can gain insight into which products are most atrisk of not meeting customerdemand So my first step was tointegrate all of ourtransactional data withinBigQuery  primarily usingFederation with Cloud SQL andSpanner This provided a unifiedenvironment for aggregation andanalysis  where we can jointransactions back to producerdetails and the associateddrought risk scores Now  as you all know  werecently migrated To Workbench which has really  really helpedwith our basic compute andresource management    Quick question  Nakita Is Vertex AI workbench      That is an excellentquestion  Derrick Vertex AI workbench containsrecently updated managednotebooks to bring forth moreintegrated data capabilitiesinto our data scienceenvironments So we can ingest and analyzedata and deploy and manage MLmodels all from one spot Now  let me show you a littlebit about what this    after Iprovisiond the new managednotebook  my first task was toanalyze the demand on thehigh volume excuse  and withthat I used the BigQueryconnected to view the salesdata With Vertex AI workbench  I caninspect BigQuery metadata preview tables  and automateentirely from my notebookenvironment Okay It s my turn on the emojis I want to see the fire emoji forpeople who are as excited as Iam about being able to accessBigQuery from the notebookinterface I m not the only one    Hey  Nakita  hold on Sorry to interrupt your emojifest  but I see you re workingout of the lakes project but areable to pin other projects thatyou have access to  like the opsproject Is that new    Yes That s correct  Brad Vertex AI workbench also enablesme to interact with all servicesvia my own identity It also provides code to helpbuild out queries and projectresults So here  we can see    oh  lookslike I just need to refresh Sorry about this Just going to refresh my Jupiterlab instance here I guess I had it open for toolong    And I guess while Nakita isdoing that  like who usesJupiter now Let s see the fire emojislighting up I personally use it a lot in myday to day    All right Thank you So perfect You can see a simple view of alltransactions by the day of week And Vertex workbench also allowsme to launch different kernelsentirely in the same instance So Brad  I think you wereworking out of a notebookearlier  is that correct    Yeah So the data    the back end as apart of spark on Google Cloud We can access all supportedkernels on the cluster including High Spark    Well  from my analysis  Ijust needed a python kernel So let me show you what s goingon in this notebook and whatI ve been up to this week You can see I did a deep diveinto the data I ve created a heat of purchaseaccounts across variousdepartments    the short termdrought index across variouscrop types  like corn  drybeans  pears and rice My last task  and the mostimportant task  was to map skewsto suppliers and calculate arisk score based on theaggregate field data So to do this I started with ourmapping table  which you can seea sample of right here This helped me to determine    Icalculated a weighted risk scorethat takes into account eachproduct s demand and theassociated farms  overalldrought risk and you can seeeach of these measurements inthis data frame right here Ultimately  this goal will helpus to prioritize managing itemswhere we re most at risk of notmeeting customer demand So as a quick summary  I use   to connect    and create aweighted risk score for each oneof our products  combining bothclimate and demand data Leah  I went ahead and put allof this back in to BigQuery justto keep it centralized    Awesome Thanks so much  Nakita That risk score is going to behuge in helping our teamunderstand supply risk So to make sure the team has atrusted view of all thisinformation  I startedincorporating everything into aworker dashboard Now  with Looker  we define ourmetrics like average risk  usingwork ML looker s data model  andLooker uses this to compile SQLqueries and send them back So that our users can explorethe results of these efforts So let me show you thisdashboard that I ve been workingon And first off  you can see thiscustom map layer that I use tovisualize drought risk acrossfarm regions in the west  andwe re able to take    geospatialfunctions and drill from thisaggregate view all the way toproducer or even grocery preppedlevel Let me show you So I ll drill to fields ID andthen I can jump right in to looker s explore Maybe I want to specificallylook at products that are atrisk in this region where we requickly expanding Making all this data accessibleto the broader team is reallyimportant  so can we make surethat the team is aware of thisso that they know how to holdoff on coupons or advertisementsfor these products    Yeah Actually  we can set up aschedule from Looker so that theregional marketing teams arealways notified if new productsseem to go at risk So putting this all together  weused Earth Engine to process newearth science data signals We used    to efficientlytransform geospatial data andpush drought indicators in toBigQuery  Spanner to scale ourtransaction systems  enduring wehave the infrastructure tohandle future growth  and VertexAI Workbench plus Looker backedby the power of BigQuery toevolve our data sciencecapabilities  and serveimportant trends back to keydecision makers Across these services  anyone isable to explore and take actionon insights that combine climatedata and transactions Now  we can understand whereproducts may go out of stock dueto increasing droughtconditions  and identify areasto build resilience againstchanges in our climate So with this geospatialinformation  we re given a wholenew set of data so that we cannot only be considerate of ourcustomers  but also find ways tobe more considerate of ourplanet And with that  it looks likewe re right about at time So thank you so much for joiningus today  as we ve shown howGoogle Cloud can offer a worldclass experience for creating ascalable  unifightifiedplatform And a special thanks to mycolleagues  Nakita  Derrick  forintroducing us to Spanner andit s Postgres interoperabilityand Brad  for walking us throughSpark on Google Cloud    And thank you  Leah  forshowing us Looker and EarthEngine Last  but certainly not least thank you all out there so muchfor your engagement We love those emojis and seeingall of your comments in thechat So stay tuned for our live Q A which is going to covereverything from our spotlightthrough this demo    Thank you Bye    Thanks  everyone    Thanks  bye    Thanks  data cloud team What a cool demo I m Dane Hanson Welcome  everyone I m the director of productmarketing for data cloud Joining me today is Garrett    Hey  Dane I m Garrett I m the vice president of dataanalytics  data basis andLooker  and I do look forward toeveryone s questions    I do  too A reminder that this Q A islive You can engage with us directly So ask your questions  say hi inthe chat  hit those emojis  andif you can t  just go   experience button    So let us jump right in But before we do  I do like tosee what you like most today  sowe have quite an exciting set ofannouncement  and is it for you let s go bigger query  Postgresfor all  or is it show me theearth engine    So while we wait for thoseresults  tell ple  what are youmost excited about    Two things  openness andsimplicity All of our announcements weremade to make data moreaccessible  more simple to use from Postgres for Spanner  fromcross cloud analytics from Omni everything is in the spirit ofmaking data simple to work withand creating an open ecosystemaround GCP for working withdata    That s right And being open and simple isreally what it s all about  andit s what we saw from the demoas well Just a reminder that our friendsfrom the live demo are stickingaround to help answer thequestions So you might see their facesthroughout the show It looks like we ve got some ofthe poll results It s all about BigQuery Look at those hearts come in So this is really good    Bigger query    All right So let s get into our firstquestion It s from Tianna She asks about the availabilityof earth engine We saw the great demo earlierfrom the team How can folks get their hands onthis How can they access it andanother follow up question whichis how does it work withBigQuery GIS    Awesome Hey  Tianna  awesome question So first of all  Google EarthEngine is now in preview  whichmeans that you can go toGoogle com earth engine previewand register  and we will bereaching out to you to set youup with an account the second key piece of what wehave seen earlier is that inearth engine  you have a datalake of satellite imagery  andyou get a means to do geospatialanalysis on imagery data  andthe results can be brought intoBigQuery to match them up withall of the data you haveresiding there  or usingBigQuery  for instance  as wehave seen so all in all  awesome package a lot of data and dataintegration right back in toBigQuery    I love this  and it s what wecan provide with the best ofearth data and the best ofanalytic solution this is a great solution    that s exactly right    so another question comes infrom Chris This is exciting What is Google s plans arounddata management Does it have plans  GarrettGerrit  to expand ourmulti cloud approach to otherdata management services    Great question  Chris  and Ithink it all starts fromrecognizing basic fact For the majority of companies multi cloud is a reality  andwhen we talk about data silos we have to recognize that amodern data silo and amulti cloud data silo  and atbig query omni  we wanted tobring simplicity to multicloud which means that you can docross cloud analytics  and weare constantly evaluating whichof our data services makes sensein an omni delivery  andbasically following our customerscenarios where you are takingus  too  and you can expect muchmore to come in that direction    This is great Really go forward withmulti cloud as more and morecompanies are using that astheir norm    I think it s 90 percent ofthe companies that we vesurveyed actually say thatmulti cloud for them is thereality they are living intoday So let s place to thatcomplexity and delivermulti cloud across coreanalytics    Okay Good Good Thank you  Gerrit Next question comes in from Anonat CVS health He asks this question to Leah actually Is there any map that revealsall the services I think you might have shownthis at the tail end of thedemo  but maybe you could goback and do sort of an overviewabout the visualizations  thecapabilities of looker  how doesLooker and BigQuery play a rolewith the visualizations for thatmap of Earth Engine    Awesome Yeah Thanks  Dane So basically  I think what thisquestion is asking is if you canvisualize the actual Samanthaicmois he mantic moindel that you rcreating in Looker  and actuallyearlier this year  lookerlaunched the diagram which isavailable in Looker smarketplace  so you can directlycreate a visualization of yourdata model  whether your data isin BigQuery  Spanner or otherplaces that are connected toLooker  so check it out    That s awesome Thank you  Leah The next question comes inaround our Cloud SQL and Cloudspanner capabilities We offer both capabilitiestoday  as you know  Gerrit Which one is for which purpose Could you talk a little bitabout how spaSpanner is our    tis your recommendation What use cases to use Cloud SQL as well as Cloud Spanner    That is a great question because both of them  actually are pretty awesome services Cloud SQL is really a managedservice where you can decidewhether you want Postgres or mySQL  and it s really for thosecustomers who want to leveragebasically perfect compatibilitywith the my SQL ecosystem  useall of the packages  useeverything around that type ofsystem And it s a really highlyreliable  high performancemanaged service that we areproviding Spanner sits on the other end ofthe spectrum  if you like Like it s a cloud nativedatabase  it combines a uniqueset of abilities It has five 9 s of availability That is unheard of It has global consistency It s horizontally scaling All of the things that you wantto use when you re buildingmassively scaling globalapplications And so if you are looking onthat spectrum  that s reallyabout scale  and yet  havingstrong consistency  then Spanneris the way to go And the good news is fromtoday s announcement  you canuse Spanner with a Postgresinterface  which means that youdon t need to relearn or takeanother tool set chain in orderto work with it You can basically seamlesslyapply your Postgres skills andgo Cloud Native with GoogleCloud Spanner    That s awesome Let me ask another question It s about a relour recentpartnership with Tableau This is a pretty excitingpartnership We ve done a lot with them And could you talk a little bitabout how this strategy in thispartnership helps our Lookercustomers specifically    It is really about simplicityand openness This announcement means that ifyou are a Tableau customer  youwill be able to use looker ssemantic models and Looker fortrtrusted governance  for highperformance database access If you are a Looker customer  itmeans that you can add Tableau sself service data visualizationright on to your Lookerscenario So the bottom line is that itmakes working with data moresimple It helps our customers who aretrying to find the right balancebetween self service andflexibility and stronggovernance and semantics It really brings together thebest out of two worlds    That s great And I think it s really areflection of our open strategyfor the data cloud Again  it brings more community more customers More use cases  morecapabilities that we can bringas part of one ecosystem    That s exactly right    That s awesome    The next question comes infrom Andrew It s    another questionactually to Nakita about thelive demo we showed inintegration of Vertex AI How does this work with BigQueryML or BigQuery machine learning Nakita  could you take thatquestion    Absolutely That s a great question So for those of you who don tknow what BigQuery ML is  itallows you to I build and trainmachine models with just a fewlines of SQL So in the demo we showed how youcan work Vertex AI workbench tohave access to essentially theBigQuery console but from anotebook interface  which I msuper excited about So you could actually create andtrain your ML models in SQL anddo all of this from the notebookinterface  but still have thatconnection to BigQuery and beable to use whatever Tableaudata you have stored in yourBigQuery    Okay That s awesome Thank you  Nakita Next question comes in fromJeff Another question on looker Could you talk a little bitabout how the Looker is enablingthese new sets of API drivenexperiences  specifically Jeff s asking  what are theconsiderations that we shouldhave in place for the way weintegrate it with other toolsthat we should be using Could you talk a little bitabout how we can now connectwith Sheets  so there s some newthings that we connect with withLooker    Hey  Jeff  awesome question and I do want to pre text itwith the following statement Looker is going to be a keyelement of any data landscape Basically  what Looker is doing it s giving you two amazingthings One is it gives you a modelsemantic layer  basically whereyou can describe your datalandscape with very rich annotations And this semantic layer isconnected directly to thedatabase system For instance  when it runs onBigQuery  it completelydelegates all of the queriesright in to BigQuery so itdelivers stellar performance And Looker as a whole  has apretty amazing API firstconcept  which means that it isso beyond AI  it allows you tobuild data rich applications It allows you to basically runaway to customer experience  tointernal applications and makethem really data rich And last but not least  youknow  it kind of gave it awayhere  we are also integratingmore and more products withlooker semantic model For instance  we are addingconnected sheets as a way howyou can analyze and discoverdata described in look ML So check it out It s going to make a bigdifference in your landscape aswell    This is great We are almost out of time  sowe re just going to take onemore question before wrappingthings up The last question is fromFerard He talks about how at USDA  manyof the methods are usingdifferent visualizations How does this complement ourstrategy with how we reconnecting all this data It s over 50 petabytes of data How do we connect all that datawith Earth Engine and enable allthese different ways to processit with all these differenttools    Absolutely Hey  great question So first of all  it starts withmaking sure that there is data and with Earth Engine  you getthis amazing data link ofsatellite imagery and GIS dataavailable to you The second piece is that youneed to have a powerfulprocessing framework  and by theintegration to BigQuery  you canuse the most    scale in thefastest warehousing system onthis planet to analyze this dataalongside with all of the restof your data  of your businessdata And last but not least  like yousaid  it s about completeopenness The announcements that we haveshared today with Look ML allowsthat you can connect top low connected sheets  all on top ofthat right tool and make surethat you are using the tool thatlook like the most  but theyprobably close it off  because Ithink we are out of time Dane and I have a very importantmessage for all of you  and hereit goes    Thanks to Gerrit  as well asour friends over on the demo setfor answering some greatquestions  and it s really beenawesome hearing from all of you Be sure to hop back over to thenext Web site Javier will share all the greatthings happening with GoogleWorkspace Thanks so much for joining us    Hello  everyone Welcome to Google Cloud Next All right We ve had a lot of great contentso far And I have to say I learned alot just now watching the dataCloud session I hope you re ready for evenmore I m Javier Soltero  vicepresident and general managerfor Google Workspace And I m super excited to be withyou today Though  to be honest  I reallywas hoping we could all be inperson by now Since flexibility is one of thekey themes for today  we llembrace what we have to workwith  even if we can t betogether Although the last 18 months havebeen filled with uncertainty  wecontinue to be inspired bycustomers and users who foundnew ways to create and stayconnected Driving innovation and embracingchange are always challenges forevery business  whether you reten people or 100 000 The goal seems simple Empower your teams to workeffectively together  in part byEnduring that your technology isan enabler  not a blocker As we ve evolved GoogleWorkspace  we ve been focused onhelping individuals  teams andentire organizations driveinnovation and greater impactwith the tools they use everyday And flexibility has beenfoundational to our approach It s why we decided to not onlybe Cloud first  but Cloud only It s why we ve alwaysprioritized user choice It s why we take a zero trustapproach to security and why wemake sure any product innovationwe introduce is truly built foreveryone And those principles are core towhy our customers choose us Millions of companies  schools government agencies and otherorganizations rely on GoogleWorkspace to help them navigatewhatever challenges they mayface They re also a big contributorto our own continued success asa business Google has run on what is nowGoogle Workspace for the last15 plus years  and it s part ofwhy we have been able tocontinue to innovate and adaptas we ve scaled In that time  we ve grown toover 144 000 employees  made 213acquisitions  added dozens ofoffices around the world andmore than quadrupled the numberof annual searches we support We ve weathered changing marketconditions and user preferencesand competitive challenges  justlike any business  just likeyour business One constant for us has alwaysbeen our tools  which weredesigned to meet ever shiftingneeds and circumstances Today  we ll walk through someof the most impactfulinnovations in our tool set thathelp unlock the full potentialof individuals  teams andorganizations in a flexible more effective hybrid workplace First  let s talk aboutcollaboration Effective team collaborationpowers creativity andinnovation It s how people work towardsshared goals  build team cultureand are able to achievesubstantially more than theywould on their own With everyone s time in suchhigh demand  and the explosionof collaboration tools giving usno shortage of ways to reach oneanother and work together collaboration can often feelfragmented Conversations and work flowsscattered across disparatechannels Now more than ever  thetechnology you choose shouldwork intelligently and effectiveacross every medium to enablecollaboration wherever ithappens Last month  we introducedSpaces  the central place forflexible team collaboration inGoogle Workspace Spaces are built to be tightlyintegrated with Calendar  Drive Docs  Sheets  Slides  Meets andtasks to provide a uniquelybetter way for people to engagein topic based discussions share knowledge and ideas connect across organizationalboundaries and build teamculture All in the service of movingprojects forward In the same way that a group ofpeople can accomplish more byworking together  our integratedexperience is greater than thesum of its parts  helping peoplebetter collaborate and managetheir time and attention By thoughtfully bringingtogether collaboration  surfacesand channels  the goal is tohelp people stop thinking aboutwhere and focus instead on thewhat To continue investing in thisarea  we ve been working withcustomers to understand theirneeds  especially the changingcircumstances we ve all beenfacing In line with feedback we vegotten  we ll be adding in linetopic threading  more powerfulSearch  robust security andadmin tools for contentmoderation  and discoverablespaces We hope this makes it eveneasier for organizations andteams to both proactively shareand find the knowledge they needto deliver greater impact Those of us working remotelyrealized early on how crucial itis to cultivate a sense of humanconnection with our colleagues No matter where or how we work we all need to be seen  heardand understood Towards that end  in the last 18months  we ve invested heavilyin bringing functionality toGoogle Meet  that makesconnecting virtually almost asnatural and rewarding asconnecting in person This includes AI powered noisecancellation  low light mode andlive translated captions  alongwith a new intuitive userinterface  complete with polls Q A and breakout rooms that helpmake meetings more productive inclusive and immersive With all of these improvements we re seeing customers whopreviously opted to usethird party meeting solutionsreconsider moving to GoogleMeet We ve also invested in makingour tools reflect the new wayspeople are working  to supporthybrid and distributed teams We ve added the ability to setworking hours  work location andrecurring out of officenotifications from calendar We also introduced the abilityto RSVP to a meeting with avirtual or in person location All these investments are gearedtoward helping people bettermanage their time and ultimatelyprotect their well being Well being has been a centralfocus for us at Google over thepast year  and for countlessorganizations supporting theiremployees during this longperiod of remote work Two clear  bright spots we veseen in pivoting to remote workhave been  one  an increase inempathy for our co workers lives outside the office  andtwo  an increased focus incollaboration equity  where theability for everyone toparticipate equally and fully For more than a year  we ve allbeen reduced to digital videotiles  and it s had ademocratizing effect onmeetings We thought a lot about how wecan sustain this equity as moreof us move into a hybrid model Next month  we re launchingcompanion mode  which offers aunified experience for people inthe office and their remotecolleagues Those in the office can stilluse the best of in room audioand video from Google madehardware  but they can also jointhe meeting using companion modeon their laptop This lets them fully participatein the meeting with access tochat  white boarding  Q A andpolls  along with the ability toshare content  just as theywould from home We think it s imperative toprovide one unified meetingexperience regardless of whereand how people connect And as more people head back into shared physical spaces  we reenhancing our meetings hardwareto give organizations greaterchoice and flexibility We have two new all in one videoconferencing devices  the GoogleMeet Series 1 Desk 27 and Board65  with new standards of audio video and white boardingcapabilities built in These devices can turn any roomor shared space into acollaboration hub We also have new third partydevices coming to the GoogleMeet ecosystem  including thisLogitech rally bar  and this newRayz Rally Pro Speaker doc  thatwill automatically launch GoogleMeet for video meetings andprovide an improved audioexperience from a mobile device Lastly  we re partnering withCisco to bring interoperabilitybetween our video conferencingdevices  so you ll be able tolaunch a Google meeting on WebExdevices and WebEx meetings onGoogle Meet hardware This sort of partnership iscrucial in delivering on ourvision of Google Workspace foreveryone Last year  our focus was onintroducing an integratedexperience across ourfirst party products This year  we re prioritizingstrengthening the platform thatsupports and extends them This is so important  becauseour success is measured not onlyby how people use and love ourproducts  but by the ecosystemthat thrives around them And what we re seeing in thenumbers here is remarkable Google Workspace has more than 3billion users and a stunning 4 8billion apps have been installedin Google Workspace to date There are more than 5 300 publicapps available in the GoogleWorkspace Marketplace  plusthousands more private apps thatcustomers have built forthemselves It really shows what s possiblewhen you combine great ideas easy to use APIs and the abilityto reach a user base that s 3billion strong While users and developers havebeen able to build on apps likeGmail  Drive and Docs for years we re now in the process ofmaking it just as easy tointegrate with Chat  Spaces andMeet If Google Workspace is how it sdone  then the Google Workspaceplatform is how it s integrated Miro has already announced theirplans for integrating in GoogleMeet  and today we re pleased toannounce a new integration forChat and Spaces from Atlassian Joining me now to share more please welcome Atlassian s ChiefProduct Officer  Joff Redfern Hey  Joff Great to have you with us    Thanks for having me I m delighted to be here    You know  for those thataren t familiar with Atlassian can you tell us a little bitmore about it    Absolutely At Atlassian  our mission is tounleash the potential of everyteam Our team collaboration andproductivity software helpsteams organize  discuss andcomplete shared work But at the same time  we believethere s no one tool to solve allof the complexity of teamworkand collaboration Every team is unique Each team is its own snow flake and they should be able to usethe best tools for the job We focus on building the best inclass collaboration solutions and then deeply integrating itwith all the other tools thatteams need to get things done    You know  it s so true thatthe nature of every role in teamwithin an organization  and thechallenges they face  areunique Can you tell us a little bitmore about the opportunity thatAtlassian sees in building forGoogle Workspace and how you gotstarted    Yeah Today  Atlassian solutionsalready extend and connect withthousands of tools fromcollaboration  chat security devops and a whole lotmore  and we re excited tocontinue to extend ourpartnership with GoogleWorkspace to deliver for ourjoint customers Work is everywhere  people schat  in inboxes It s all flooded with work And to help turn that clutter into progress  we believe inintegrating Trello and Jira andour other tools everywhere workis The better together integrationhelps Atlassian and Googlecustomers make work flow acrosstheir organization Rich content and easy actionshelp move work forward In fact  we are Workspacecustomers  and this is what ledus to dream up our initialintegrations One of our first launchestogether was the Trellosegregation in Gmail  and thatwas back in 2017 Today  more than seven millionpeople have installed it  whichis really impressive    That s awesome Today  we re so excited toannounce the new Jira bot thatconnects to Google Chat and toSpaces and enables users tocreate issues  monitor them  allwithin the space that you arealready using for collaboration    Well  Workspace is unique inthat it offers such asignificant opportunity to reachover three billion users And today  Atlassian and Googlehave invested in a number ofWorkspace products to help userswork better together  whetherthat s unfurling Google Drivelinks across Atlassian products embedding an important GoogleDoc in Jira work management orcreating new Trello cards fromGmail But there s more And we ll continue to listen toour customers and then followtheir needs    It s great to see the powerof Jira built in to Chat and toSpaces It s such a powerful example ofhow developers can build newexperiences from across theplatform  from Gmail to Chat toSpaces to drive Thanks  Joff Really excited to have you herewith us  and I m very excited tocontinue to develop ourpartnership    Thanks  Javier Have a great show    As we work with Atlassian andAll our partners to buildand deliver new experiences rest assured that every one ofthem is designed with securityin mind Starting with a zero trustapproach to identity in GoogleWorkspace We also employ a rigorousvetting process to ensurepartner apps are secure beforebecoming available in the GoogleWorkspace Marketplace Finally  we recognize that it snot always developers  but theperson closest to the problemwho best understands how to makework more efficient andimpactful To better empower these users we re providing them access tocustom  no code Appsheet appsright from Gmail with dynamicE mail This lets users interact to datadirectly and securely from theirinbox to get things done inpowerful new ways This experience is availabletoday and you ll see it inaction in the upcoming demo in afew minutes Now  we ve talked about thepowerful new experiences inGoogle Workspace  and theplatform that surrounds it  butneither of those can transformthe way your organization workswithout a broader foundation forsecurity We ve seen too many headlineslately about data and securitybreaches  and it s a powerfulreminder that moderncollaboration requires modernsecurity Legacy collaboration tools werebuilt for a by gone era andthese headlines make it clearthat a band aid approach to asecurity problem with legacytools just doesn t cut it We built Google Workspace withmodern collaboration andsecurity in mind to keep yourpeople and your data safe Let s talk about what makesGoogle s approach unique First  because we re cloud only we don t have the samevulnerabilities that legacycollaboration tools andinfrastructure have With Google Workspace  there sno need for thick clientapplications  E mail attachmentsor on prem identity directories Each of which provide bad actorswith too many opportunities forattack And all our security protectionsare included with GoogleWorkspace  which means you don tneed to spend money on securityadd ons  and you re lesssusceptible to supply chainattacks Second  our entire platform isbuilt on the BeyondCorp securitymodel  which is Google s uniqueapproach to Zero Trust This lets employees worksecurely from anywhere on anydevice without the need for aclunky VPN Finally  we re best in class atautomatically detecting andpreventing attacks  fromphishing to malware to supplychain to ransomware Every day  we block 15 billionspam messages  100 millionphishing attempts and 3 milliondeceitful URL s from reachingour customers All of this comes together toprovide Google Workspacecustomers with the most trustedway to connect  create andcollaborate All right Now how are we working toenhance that security visioneven further Well  I m excited to announcesome new capabilities that buildon this unique approach tosecurity First  we ve already madeclient side encryption availablein beta for Drive  Docs  Sheetsand Slides  and now we rebringing it to Google Meet aswell Client side encryption gives ourcustomers complete control overthe privacy and confidentialityof their data Next  we re introducing dataloss prevention for chat You can think of this as part ofour promise to enablecollaboration without compromisewhen it comes to security With data loss prevention inchat  you can supportspontaneous conversation whilepreventing sensitive andconfidential information fromleaking outside of yourorganization Admins can control what usersshare inside and outside theirdomain on Google Chat  andthey re alerted about anyviolations Finally  I m excited to sharethe Drive labels is nowgenerally available This gives organizations theability to classify files storedin Drive based on theirsensitivity level Labels also integrates withGoogle Vault  and GoogleWorkspace data loss preventionto prevent external sharing downloading and printing ofsensitive files Just another example of howGoogle Workspace enablescollaboration while providingthe right controls to manageaccess and protect data So we ve talked you through alot today  and the common threadis that we re continuing tobuild a modern productivitysolution One that seamlessly bringstogether the right tools in theright way to help you moreeffectively collaborate  thatsupports hybrid and distributedwork and that lets you extendand customize solutions for yourorganization And of course  one that s secureby design It s a platform that helps youunlock the full potential withinyour organization so that youcan innovate  even thrive  amidthe disruptions of today andtomorrow There will always be certainthings that are out of ourcontrol  but with the rightpeople and tools in place  aswe ve seen firsthand with ourcustomers  we can always beprepared to effectively adapt no matter what comes next Though in this case  it just sohappens that I actually knowwhat s coming next  and it s ourdemo with Eric Troutman Director of Product Managementfor Google Workspace Stay tuned to see what we justtalked about come to life    Hello  everyone I m Erica troutman  productdirector for Google Workspace There has been a ton ofinnovation in Workspace thispast year  and in the next 15minutes  I get to share with youall the progress we ve made helping teams of all sizes toconnect  create and collaborate This demo has two parts In the first  I ll be showingyou Workspace in action througha little story that we vecreated  and then I ll hand itoff to my colleague  Matt  whowill show you all the waysdevelopers with build on theWorkspace platform And as a reminder  before we getstarted  this is truly a livesession  so you never knowwhat s going to happen If you haven t already  clickthe button that says   join theinteractive experience  on thenext Web site  and then we llsee you in the live room whereyou can interact with chats anddo fun stuff like take polls which we re going to try outright now How many employees have you oryour company onboarded virtuallythis year Let us know in the poll below Under 10 11 to 25  26 to 50 or more than50 I know that for me  recruitingand getting people in the door that got harder during the startof the pandemic  taking thatbrand new person that you workedso hard to find  bringing themup to speed  making them feellike they re part of the team and really enabling them to dogreat work  those critical firstfew weeks on the job And of course  it is sooverwhelming to be that newhire You re learning not just therole  but the tools  theprocesses  the people  and theculture  maybe remotely So I want to show you howWorkspace is making this easier Please meet our hero  Pablo He is in exactly this position He s about to receive an offerletter from his dream employer Symbol Manufacturing  which  bythe way  is not a real company in case you were wondering  sodon t go look it up His hiring manager Allison isgoing to help onboard him at thecompany Allison is really  reallyexcited to get Pablo on boardand there are a ton of steps inthe process that she s going tohelp him through  so we ll showyou how she uses Workspace tohelp streamline that experiencefor bochl both of them  evenwhen it requires using thirdparty software or custom tools the symbol team built withinWorkspace The point here  as Javiermentioned  is that Workspace isa platform that brings togetherthe right tools in the right wayat the right time  whether thosetools are built by Google orsomeone else Okay Before we get back to Pablo andAllison s story  let s see whatyou ve been experiencing here inthe poll Wow  it looks like 48 percent ofyou say that you ve onboardedmore than 15 people this year that is a lot  so I think youare going to be able to relateto Allison When you were recruiting andonboarding  what was the firstthing you did I think probably you sent theman offer letter  and fingerscrossed  hoped they signed it Allison uses the Docusignintegration in Google docs to dojust that So here you see Pablo is goingto check his personal G mail On his mobile phone And oh  he just scrolled pastit There s an offer letter inthere He s going to click on that andopen up Docusign within theE mail He clicks the nice cover letterAllison has sent him  clicks toopen the document itself Okay Great Start date is right They spelled his name right Salary is what they talkedabout I think he s going to accept He signs the offer letter withhis finger right there on hisphone without leaving G mail Come on  Pablo  sign Sign And he signs and clicks submit and Allison is so relieved In other systems  Pablo mighthave had to stop what he wasdoing  open another tab toreview and sign the agreement go back to G mail  hopefullyremember where he left off It s just unnecessarilydisruptive if you re signing asingle contract But if you re someone whoreviews and signs contracts allthe time  it s incrediblyonerous So deep integrations withleading third party apps likeDocuSign save you valuable timeand keep you focused on yourcore job Audience  if you have anyfavorite Google Workspaceintegrations  we would love itif you would share those in thecomments because it s a greatway to help everyone who s outthere    everyone who s therediscover the best of what s outthere All right Back at Symbol  when Pabloreturned his offer letter  thatkicked off the onboardingprocess So let s see what happens next Pablo s now at home  on hispersonal G mail  and he sees anE mail from Allison come in atthe top there He opens it and notices  hmmm this is not a typical E mail This is an Appsheet dynamicE mail  which allows him tointeract with the forum withoutever leaving his in box  so he sselecting his laptop  accessorytype  desk type  T shirt size really important  he enters hisaddress  clicks submit  and justlike that  Cymbal can now sendhim his stuff This is especially importantbecause Pablo is going to workremotely initially Just like with the DocuSignexample  the dynamic E mail letshim complete his tasks where heis  in G mail  saving him thetime and hassle of toggling backand forth between tabs and apps just to fill out a form And  by the way  throughout anyof this  if you have questionsregarding dynamic E mails orAppsheet  please comment belowso that we can address them inthe Q A Let s fast forward to Pablo sfirst day on the job He s received his equipment He s excited to work He s in calendar and notices ooh  he s got a one on one withAllison coming up He s going to click on thatcalendar invite And join the meeting withAllison Allison welcomes him to Cymbal offers to give him a shortoverview of calendar and meet So here she s showing him how toset up his working location andworking hours so that hiscolleagues  who are global  knowwhen he s available and whenhe s off Features like these onlyavailable in Google Workspace really improve employeewell being and overall jobsatisfaction They help employees bettercoordinate how and when to worktogether  enduring bothproductivity and time torecharge  even though ouroffices might be in our bedroomsor our kitchens Honestly  Pablo is going tospend a lot of time in meetings So Allison shows him all thegreat new features that havebeen added to Meet over the pastyear The interface is new  and easierto use Here she s showing him how toget into some of the advancedfeatures He s definitely going to want toturn on noise cancellation which is great for filtering outdogs barking or neighborhoodconstruction And if you re having troublehearing someone  you can justclick on the closed captionbutton to follow along in livecaptions  even in a differentlanguage than what s beingspoken  which blows my mind AI powered translated captionsin meetings are launching soon and only Google Workspace offersthis built in for everyone There s also a host of newfeatures which she s showing himhere to boost engagement So Chat  Polls  these magicallittle breakout rooms that she sgoing to show that transport youto smaller groups where you canbrain storm  white boards integrations with Google Docs At this point  the one on onewraps up  and Allison suggestshe goes into G mail and startexploring the spaces he s beenadded to We recently launched Spaces which are the evolution of roomsin Google Chat  and they re thecentral place for teamcollaboration in GoogleWorkspace Pablo looks at the team spaceand notices his team has writtenhim a bunch of welcome messages which is really great He responds And now he s going to browse thespaces that are available tohim  and he discovers oneparticularly relevant to hiswork  quality inspectInspector sNorth America Awesome That s him In this space  he can connectwith his colleagues  discussideas and collaborate onprojects easily Spaces are unique in thatthey re tightly integrated withthe entire Google Workspaceproduct set So calendar  drive  doDocs  Shesand Slides  meetMeets and Tasksbringing all the tools you needto engage in topic baseddiscussions  share knowledge andideas  move projects forward and build communities and teamculture Pablo notices there s a littlebubble with Allison s image which means she sent him adirect message Turns out this is Pablo s firstproject  and Allison shared adocument with him  which we cansee in the preview here Pablo opens the document  andnotices his co workers alreadypre populated some of the coreinformation So facilities  number of workersper facility on the floor andtypes of tasks that they peperform Pablo s job is to optimize theprocess that managers use tocreate tickets and send them totheir quality inspectors on thefront line Okay To do this at his prior company Pablo would have needed tosecure IT assistance at aminimum  and more probably acontractor and probably a budgetto go build an app So not fast  not easy Instead  Pablo s going to UseAppsheet He connects his sheet ToAppsheet  and after about anhour of customization  he screated a quality inspection appthat his team can use So he opens it up to file hisfirst ticket to one of hisquality inspectors Now what All right Over on the manufacturing floor we meet Adu  a quality inspectorreporting to Pablo  and he sjust received a pop upnotification on his phone withthe inspection ticket Pabloassigned to him So he pulls up the Appsheet appthat Pablo created and performsthe inspection He s going to take a picture ofthe completed job Let s see if he    we can gethim over there in to theAppsheet app He noticed    yeah  there we go All right He s now in the app And he s going to take a pictureof the job once it s completed He s going to annotate that  andsubmit it as closed  and that then  will notify Pablo onPablo s side of the app So hopefully you saw how easythat was to make collaborationmore efficient between an officeand front line worker He used a really simple butpowerful Appsheet app toeliminate the whole back andforth between E mails and chatsand then maybe switching to anexternal ticketing system  andthen more E mails and more app   more chatting At this point  if any of you canthink of useful    for Appsheetin your organization  pleaseshare those with us in the formbelow So think about all of thoserepetitive tasks that make youcrazy day to day Those are the kind of thingsthat app sheets can automate foryou All right Back to Pablo Let s see if we re    okay Still on Appsheet here for justa quick second We re going to get back toPablo s day to day in his firstweek on the job any moment here Let s see I m looking here You guys have some reallyinteresting    oh  scrollingthrough here Asana  people like thatintegration I saw people use hello sign That stuff is all fantastic All right Are we back to Pablo We re back to Pablo Back to Pablo It s the end of his first weekon the job He pulls up calendar in the sidebar while in G mail  and henotices he should be joining theteam meeting So he clicks to join On the call  Allison introducesPablo to everyone and asks himto share his reflections on hisfirst week And he is really proud of theapp that he built in Appsheet especially since this is a toolthe entire team can adopt He shows how easy it is tocreate requests  assign one toan inspector  follow the status He also pulls up the requestthat he sent to Adu that wasalready completed And if Pablo were joining from aconference room  becausehopefully some day he will  hecan use companion mode  afeature that s only available inGoogle Meet to seamlessly joinand share from his laptop whileusing in room audio and video And with some congratulatoryemojis from his colleagues Allison closes off the meeting and the weekend can begin I think it s fair to say thatPablo helped us with quite a lotof demos in his first week onthe job  so thank you  Pablo And I hope that you can see howat every turn Google Workspacehelps people connect andcollaborate to get more donewith an intuitive and integratedset of tools With that  I d like to introducemy colleague  Matt  who s goingto take us under the hood andtalk about how you can build onthe work space platform    Thanks  Erica Hi  everyone I m Matthew  and I m leadproduct for the Google Workspaceplatform Under that hood is a powerfulengine Let s talk about how you cantake advantage and build on theWorkspace platform As Javier mentioned  more than4 8 billion apps have beeninstalled in Workspace to date This stunning number illustratesWorkspace is more than a huntfor work  it s a powerfulenabler work flows The opportunity to build on theWorkspace platform is massive Nearly half the world spopulation is already usingWorkspace That s a huge audience Whether you want to grow anexisting business app or buildan entirely new business on theplatform To help you build experiences toreach these 3 billion users  wedelivered a range of tools soyou can innovate  from no quoteoptions like Appsheet  as Ericadiscovered  to powerfulscripting with app script  to acomplete set of easy to useAPI s And we ve been listening to yourfeedback Developers want to use their ownpreferred language in tools whenbuilding integrations likeWorkspace add ones or shoutouts or even using our largecollection of rest API s So we ve opened up the platformto support development with anytools  any language  on anypublic cloud Of course  if you re usingGoogle Cloud  why wouldn t you Take advantage of Cloud Run cloud Data Store and cloud AI topower your app So there s a big opportunity tobuild on the Workspace platform with the tools and languages ofyour choice Now let s take a look at how theprocess works across the threecore stages of app development the design  develop and publishphases of your app Starting with the design phase we have created the card buildertool This tool allows you to lay outthe visual design and userexperience of your app There are a range of templates and you can also drag and dropcomponents of the app around l But for more davensd developers you also have the ability to seethe code in the editor  and makeupdates there as well As we move on to the developphase  you now have a range oflanguage and tooling to choosefrom  whether using options fromGoogle like app script  or yourown preferred language andtooling While App Script is a greatoption for developers  we haveinvested in and extended theGoogle Workspace platform toembrace a wider range ofindustry standard tools  sodevelopers can use their owntext stack and reuse theirexisting code when building forWorkspace When your integration iscomplete  congratulations You re ready to publish it For public use in the Workspacemarketplace  where it will beavailable for billions of users It s worth noting that there aremore than 5 300 publiclyavailable apps in themarketplace today This number really highlightsthe diversity of use cases andremarkable opportunity fordevelopers in building forWorkspace And of course if you redeveloping a custom app for usein your own organization  youhave the ability to publish thatapp privately so it remainsexclusively for the use of yourorganization So there you have it A quick tour of how you design develop and publish newintegrations for Workspace Now let s hear from you  whatWorkspace platform technologiesare you curious to learn moreabout throughout the conference Is it developer tools andlanguages Like app script oScript Or AppsOr maybe it s productintegrations through add ons orshoutouts Or perhaps data API s or dataaccess through Workspace API s But that s enough about thetechnology platform Why don t we take a look at afew real world examples ofintegrations developer workspace  and let s start with thecopper CRM add on for G mail Copper identified a problem Workspace users needed a simplebut powerful CRM solution thatis built to work for apps andWorkspace So naturally  they built thesolution Here you can see it in action as a sales rep receives anE mail from a customer The copper add on detects theE mail After installation  the copperadd on will detect the E mailand pull up the customerinformation automatically But the customer s informationright next to their E mail  sowe click through theinstallation  we will eventuallyget to    there we are As you can see  the cop peradd on has detected the E mailand pulled up   With the customer s informationright next to their E mail  thesales rep has all the insightthey need from copper  and cancraft the best response to thecustomer All without leaving G mail Copper is a perfect example ofbusiness that has grownalongside Workspace  and today 100 percent  yes  that scorrect  all of their businesscomes from Workspace customers these are customers who value abest in class solution withdeep  native integration tocritical Workspace apps  such asG mail  Drive and Calendar The opportunity to build for theWorkspace platform spansmultiple surface areas So let s take a look at how thislooks in chat anChat and Spacesthe brand new Jira bot fromAtlassian Here  you can see a customerservice rep jumping into thecustomer success chat space toreport an issue with a customer After a short discussion in thespace  the rep creates an issuein Jira  via the Jira bot Every time there s a statusupdate on the case  the Jira botwill update the team members inthe chat This saves the entire teamvaluable time they might havespent going back and forth toJira  and check updates It also helps the team focus oncollaborating together to helpthe customer instead ofnavigating their tools We would love to hear whatWorkspace integration  or appsyou re using or would like tosee Please share in the comments sothat the community hears whatare some of the best These are just a few examples ofhow developers are innovating inWorkspace  and driving businessvalue for themselves and theircustomers We can t wait to see what youcreate and innovate onWorkspace  whether it s anotebook app or a professionallydeveloped solution Either way  we ve provided arich canvas here for you tostart with and believe the bestis yet to come So look at the chat It looks like you re excited toget going and ready to getstarted Visitdevelopers Google com Workspaceto learn more about integratingwith Workspace    Thanks to my colleague  Matt for taking us under the hoodwith Workspace today  and aspecial thanks to you  ouraudience  for your engagement We love seeing your comments onChat Now stay tuned for our live Q A covering everything from ourspotlight and this demo Thank you so much We ll see you soon    All right So we want to hear what s yourrole at work Do you work in the ITdepartment Are you a developer Are you a person who usesWorkspace just in the process ofyour daily work  or somethingelse entirely Let us know in the chat LAll right  Javier  while we waitfor those results  why don t youanswer a question for us    Let s do it    Tell us  what are you mostexcited about in Workspace thesedays    Well  I m excited about thepace at which we re deliveringsome amazing new capabilitiesfor people But I m most excited about theway we re bringing the Workspaceproducts closer together andhelping people be that much moreeffective at getting their workdone  collaborating with theircollcolleagues  building documents basically coming    translatingideas and turning them in toreality by using our products It s very exciting and we can twait for all the stuff thatwe re going to come out withthis year and next    Absolutely And I feel like  as I ve seenthe chats coming in  customersare talking about examples ofhow they re using Workspace  howthey re going to use some of thenew Appsheet innovation in Gmail  so it s really exciting Okay Our poll results have come backin  and it looks like  do youwant to read the results    Yeah Well  it looks like we havepretty even split between our ITfolks and developers  which isgreat and consistent with  like one of the key    there s a lotof great content for all of youat Next  and then we also have ahealthy portion of end users andother folks So widening the audience  tryingto make this a highly relevanttopic for a lot of people Not just technologyprofessionals People use this every day  soI m excited so see this manypeople    Absolutely All right Let s jump in with our firstquestion    Let s do it    It comes from Lucy  who saysI saw the news about the newWork Safer Program Can you tell us a little bitmore about that    Sure Thank you for the question Lucy So we re announcing Work Saferas a way to bring the best ofGoogle Workspace and from GoogleCloud Security together to helpcompanies rise to the growingthreat of cybersecuritinessacciincidents  et cetera So it brings together GoogleWorkspace with some excitinginnovation from our securityteam and from those customersthat have broader  more diverseenvironments  we ve partneredwith two of the best names inthe security business  Palo Altonetworks and clCloud Strike toprovide a complete solution forthese companies to remain securein a much more challengingenvironment    That s great That s great to hear Kind of an all in 1 solution All right Let s move on to question number2 And this one is going to be forErica And it comes from Michael whoasks  is it hard to collaborateif members are using differentplatforms to join For example  he s running off ofGoogle s tool set that has apartner who s using a differentplatform    Yeah This is a really importantquestion  Michael  thank you forthat I think the reality is that welive in a world where there sdifferent different sets ofproducts that you or yourpartners or your customers aregoing to be using  and it sfundamental to  you know  ourway of viewing the world thatWorkspace should not be a wildgarden It should intersgrate It should connect and make thosetasks easier for you So on the Drive Doc Sheets side we invest really deeply inmaking interoperability withMicrosoft in particular  totallyseamless  and very easy to use So  for example  you can open upMicrosoft documents in theGoogle editor and have the fullinteractive experience Those changes will be saved backto your Microsoft document including comments All of those changes are updatedin real time But there s also a reallyimportant third party ccompatibility component  which Iwould love for my colleague Matt  to discuss    Yeah It s fundamental to us  thatwhether using tools whichinteroperate with the Workspacesusuite  or things that complementthe types of functionality thatwe offer  that these all worktogether I ll take one example We recently announced our newpartnership with Miro  whichdoes virtual white boarding orcollaborative canvassing  andyou can imagine that as you reworking with that tool set  notonly are you building things onyour canvas  but you rescheduling meetings  you reholding meetings at which youwant to collaborate in real timewhile changing things withinthat tool And so what we are doing ismaking it really easy for you toschedule a meeting in calendar join a meeting in meet  at thesame time be working in yourMiro white board So yes  we believe very stronglythat all these things shouldwork better together Workspace and partners  a reallygreat experience for ourcustomers and users    That s great Thank you All right Our next question comes fromBambang  who s with ChandraOsrey and asks must we have oruse G mail or company E mail forusing the rest of Google Cloudand Workspace I think Erica  this is probablyone for you    Sure Yeah Happy to take that We have tailored offerings thatdon t require you to migrateE mail So Workspace essentials isperfect for that It includes the Workspaceproduct set  but without E mail so it integrates easily  and wealso have deep interoperabilitywith Outlook and Calendar    Got it Great Javier  we re going to move toyou next  and the question comesfrom Hannah  and Hannah saysthat her team loves newfeatures  like being able tojump right in to a Meet from aDoc  and is wondering  willintegrations like that beextended to other apps insideWorkspace    Yes In fact  we re very excited tocontinue to advance theopportunity of bringing  youknow  to start  actually  fromthe real time collaborativeexperience that Google Docs pineneared  now I think 15 yearsago  I believe yesterday was our15th birthday One more and we can drive  andbe able to allow users to gofrom working in real time withothers in a document to easilygetting into a live video feedwhere you can hear and see eachotother And so that progression  Iguess  is a really importantpart of enduring seamlesscollaboration  and it applies we see  to both our own editors as well as  as Matt mentioned third parties over time    Yeah Got it    Yeah    Makes sense Well  we re going to stay withyou  Javier  for this nextquestion  which comes fromCarter  who says my company usesa lot of point solutions likeZoom and slack  in addition tohaving Microsoft and Googlelicenses So what would you say is theargument for consolidating    Well  look  one of thestrongest aspects of GoogleWorkspace is the element ofchoice  the choice that usersmake  billions of users havemade to select individualproducts for key roles in theirlives  at work  et cetera And so we re guided as we refinethese products by that sense ofaffinity that people have to theapproach that we bring That approach is best expressedand nurtured  I think  throughdeeper and interintegrationbetween our products to createsomething that s ultimately new and at the same time werecognize that there are othercompanies around us in thisvibrant ecosystem of ours thatoffer different points of viewand different aspects of what wealso provide  and we believethat there s ample opportunityfor users to make an informedchoice and really recognize comes down to familiarity withthe products  the sense ofsecurity that you get from anintegrated solution  andultimately the cost So it s a choice everyone makes and over time  we feel reallygood about the continued growthof our ecosystem    That s great That s great Thank you Erica  we re coming back to youwith a question from Miles  whoasks  how do you plan to winover die hard fans of Excel andother Microsoft creCreation too   All right A good  challenging questionfrom Miles Well  as Javier said  weintroduced Docs and Sheets about15 years ago  and thatfundamentally rethought what afile should be Our belief is that theseexperiences need to leverage thebest that the internet has tooffer So inter actactivity  connectivand data And they shouldn t just be aphysical    a digitalrepresentation of a physicalpiece of paper that lives onyour hard drive And that s in stark contrast tothe mental model that  you know Legacy Solutions have evolvedfrom Over the years  we ve invested aton in user experience andmaking this incredibly easier toadopt And the great thing about all ofthis ease of use which billionsof people take advantage of which is has Google Security you know  enmeshed inunderpinning the wholeexperience I would call out a couple offeatures that I hope people areaware of but if they re not they should be So Connected Sheets  forexample  if you weren t aware allows Sheets users to analyzebillions of rows of data  yes billions  in a single sheet  sothat really pushes the envelopefor what a productivity tool cando  and our innovation aroundSmart Canvas  which really supercharges interactivity intelligence and connection torich data in the form of thesemodular building blocks thattranscend the file boundaries So we bring in smart chips thatbring rich information aboutpeople  events  files templates We have tables and task lists All of that really furtherenhances what these files shouldbe So what I would say is for anyof the die hard Microsoft fans we would love to give you atour  and I bet we could changeyour mind    Got it Great Thanks  Erica All right  Javier  we re comingto you next with a question fromJuliet  who comments that therehave been a lot of securitybreaches lately in the news  andwhat are we doing from aWorkspace perspective to protectcustomers from these sorts ofattacks    Well  look  it s a greatquestion  and thank you for thequequestion A lot of these attacks originatefrom the architecture and habitsthat went along with legacyproducts  right The sharing models  the approachfor storing content  a relianceon heavy  powerful clients thatstore a lot of informationlocally on them  but don tnecessarily provide the bestdegree of security Google Workspace was born in adifferent era  and has had since the very beginning  adifferent point of view aboutthe design and the approach tosecurity  and it comes  I guess as part of us being cloud only right Like  we grew up  and werearcharchitected for the distributedworld in which we live in now and it is the reason behind whyour products are so effective So we ve continued to investaround that to make it evenclearer how  along withenhancements and innovativecapabilities from Google CloudSecurity  like BeyondCorp  etcetera  we establish acompletely different model forsecuring content while ensuringthe absolute best and mosteffective level of collaborationwithin a company  and beyond acompany  so externally withother third parties  et cetera in partners    Absolutely So it s really architected to besafe in this world    Yes Yes And actually  look  youmentioned this idea of habits right Like  these    the notion ofattachments  right  like themodel that people grew up with you know  when productivitytools  digital productivitytools were first introduced  isthe main vector through which alot of these ransomware andother terrible things tend tostart  right Somebody  you know  either opensan E mail  like a phishingattempt  or a maliciousattachment  something that getsspread through  you know various different tools thatpeople are used to using  andcompanies are really desperateto say how did we actually put astop to this  and by the way can we do that in a way thatactually works us towards asense of transformation in ourcompany So it s not just about securingyour communication andcollaboration  but ultimatelytaking that full opportunity tomake it more effective at thesame time So it s pretty exciting    Yeah  it is exciting And  you know  you have beenworking in this space for solong You mentioned the legacy tools and  you know  from    to workat different companies  you vebeen so focused on theproductivity market What have you learned acrossthose different experiences anddifferent products that is mostimportant  and the knowledgethat you use today    Well  listen  I think themost honest answer to thatquestion  and it may surprisesome of you  is you can t pleaseeverybody  right You re building products thatplay these very crucial roles inpeople s personal lives  atschool  at work  and part oftheir power is that they arehorizontal in nature They allow people    they recanvass for creation articulation of ideas  etcetera And so you have to be verycareful and deliberate about theprocess you use to evolve thoseproducts And so as I mentioned earlier you know  we began our journeyas Google and the collaborationspace in a very particular pointin time  where it allowed us tohave a different point of view a more modern approach  and aneye towards simplicity thatdidn t undermine thesophistication and power of theproducts that we were doing That s what we are reallystrongest at And so as a result  the task ofevolving those opinions inproducts like Docs Or Sheets orG mail or Meet  any one of theseproducts  they are therepresentation of a point ofview that Google is bringingthat customers are actuallyinfoinforming  and helping us refineover time  but it ultimatelyrequires that we reallyunderstand what it is that webelieve the right result is forour customers  and that maymean  in some cases  that theseproducts are uniquely powerfulfor key scenarios  and are notnecessarily just like somethingthat can be used for    they renot multi tools  right    Absolutely    So it s a really exciting andchallenging journey for us    Absolutely And having that strong point ofview  it s a great opportunityto get feedback from all thecustomers who are living   listening now Please  share your feedback inthe chat We want to hear from you All right We have another question  it scoming in from John  and it sgoing to be for Matt  and Johnasks  what are you doing toencourage developers and ISV sto build for Workspace    Yeah Thanks for asking the question I know we talk a lot aboutcustomers and users  but at theheart of a successful platformis developers  and so what makesthe developer want to build onthe platform Let s start with our openphilosophy Google as a company  for a longtime  has believed in theopenness of data  the opennessof access to platform  and thatextends to work space We built an extensive set ofAPI s and frameworks  and theyare completely open You can go build somethingtoday  right now Second is reach If you are building something ona platform  you want anaudience  right You want to build a business ona platform  you want to havesuccess  your customers havesuccess  and we have    orWorkspace certainly has a lot ofreach We re talking about 3 billionusers  millions of payingbusinesses and paying customers And this is fundamental for whyyou want to be on the Workspaceplatform Finally I mentioned API s andframeworks We ve put a lot of effort in tomaking sure that they are easyto use and easy to build on When I say easy  easy from adeveloper point of view It shouldn t take you months andmonths to deduce how it works It should be something that youcould build up prototype in anafternoon  and we believe thatwe have made it that way foryou And finally  you ve seen a lotof logos on our slides today andour discussion points today These are really proof points Atlassian  Miro  DocuSign  tothe success that our partnersare having  and the fact that webelieve in being bettertogether    Absolutely Thanks  Matt Okay  Javier  we re coming backto you    Okay    With a question from Laura    Hello  Laura    Laura is asking  with COVIDstill keeping a lot of companiesremote  what is Google andGoogle Workspace team doing tooffset the difficulties ofdistributed work    A lot  is the honest simplest answer Now  look  I think there s a   it s a two part answer From one side  as anorganorganization  2007 listen topeople We have to get feedback fromemployees We ve learned a lot from eachother  and about the differentsituations that our employeeshave to face on a daily basis It s informed our point of view as I mentioned earlier  thisidea of having an opinion We know that just because workdoesn t take place in a specificnamed location anymore  thatthere aren t  like  a whole hostof new problems andopportunities that you have toaddress  not just with products by the way  but also withpolicies that accompany theadopt  like recovery days and ahome office stipend  et cetera Now  from a product perspective we have this incredibleopportunity to channel thoselearnings  along with the inputthat we get from our customersfrom around the world  and evenour users  to say  how are wehelping and how can we be morehelpful  I guess  in such abroad  uniform sense of theword  by helping people not justuse these products  but besuccessful with them  and again this involves a whole lot oflistening  which is why I mgreat to have    it s  you know   the dynamic experience herewith Cloud Next and also allthese great conversation withusers and customers that we vebeen having everywhere So  you know  it s anever ending task    Absolutely Across two parts The org and the product    That s right That s right    All right Well  it looks like we are outof time So I d like to thank everyonefor joining us today  and a hugethank you to Javier  Erica andMatt    Thank you  Katie    Of course It s great to be here alltogether Thank you everyone online forjoining in today  and I d liketo remind you that even thoughwe ve finished the live Q A  theday is not over yet  and there sactually another great sessionthat s about to begin shortly and it s from our own chiefdiversity officer  Melonieparker who s going to be leadinga DEI spotlight session thattalks about the importance ofSTEM and creating pathways tojobs in tech It s a really important topicand I hope you all tune in tocheck it out The details are on the next Website Thanks so much for joining us Bye    Thanks  everybody    Hello  everyone My name is Melonie Parker  and Iam Google s chief diversityofficer I would also like to introduceMelanie  who s with us today She s going to be providing ASLinterpretation It is great to be back for mythird Google Cloud Next This time  for a look atnurturing and developingpathways to careers in tech Today  I d like to talk abouthow building a more diverse inclusive culture is not just acompetitive advantage for us atGoogle  but it s aresponsibility as well And as we get started  I want tolet you know that the chat isopen so you can engage with eachother during this session I want you to share thoughts share where you re tuning infrom  and we ll have a few ofour folks on hand to join theconversation We ll also get in to what we redoing to widen pathways to techcareers at Google and across ourindustry But first  I am so proud to saythat Google is committed tomaking diversity  equity andinclusion a part of everythingwe do From how we build our productsto how we build our workforce I work hand in hand with ourCEO  Sundar Pichai  and as partof our executive leadership teamto bring our global DEI strategyto life I also lead a passionate groupof people whose work touchesevery 140 000 employees stretches across nearly 60countries  and helps millions ofusers who use our products andour platforms to run theirhouseholds  their businesses andtheir lives And personally  I travel down avery unlikely path to land thisposition I m a black woman who grew up ina small town in North Carolina My brother and I are proudalumni of HBCUs  and we were thefirst generation in our familyto bring home a college degree which I earned at a historicallyblack university  HamptonUniversity And unlike many of my peers  Ididn t have anyone at home toexplain the unwritten rules ofcorporate America  or to help menavigate my career Instead  I cobbled together acommunity of mentors  managersand inspirational voices whohelped me find my path And that s an idea that I wantto focus on today Access  opportunity andnetworking are criticallyimportant building blocks for asuccessful tech career And that s especially true forfolks like me who come fromnon traditional backgrounds Too many people fromunder represented communitiesfind themselves confrontingsystemic inequalities  such as alack of access to STEM educationor access to high speed internetthat disrupt the pathway to techcareers And for many who do navigate thepathway  we ve seen how theweight of inequality can fostera sense of imposter syndromeonce on the inside It s that sinking feeling ofdoubt in our abilities  and afeeling like a fraud  or that wedon t belong  and that for some it will linger for an entirecareer Overcoming inequality andcreating access to opportunity including for ourunder represented talent  thisis a top priority at Google And we re taking decisiveactions that createopportunities and steadily growa workforce that reflects theworld around us  to launchprograms that support ourcommunities globally  and tobuild products that better serveall of our users In other words  to build foreveryone  we need to weavediverse perspectives into everystep of our design  engineeringand testing processes  andinnovative ideas can come fromeveryone  from Googlers at thegrassroots level to engagedleaders and to establisheddiversity councils across thecompany As an example  the Google Meetteam worked on the lightenhancement feature to betterrepresent all skin tones in aninclusive way during our videocalls  and through feedback froma diverse group of testers  theteam was able to determine theright algorithm that betterworked for all individualsacross the skin tone spectrum And this ensured that all videocalls on Meet work well foreveryone  and it was thanks tothe collaborative work andquestioning  how to makeproducts equitable for all In another example  Google hasinternal teams to foster equitythrough technology  empoweringour black   community toinnovate in the digital space bysupporting their educationaljourney and helping them landroles in the tech industry And we ve also built externalprograms like Cloud Career JumpStart It s a 12 week learning coursefor under represented studentswith computer science orinformation systems relatedmajors  or the relevantexperience  and it offers freeaccess to Google Cloud Associate Cloud engineertraining to prepare for thecertification exam We just published our diversityannual report  and I am excitedto share that 2020 was our bestyear yet for hiring our Black  and Latinx   Googlers in theU S It was also our best year everfor hiring women in tech in theU S  and globally In addition to hiring  we realso re doubling the focus onretaining Googlers from allunder represented groups  and infull transparency  this is anarea where we can and we must dobetter Google depends on the best brightest and most diversetalent to not only help us solvetoday s challenges  but also toco create solutions totomorrow s problems So we are building a lastingmodel of inclusion and belongingthat helps all employees feelseen  connected  supported  andI hope proud to be in service Belonging is key tore recruiting our workforce andacknowledging that not everyoneexperiences our culture the sameway Despite our best efforts  someteam members are still feelingleft out There is work to do aroundbuilding the culture that we veall been working so hard toco create Clearly  there s some othermissing element And as different as we are  weall have the same underlyingneed to feel valued  to feellike our contributions matter to feel like we belong here But being included isn tnecessarily feeling like youbelong Being invited to the table isnot the same as being welcome atthe table  and being valued likeyou belong Personally  I ve been inspiredby the work of John Powell John is an expert in the areasof civil rights and civilliberties  and he s a professorat Berkeley  and he leads theInstitute of Othering andBelonging  and he looks at ourprimal need as humans to belong And what fosters belonging andwhat prevents it And if you think about it  oneof our primal fears is to beostracized by a group Aside from food and shelter  weneed to belong And we ve evolved to ensure thatwe could maintain access to theresources that the group has Many behaviors are dictated bythis need to be accepted byothers But we re also guided by thefear of losing that veryacceptance And we ve evolved to protectourselves and our loved ones bydetermining who s in and who sout  and that comes down toidentifying who s the leastrisk Those that we perceived mostlike us and relegating those whoaren t to the out crowd And John calls that primalsorting of people in to ins andouts The cool kids versus thenon cool kids  as othering  andwe don t necessarily do thisconsciously  but it is thebehavior that stems from ourunconscious bias The subtle ways that we sendmessages  even virtually  ofwho s cool and who s not  who svalued  who s not And that s why I believe thatour work  it s not trulypossible until we foster thissense of belonging And as evolved humans  we havecontrol over this There are ways to check in withourselves  noticing  are weturning inward  focusing on onlywho and what we know  versusturning outward to connect withand explicitly work with othergroups  and to seek ways tobuild common ground And that would foster belongingand empathy And this means that we need toco create the workplaceexperience with our internalcommunities  and redefine whatit means to be a Googler byacknowledging where we can dobetter And by enduring everyone feelslike they belong  is positionedto do their best work  and canconfidently bring theirauthentic whole selves to workeach day And for under representedtalent  this can make thedifference between surviving andthriving in their career And for those of you who areaccustomed to muting parts ofwho we are  authenticityempowers us to do our very bestwork Last summer  at the start of theglobal racial justice movement I realized that I could nolonger code switch or checkparts of myself at the door inan act of self preservation DefiniteTo lead our deep work in racialequity and address the systemicand structural barriers  Ineeded to reconcile the mostvulnerable parts of my ownidentity as a Black woman  withthe Black woman who happens tolead diversity at Google And I know that this is the typeof workplace environment that westrive to build And with our collective consistent efforts  that is whatwe ll do We know that Google is mosthelpful when our products andour services reflect the needsof the people who are usingthem To best serve our diverse globalcommunity of users  we need amore representative workforce and while we re making progress we know equalities persistaround access  opportunity andnetworking for under representedtalent  but through investmentsin creating pathways to STEM and with help from allies  wecan broaden the talent pipeline improve product design  andfulfill our mission of makingDEI part of everything that wedo What does this look like There s a real cognitive shifthappening both inside andoutside of Google arounddiversity  equity and inclusionand how we can best build foreveryone Externally  we re creatingpathways to careers in tech asearly as elementary and highschool We re expanding recruitmentbeyond the usual campuses likeStanford  Berkeley  Harvard Yale  and doubling down onhigher education institutionslike Hispanic servinginstitutions  and historicallyblack colleges and universitiesserving under representedtalent Why Because we know that 25 percentof Black graduates with STEMdegrees  for example  they comefrom HBCUs  just like the one Iattended So this year  we began deepeningand sustaining relationshipswith HBCUs In June  we announced anhistoric  unrestricted  50million grant that allows thepresidents of these esteemedinstitutions to allocateresources in the manner thatthey see fit We re also broadening careerpathways to include a major growwith Google career readinessinvestment that supports ourHispanic serving institutionsaround the U S A significant economicopportunity grant for workforcedevelopment and digital skillstraining to thousands ofHispanics  and a careerreadiness program for NativeAmerican  indigenous servinginstitutions And that s just a slice of thework that we re doingexternally And while I mentioned that 2020was Google s biggest year interms of hiring Black and Latinxtalent in the U S   we know thatthere s much more work to doinside of Google to retain andgrow our talent So broadly speaking  we vedeveloped racial equityeducation for all Googlers  andincorporated it into our newhire orientation We ve doubled the size of ourretention and progression teamand we re on the path to triplethis team by 2022  and thisenables us to provide one to onesupport like coaching networking and internal mobilityopportunities to helpunder represented Googlers growand thrive And we rolled out an innovativeonboarding program for our newBlack Googlers so that they feelmore supported right from thestart Our employees want to know thatthey can grow and thrive withus  and that s one reason whywe re working to increaseleadership representation amongwomen  Black and LatinxGooglers We ve introduced pathways tosponsorship for women intechnical leadership roles It s designed to acceleratetheir careers We leverage partnerships withmore than two dozen communityand professional organizations and these partners offerprofessional development  aswell as networking opportunitiesfor under represented Googlers We also offer skills buildinginitiatives like the CloudTechnical Residency Program  andwith great intention  we vebegun building a culture ofbelonging for all Googlers  as Idescribed earlier We ve been very open about oursuccesses and our challenges and I invite you to explore thisfurther at diversity google com Now  I know many of you arewondering what can I do So as you weigh your ownstrategies around engagement andretention  consider the role ofallies in driving results Allies can be leaders  managersand peers They support folks inmarginalized groups to whichthey don t identify Internally  we ve created anumber of pathways for alliesthat are seeking to support ourunder represented colleagues These include an allyshippathway with group exercises forallies and resources forpracticing or encouraginginclusive behaviors We also have Google inResidence Experience Google s softwareengineers  teach introductorycomputer science classes on HBCUcampuses  and we have faculty inResidence We have more than 50 facultymembers from 30 HBCUs to designproject based industry informedcontent and to implement thatcontent in their classrooms Google has doubled in size inthe past few years We know our DEI commitments needto keep pace Operating at this global scale it heightens our sense ofresponsibility around DEI  andto paraphrase my colleague Annie Jean Baptiste  there s noblueprint to doing thisperfectly And while we don t have all theanswers  we re eager to sharewhat we ve learned and also tolearn from others And in the spirit of learningleadership  I hope I ll beinvited to share morediscoveries and progress atGoogle Cloud Next 22 In closing  when our mission isto build for everyone  we haveto work with everyone So with help from allies andthrough smart investments andcreating pathways to STEMcareers  I am confident ourtalent pipeline will flourish Real  meaningful change can takeyears We know But there s no need to wait forchange to come If you haven t already  pleasehead over to the chat room andjoin the discussion There  you can share some ofyour successes and yourchallenges around creatingpathways to tech careers This kind of dialogue can openminds and truly inspire change I look forward to sharing moreprogress with you next year  andI ll bring others along who havebeen on this journey with us toshare their experiences Thank you for wrapping up Day 1of Google Cloud Next with me It s been great sharing thistime with you We look forward to seeing youall here tomorrow for Day 2 which kicks off with ourdeveloper keynote Until then  be sure to registeratg co cloudnext for access toall of our content  and join theconversation on social using Googlecloudnext See you tomorrow Google Cloud NEXT  21Google Cloud NEXT  21Google CGoogle Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21 GoogleCloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21Google Cloud NEXT  21  APARNA SINHA  Hi I m Aparna Sinha Welcome to Day 2 of Google CloudNext I ll be co hosting today alongside Urs H lzle  and we vegot a really great lineup of newtechnology and demos from GoogleCloud But before we get started  Iwant to thank you all for takingtime out of your busy week to behere with us Yesterday was an incrediblefirst day at Next Thomas and Sundar made someamazing announcements This week  we re releasing over100 new products  services andprograms for you We re kicking off Day 2 now withThis keynote   a cloud built fordevelopers   after this  we lljump right into a live developerQ A  where you can ask usanything And then tomorrow is communityday  and that will be totallyDede indicated to you forlearning  discussions networking and all kinds of fun All right Let s get things started bywelcoming Urs Hi  Urs    Hi  Aparna  hi  everyone I hope you re as excited as I amto get started today  and sincethis is a developer keynote we ll kick it off with our firstdemo right away from GoogleResearch  and our colleagues whohave been working on someamazing voice technology    Welcome to Next  21 To start  I d like to thank you Wherever you are in the world For coming and sharing Your time with us today   URS H LZLE  Now  before youget too impressed by my languageskills  I did not actually speakthese words What you just heard is a customtext to speech model that hastrained on my voice and cangenerate synthetic speech indifferent languages And our teams have alreadystarted using it to improve ourvoice technology for all users   APARNA SINHA  That s right Google s Project Euphonia isalready using Custom Voice tohelp people with atypical speechto communicate and be betterunderstood Technology like this creates amore inclusive world To find out more about this check out the Project EuphoniaWeb site Custom Voice is available todayfor select Cloud customers We re very mindful of thepotential misuses of thistechnology and we re takinggreat care to prevent them byreviewing each unique use case URS HOLZLE  Now  look aroundyou Every day you see innovationthat was brought to life bydevelopers like you withpersistence and talent Google has a long tradition ofsupporting developers in opensource For years  developers have beenUsing technologies likeKubernetes  Firebase TensorFlow  Go  Angular and gRPCAnd many others When we built Google Cloud  webuilt it fordevelopers  and we re inspiredBy all the things we ve createdwith it Our job so to make it easier forFor you to do what you love So we focus on making you asproductive as possible with theleast amount of effort So in everything we design  wetake all the feedback you reSharing with us and build aCloud platform that just works Whether that s by nativelyembedding key security orsustainability features intoThe platform itself  or byfeaturingpartner solutions right withinour console  we re focused onone goal  giving you the bestdeveloper experience of anyCloud provider   APARNA SINHA  Google Cloud hashad tremendous traction withDigital Native customers sinceOur very early days How have you seen customer andpartner adoption evolve sincethen URS HOLZLE  Well  many of ourbiggest customers are cloudNatives  but we see tremendousadoption by a broad segment ofenterprise customers intraditional industries as well like entertainment and financialservices For example  Major LeagueBaseball  which is NorthAmerica s oldest andmost attended professionalsports league  is using GoogleCloud to modernize fanengagement and increaseoperational efficiency Equifax  founded in 1899  andone of the world s largestConsumer credit reportingagencies  is transforming itselffrom a credit bureau to anext generation data  analyticsand technology company built onGoogle Cloud   APARNA SINHA  We ve seen ahuge shift  essentially everycompany is becoming a techcompany to increasecompetitiveness and establishleadership in their industries Developer talent and cloudservices are at the heart ofthis shift Whether you call it  digitaltransformation   or somethingelse  companies of all sizes arefinding that Google Cloud isoptimized to help make yourdata  your applications and yourtalent more useful and relevantto your business    URS HOLZLE  Exactly As Thomas mentioned yesterday everyone needs to be thinkingthrough how they llfundamentally shift into atechnology company to servetheir customers in the mostmeaningful ways ten years fromnow You  as developers  are key toMaking this happen So  organizations askthemselves  do we have the mostcutting edge technology tobecome a leader in our industry You  the developers  arewell equipped to answer this And we are super focused onmaking you and your companysuccessful And there s two areas wefocus on to support your growth First  of course  making iteasier for developers to gettheir job done Second  investing in thedeveloper community  so thateveryone can learn and grow fromeach other Let s talk about how GoogleCloud is making it easier fordevelopers to get their jobdone From our transformationalinfrastructure stack to our deepinnovations in data  securityand AI  every feature we releasestarts with simplifyingdeveloper experiences Take our open cloudExperience  for example We recently expanded our computestack to include Tau VMs  whichdeliver 42 percent better priceperformance over othercomparable options in themarket Thanks to our Zero Trustapproach to security  GoogleCloud was ranked leader in IaaSPlatform Native Security byForrester Well ahead of the competition And of course we re focusingalso on managed services thatmake iteasy for you to deploy  scaleand manage Kubernetes anywhere Google Cloud has the mostcomplete and most securecontainer experience fordevelopers The 2021 Gartner s SolutionScorecard for Google KubernetesEngine gave GKE an overall scoreof 92 Again  well ahead of thecompetition And with GKE and Anthos  you canrun these containers anywhere On premise  other clouds and onthe edge You know  anywhere So it s fair so say that in theyears since Googleinvented Kubernetes  containershave completely revolutionizedIT operations Recently  European filmmakersfrom Honeypot io created adocumentary on the history ofKubernetes  and it will be outin January 2022 You re the very first audienceto have a look at the trailernow So let s roll that    Do I look at you Look at camera    2013  it was clear that Cloudwas a thing  but most folks werefocused on infrastructure Cloud The dirty secret for a long timeis like  you know  people whowere either building their owndata centers or using COLOs there s a huge resource waste    And so at that point automation tools are all therave People are now trying toabstract away the servers    Google was looking for waysto apply its internalinfrastructure expertise to theCloud    As we started looking attechnologies like Docker  wewere  like  impressed by thestrength of what they hadaccomplished in solving a veryspecific problem    This is going to happen withus or without us    Google had to make a boldmove in the cloud space to bethe long term winner    Every big start up I feltlike had a containerorchestration project  and halfof them were announced atDockerCon 2014    Open source is mostsuccessful when it s played as apositive sum game   APARNA SINHA  This is such agreat community    URS HOLZLE  Yeah Absolutely I recognize a lot of the faces and I can t wait to see thefilm Now let s get back to how we remaking it easier for you tobuild the leading technologycompanies of tomorrow Kubernetes deployments caninvolve a fair bit of manualconfiguration of clusters nodes  load balancers  YAMLfiles  et cetera But not on Google Cloud We offer you the most automatedand secure Kubernetes experienceavailable With GKE Autopilot  Googleprovisions and manages thecluster s entire underlyinginfrastructure  including thecontrol plane  node pools andWorker nodes  and that lets youfocus on the higher levelservices and applications thatyou re building Nobody else offers anything likethis Beyond managing node upgrades GKE Autopilot automaticallyconfigures security featureslike Shield GKE Nodes  SecureBoot  and Workload Identity And it also implementssecurity best practices byblocking less safe features suchas external IP and legacyauthorization You don t get a toy Kubernetescluster with GKE Autopilot You get a sophisticated clusterthat uses the best practicesbrought to you by the team whoBrought you Kubernetes itself You re always up to date and getthe same results as the experts without having to be an expertyourself   APARNA SINHA  The pandemic putdevelopers in the driver s seat and they drove GKE usage toall time highs At the same time  we sawexplosive growth in the use ofGoogle Cloud s serverlessofferings  especially Cloud Runand Cloud Functions It s mainly enterprisedevelopers who have driven thisgrowth Cloud Run excels at developerExperience It s earned the highestcustomer satisfaction ratingamong developers as measured byUser Research International Cloud Run combines the best ofboth worlds  bringing youServerless  and containers There s no cluster to set up orconfigure  so developers areable to scale seamlessly andsecurely Under the hood  Cloud Run scalescontainer instances in isolatedsand boxes Any access outside a sand box ismediated by network controls Or identity and accessmanagement  or both And this isn t just for newapps Cloud Run supports traditionalworkloads like Java Spring Boot and ASP NET We also recently introducedcommitted use discounts to lowerthe cost at scale and  always onCPU  enabling asynchronous andBackground processes to be usedon Cloud Run So you have all the benefits ofserverless without therestrictions The theme here is easier  moresecure development Especially with remote work    URS HOLZLE  You re absolutelyright We ve been focusing on remotedevelopment for some time now But the pandemic has certainlyaccelerated the shift What s more essential to remotedevelopment than to be able touse the full power of GCP rightfrom a laptop with zero localsetup Cloud Shell Editor is a contextaware  remote developmentenvironment that lets youdevelop and manage applicationssecurely from any browser It supports languages like Go Java  Node js  Python and C  It comes with an integrateddebugger source control  APIexplorer  and if you want totest locally on your laptop  itAlso comes with local emulatorsfor Kubernetes and serverlessAPI s   APARNA SINHA  Thanks  Urs Next  let me introduce AbbyCarey  who will show us howGoogle Cloud makes it easy foryou to securely build modernApplications  again  right fromyour laptop Hi  Abby    Thanks  Aparna We developers have had a hardtime writing  extending deploying and operatingapplications  but it doesn thave to be difficult Let s start with Cloud ShellEditor It comes with current versionsof your favorite Dev tools  likedocker  minikube  skaffold andmore There s nothing to download orinstall locally Tutorials are built into CloudShell Editor  which makes iteasy to come up to speed oncomplex topics like GKE   APARNA SINHA  No moreswitching between tabs  docs your terminal and your code This integrated experience ishighly differentiated from otherclouds You can even author your owntutorials  allowing yourorganization to share bestpractices and onboard new hiresfaster    Another popular feature isKubernetes YAML authoringassistance Let s say I want to add YAML fora service to this project I can press control   space find the Kubernetes serviceSnippet Now I can tab through and filleverything in I also get autocompletes  and ifI happen to make a formattingmistake  I m notified there s anissue in real time   APARNA SINHA  Many of youprefer to work locally in IDE The same YAML authoringassistance is also available forVSCode and IntelliJ via theCloud Code plug in Cloud Code has built in supportfor both Cloud Run andKubernetes    In fact  if you re usingCloud Run or Functions  youdon t need to know Docker You can build and deploy yourapp with just one commandbecause Cloud Build isintegrated under the hood This is an application with nodockerfile With the newgcloud run deploycommand  all I have to do isProvide a name for my service and then let it know where mysource code lives  which is thiscurrent directory  and we redeploying So nice   APARNA SINHA  Nice And thanks to this ease of use 98 percent of users deploy anapplication to Cloud Run ontheir first try in less thanfive minutes    I just showed source codedeploys to Cloud Run But there are more ways GoogleCloud has made deployment easierand more secure First  I can scan my builtcontainer images to check forvulnerabilities I ve already run an on demandscan on one of my images usinggcloud artifacts docker imagesscan Now I can copy the ID of thescan  and view my image svulnerabilities with the listvulnerabilities command And then once that s finished  aseverity level is assigned toeach vulnerability to help youprioritize   APARNA SINHA  That s superimportant It s really helpful inaddressing security concernsearlier in the softwaredevelopment life cycle But what if your build pipelineis compromised    For that  I can enable BinaryAuthorization on my deployedCloud Run services This way only trusted containerimages are deployed toproduction   APARNA SINHA  BinaryAuthorization is truly unique inthe industry It enables you to put proactivesecurity measures in place toreduce software supply chainattack risk by blockingdeployments that violate policy And speaking of deploying  we remaking it seamless for you to doCI CD securely You can take advantage ofserverless build environmentswithin your own private networkwith Cloud Build private pools    And for advanced CD  we haveGoogle Cloud Deploy  whichallows you to create customdelivery pipelines for yourspecific use case and needs    That is so cool   APARNA SINHA  A realapplication connects to manysupporting cloud services Can you show us an example ofhow we ve made integrationseasier    Sure When creating a CloudFunction  it s easy to integratewith Secret Manager First  create a secret thatstores an API key  which Ialready did Now  either mount it as a volumeor expose it as an environmentvariable I ll mount it And then I ll name my mountpath This will always point to thelatest version of the secret Now I can securely referencethis API key from my sourcecode This abstraction enablesportability and a better localdevelopment experience Cloud Run also integrates withSecret manager to make it easierto do the right thing and notput sensitive data in source    Love that so much Okay So now you vewritten your app  deployed yourapp and connected your app toother Google Cloud resources what s next    Operating your app inproduction With Cloud Ops  you get oneIntegrated view for your alerts events  metrics and logs No more jumping around multipletools as you try to understandwhat went wrong    That was so awesome  Abby Thanks for sharing this with us    Thanks  Aparna   APARNA SINHA  In each of theseinstances  we ve done theintegration work for you Because the more work we putinto this  the less work youhave to do This principle applies tosecurity as well We ve put a lot of energy intobuilding security natively intoeverything we do  so you caninnovate with assurance Both GKE and Cloud Run benefitfrom the security fixes weimplement before vulnerabilitiesare exposed Just look at the famousvulnerability uncovered in howKubernetes was handling proxyrequests We found it  coordinated andcommunicated the disclosure We fixed it for the entireKubernetes community  and wepatched all our products beforeany customers were impacted Now recent cyber threats haveshifted the focus to softwaresupply chain    URS HOLZLE  That s right Malicious actors are trying tocompromise the software supplychain from bad codesubmission to bypassing theCI CD pipeline all together To help solve this problem  wehave proposed an industry widestandard called SLSA It s a security framework thatprovides common criteria forincreasing levels of softwaresecurity through automation andcryptographic signing at eachstage of the software supplychain And that makes it possible  butnot necessarily easy  and somaking it easy for developers toensure security is superimportant That s why we re focused onBuilding security right into thedeveloper tool chain anticipating and preventingissues ahead of time  not whenyou are most at risk For example  Cloud Build  ourservice that lets you build test and deploy across multipleenvironments such as VMs serverless  Kubernetes  orFirebase  now offers SLSA Level1 compliance by default Because Cloud Build gives youverifiable buildprovenance This provenance lets you trace abinary to the source code toprevent tampering and prove thatthe code you re running is thecode you think you re running Cloud Build is the first andOnly CI CD service tooffer such a capability But we go beyond that As you ve seen  Build Integrityautomatically generates digitalsignatures which can then bevalidated before deployment bymanual authorization That s another Google Cloudfirst And so without you needing to doanything  we prevent anyone inyour organization from deployingcode that has not been built byyour legitimate build system Now  Ensuring securitypost deployment is equallycritical On GCP  you can enablecontinuous scanning and use ourService Mesh to embrace azero trust security model andautomatically and declarativelysecure your services and theircommunication You can manage authentication authorization and encryptionbetween services  with little tono changes to the applicationsthemselves Let me say that again With little to no changes to theapplications themselves So that means that thesesecurity improvements helpsecure not just new code  butexisting binaries as well  soyou can use them for anyapplication you re migrating tothe Cloud Both Anthos Service Mesh and nowCloud Build Hybrid are availableacross Google Cloud and on yourpremise environment and workwith VPC Service Controls andVPC Peering to automatedeveloper security for yourenterprise No other cloud provider protectsyour software supply chain tothis level Because we started working onsoftware supply chain securitylong before it was in theheadlines  and by choosing GCP You benefit from thisleading edge focus on security   APARNA SINHA  Whether we rebuilding foundational opensource technologies likeKubernetes and Istio or turningthem in to fully managedservices like GKE and AnthosService Mesh  our goal is toreduce complexity for our users By helping create industrystandards  we can provide saferand simpler services forYou  the developer This is exactly our approach tosecuring the software supplychain We co founded the Open SourceSecurity Foundation with othertechnology leaders to createsecurity standards for opensource And we re starting to bringproducts to mark like OpenSource Insights  which providesa complete transitive dependencyGraph for many open sourcepackages Now let s turn to Urs to hearwhy Google Cloud is bestpositioned to support you inbecoming a technology leader inyour industry using data as acore asset    URS HOLZLE  Thanks Yeah So far  we ve been talking aboutdeveloping and managing code But data is at the heart of manyenterprises We also have the leading datacloud products in the industry designed for optimal performanceAnd reliability for applicationsof all sizes while scaling toimmense capacity Let s start with databases When it comes to databases every cloud gives you choices They offer SQL databases  whichAre great  but unfortunatelydon t scale And  of course noSQL databases which do scale But unfortunately are not SQL Only Google Cloud gives you athird choice with Spanner Spanner is SQL And  in fact  it just got aPostgres interface  but itscales horizontally  and it canliterally handle a billionrequests her second Nobody else has a scaled SQLsystem On the data warehouse side  wehave the leading cloud datawarehouse with BigQuery Hundreds of customers are usingBigQuery at petabyte scaletoday Petabyte each And you can run BigQuery Omni onAWS or Azure Open source systems for datalake processing like Flink Spark and Beam  run natively onGoogle Cloud in a simpler andmore cost effective way than inother environments In fact  you can realize 57percent lower TCO compared toon premise data lakes for datascience projects On top of that savings  our datacloud also includes the world sfirst and only autoscaling andserverless Spark service And finally  Google has deeppartnerships with leadingdata driven companies  includingDatabricks  Confluent  MongoDB RedisLabs  and many others Together  we help customersaccess an open platform thatpowers analytics at scale yet iseasy to use   APARNA SINHA  Our partnercommunity is central to thehealth of our Cloud business We are especially excited aboutthe innovation from our DataCloud partnerships Together  we have optimized ourInfrastructure for performanceand efficiency to give ourpartners that extra edge whenthey run on Google Cloud One of our leading partners inMongoDB  we have their CEO  DevIttycheria here with us today Welcome  Dev    Hi  everyone Happy to be here   APARNA SINHA  Dev  one of thetrends we are seeing fromenterprise customers is thatthey re now competing forleadership positions in theirindustries by becomingtechnology companies How did you say Google Cloud andMongoDB working together canhelp these customers achievethat transition Well  the companies who are inthe leadership positions intheir industries are those whohave built their competitiveadvantage using software anddata to transform theirbusiness The keyword here is build You can t buy a competitiveadvantage You have to build it This means you need to enableyour developers to innovate asquickly as possible  whetherit s building new software toseize new opportunities or torespond to new threats MongoDB and Google Cloud bothdeeply understand this  which Developers choose maMongoDB andGoogle Cloud because we givethem the tools they need to beas productive as possible including having our servicesavailable in the Google Cloudconsole for easy discovery anddeployment With Google s analytic and AItools This enables our customers toinnovate quickly and emerge asleaders in their industries As a result  we re seeingexplosive growth in ourcustomers embracing the truevalue of our partnership    That s incredible So what s the biggest challengethat you re helping them solve    When you talk to developmentteams  their biggest challengeis managing data Serving relevant data at theright time to the right audienceis critical to building anyapplication Unfortunately  relationaldatabases are not designed forthe way developers think orcode Nor are they designed for scale fault tolerance  or resilience Consequently  development teamsfind it hard to use fast usingrelational databases MongoDB is designed to addressthis problem We make it very easy fordevelopers to work with data andwe re able to address the mostdemanding requirements forperformance  scale and fulltolerance The partnership enablesdevelopers around the world toeasily build modern softwareapplications  to address theirneeds today and tomorrow   APARNA SINHA  Terrific Thanks so much for being herewith us today    Thank you for having me    URS HOLZLE  Yes Thanks  Dev  for joining us There are lots of waysdevelopers can improve theirproductivity Automating tasks that arerepetitive  mastering thecommand line  using the besttools that make your lifeeasier  or reuse others  code just to name a few Another great way to accelerateyour productivity is withbuilding blocks  templates  andfully managed services in areaslike machine learning With Google Cloud  you don thave to be an expert to buildsmart applications With new services like VertexAI  you can build  deploy andscale more effective AI modelsquickly This lets you deliver theinsights to your organizationthat will help them create morepersonalized customerexperiences  run more efficientprocesses  and take a leadershipposition in your industry   ALISON WAGONFELD     APARNA SINHA  So with that let s go to our next live demo how these breakthroughs in AIare advancing Cloud adoption andredefining the world of documentprocessing Hi  Anu    Hi  Aparna We all know how to work withdata when it s in a structuredformat like in adatabase json csv  or justvariables in my code  right But what about unstructureddata Many of the world s businessprocesses start  include or endwith a document  but thesedocuments can be difficult toprocess Think about the ways you couldenhance your application if youCould just unlock that data could unlock that data This is where Google Clouddocument AI comes in DocAI is a platform that hassolutions and tooling forautomating your work flows backed by machine learning We ve bundled together some ofGoogle s flagship AI technology such as computer vision  OCR Natural Language Understanding and even Google s expertise inbuilding knowledge graphs toprovide you with a simple  yetpowerful way to buildapplications that betterunderstand unstructured data Let s see a demo of docAI inaction So here we have a receipt I was buying some officesupplies  since we areunfortunately not back in theoffice yet What I m going to do isI m going to upload this in tothe docAI Platform in the CloudConsole  where we have thisBuilt in preview mechanisms soyou can test out your documents So this going to an end point which has a specialized model wehave specifically trained on avariety of expenses Google maintains and improvesthe models for you   APARNA SINHA  Wait a minute I hope this is not with my data    Absolutely not We never use your data to trainour models Your data is only used to serveyour request So let s take a look at the dataextracted   APARNA SINHA  I ve seen thisbefore Next you re going to tell meThat you re going to automate myexpenses    I knew you would say that But have you ever seen it likethis Take a look at this field thatI m highlighting  the supplieraddress This address isn t presentanywhere in the document    APARNA SINHA   Wow  where didthat come from    This is only possible withGoogle s Document AI The secret sauce here is thattheknowledge graph  which not onlygets back the original text  butIt s going to enrich yourresponse  akin to what you d seein a search  but as part of yourAPI response   APARNA SINHA  That s great    And it s not just this We have several specializedmodels for many more documenttypes of much higher complexity Take a look at this pay slip So I ran this earlier  and we relooking at the preview outputagain You can see that we have somekeys  some fields You can see enrichment on theemployer name and the address Once your data is in aschematized format  meaning thatwe know for every document of acertain doc type  there arecommon important pieces ofinformation So what we did is we pre defineda set of keys So what we do is with yourextracted data  we merge thedata to these pre defined keys so it s much easier to work withthan raw OCR So once it s in a schematizedformat  it s easier to pass downto a service or maybe you reusing something for analyticslike BigQuery or Looker   APARNA SINHA  That makessense  but what aboutensuring accuracy  and  moreimportantly  do we havemulti language support    We know with importantdocuments such as these youcan t afford any missteps whenit comes to accuracy That s why Document AI alsoprovides a human in the loopconfiguration to trigger onConfidence scores so either forspecific keys  or on the entiredocument itself And for translation  we supportover 100 plus languages such asSpanish  Japanese and Arabic No other solution on the marketsupports such a wide array oflanguages   APARNA SINHA  Human in theloop  translation and knowledgegraph capabilities that can beapplied to a wide variety ofdocuments This seems super useful Of course  the next big questionis  can it be applied to big bulky  complex documents likebusiness contracts    Let s take a look So here I read a contractearlier this morning You can see that there aretypical things you d find in anycontract There s some document names the parties involved  somedates  and like with every easy to read  contract  beingsarcastic here  there is anexpiration term So this expiration date actuallyisn t present anywhere on thedocument  and it s actually noteasy to figure out It s not in an easily parsableformat Shocker Google s contract processor isable to figure out this datevalue by understanding signalsfound across the entiredocument   APARNA SINHA  Anu  before yougo  can you tell our awesomedevelopers how to get startedwith docAI    Absolutely I know we covered a lot atbreakneck speed  so please docheck out the breakout sessionson docAI to dive deeper You can also check out theDocumentation for code labs andquick starts We have client libraries in allyour favorite languages  such asPython  Node js  my personalfavorite  Java But it s an API  so you can usethis with whateverPlatform or framework you realready using We are thrilled and look forwardto seeing how you use GoogleDocAI to power yourapplications   APARNA SINHA  That wasamazing Can I have a high five Yeah Thank you  Anu I loved every part of it    Thank you for having me    URS HOLZLE  Another areaGoogle has invested in deeplyand that is becomingincreasingly important to moreCompanies is sustainability Many cloud providers have avision for a sustainable future and many aim to match theirelectricity consumption with 100percent renewable energy by 2025or 2030 We accomplished 100 percentrenewable energy in 2017  sowe re the only hyperscale Cloudto do this today And all of that with dataCenters that are twice asefficient as the average datacenter   APARNA SINHA  This past week Sundar talked about Google sgoal to enable over a billionusers to live and work moresustainably by next year To reach goals like this andthose outlined in climatepledges made by moreorganizations every day  we relyon developers like you to dosomething about it But we also know that it sdifficult    URS HOLZLE  That s right One of thebiggest challenges companiesface is they lack the tools toaccount for environmental costs To help developers address thisfor their organizations  webuilt sustainability toolsDirectly into Google Cloud With Google Cloud carbonfootprint  you have access toenergy related emissions datayou need for external carbondisclosures in just one click Now  you won t need thiscalculator if you just want toreport the net carbon footprintof your workload on GCP Because on GCP  it s alwayszero We also have our region picker where you can choose the datacenterregion with the lowest grosscarbon cost Of course  your net impact iszero  no matter what region youpick  but this tool helps you goone step further to becomecarbon free  not just carbonneutral Now  that s actually a tool thatI can t wait to deprecate in2030  or so  because GoogleCloud has committed to be 100percent carbon free by 2030 every hour of every day Now  we also realizethere is still a lot to learnwhen it comes to buildingsustainably To help  we just released amaster class called  sustainableIT Decoded  with some of theworld s top experts Check it out for guidance on howwe can all build moreSustaSustainably While we re proud to run thecleanest cloud in the industry we re even more inspired by thework our customers are doingwith Google Cloud to solveclimate change challenges uniqueto their business And today  we bring you apreview of Google Earth Engineand its integration with GoogleCloud With over 700 data sets and morethan 50 petabytes of data today Earth Engine gives scientistsand developers access to theworld s largest catalog ofSatellite imagery  and to toolsfor driving sustainable impact   APARNA SINHA  Now let s lookat this a bit more closely withan example of how Google EarthEngine and Google Cloud enablecustomers to assess risksarising from climate change But instead of me telling youabout it  we ve invited JoelConkling to show you    Thanks  Aparna The world is constantlychanging  and that createsOpportunities and risks Helping uncover criticalinsights about the changingworld is why we re integratingearth engine into Google Cloud That integration is now inprivate preview Today  I m demo a work flow thatcombines Vertex AI  EarthEngine  BigQuery and Maps toshow how Google Cloud makes itincredibly easy for you toinnovate and deliver insights and do it quickly Here s the scenario You work at an insuranceCompany and you need to analyzeyour company portfolio sexposure to flood risk You think that the new buildingsmay be a strong contributor tothat risk  and you want to testyour hypothesis To do that  we first need tounderstand where the builtenvironment is expanding In other words  we need tocategorize the surface of theplanet That could be hard  butVertex AI offers the tooling todevelop a best in class MLmodel  and Earth Engine providesconstantly updated data Let s fast forward a bit We finished training our model And now Earth Engine is sendingsatellite imagery to becategorized  so yourunderstanding of the world canupdate in near real time Here s the earth engine scriptshowing the results of thatmodel This area in red is where themodelestimates the locations ofbuildings That s your current builtenvironment To find the change over time  weneed a few more lines of code These lines of code give us abuilt environment to 2016 Can and here we calculate thedifference between 2016 andtoday When there s a change  it showsup in purple on the map This is where there are newbuildings So next  we re going to sampleand export the data so we can doadditional analysis in BigQuery This script clusters those datapoints here  and then outputspolygons that show the area withthe biggest changes in the builtenvironment So at this point you have a fewoptions You can identify this data withflood locations you identifyaround the world  also withEarth Engine Maybe you want to enhance yourmodel with weather data andphysical train data That s in Earth Engine  too You can also include data onyour company s insuranceportfolio to get additionalinsight into critical risks We ll wrap up this demo byvisualizing your results at anew feature available on GoogleMaps platform  the open sourcedata library  with a BigQueryconnector  so we now have aclear picture of where the builtenvironment is changing andwhere to focus next for our workon flood risks In summary  no wrangling data no need to manageinfrastructure  just actionableinsights incredibly quickly We can t wait to see what you lldo with Earth Engine s newinnovation with Google Cloud and with that I ll pass it backto Aparna   APARNA SINHA  Thank you  Joel It s incredible to see how ourcustomers can use oursustainable technologies toaddress climate change now I m inspired by all the thingswe ve talked about today Thinking about how you re goingto lead your companies into thefuture  that s exciting No pressure  but it s really upto you We ve invested millions in thedeveloper community over thelast five years  and we llcontinue to invest in the comingyears Urs  as proof of that  Iunderstand you have someadditional news to share today    URS HOLZLE  Absolutely I m really excited to announceToday our new developercommunity program called GoogleCloud Innovators I want to welcome and introduceour first group of leaders whoare driving meaningful impact inthe industry and theircommunities Take a look    This is so exciting    URS HOLZLE  Yeah Through thisprogram  we ll give developersdeeper access to earlytechnology previews and frontline engineers We ll recognize the expertise ofour community influencers bypromoting their contributionsAnd we will work closely withthem to solve the toughestproblems We re excited to come togetherwith this group of innovators Join us atcloud Google com innovators   APARNA SINHA  So cool I ve been waiting for this allthis time Community is extremely importantFor companies to create thatmuch needed human connectionwith developers We hope this gives you a windowinto the motivation you all giveGoogle to build Cloud productsand services that developerslove    URS HOLZLE  And we lookforward to partnering with youto become the greatest techcompanies in your industries   APARNA SINHA  Remember to joinus at the next live developerQ A session  and tomorrow atCommunity Day Enjoy the rest of the show    Thanks everyone    Hi  everyone Thanks for joining us today forthe developer live Q A A reminder that this Q A islive If you want to join theconversation  reach out ontwitter with the  Google Cloudnext We ve reached out asking foryour questions on social mediaand you all have beenresponding We ll be answering yourquestions during the next 20minutes And first of all  let s welcomePrianca  who s here to answeryour questions with me heretoday Hi    Thanks  Aja That keynote was absolutelyamazing Yeah So let s get into the goodnessthat Urs and Aparna shared A highlight for me was reallythat upcoming documentary onKubernetes So cool I also enjoyed the Google Cloudinnovator communityannouncement  securing softwaresupply chain  and buildingsustainably What caught your eye    A lot of the stuff you justmentioned Definitely our innovations andsustainability The ability to see your carbonfootprint of your cloudworkloads is really  reallycool When I heard about it inyesterday s keynote  Iimmediately went and looked atall my personal projects to seethe carbon cost of what I vebeen doing  and I love havingthat kind of data to use indecision making  like help memake the right decisions But I m mostly really excitedabout the cloud innovatorsprogram I ve really missed interactingwith our Google Cloud Devcommunity I want to hear and see all theamazing things the community sbeen up to And the innovators programshould let us do that  and we reworking on some super specialinnovators only events in 2022that I can t wait to tell thecommunity about    Wow Yeah That all sounds really amazing I m so excited about that securesupply chain announcement  SLSA And how the combination of CloudBuild and binary authorizationactually helps kick start yourjourney to secure your softwareartifacts by fully automatingthat build process    Yeah  that s just sointeresting So cool So let s get to our firstquestion We ve got the questions comingin Just for the folks at home  backstage crew is looking for yourtweets as well  so keep themcoming in We ll be bringing them in live So our first question is fromAsher  and it is who would begood candidates for theparticipation in the privatepreview of eaEarth Engine Yeah So anybody who is  like  doingwork on sustainable things right So with the sustainability lens if you re thinking aboutfinancial services  the customerpackaged goods and their impacton the environment and how allof that can be combinedtogether So yeah  anything that you redoing with the sustainabilitylens Obviously it s in privatepreview  so you have to qualifyand stuff  but if you have thatangle of sustainability  you reprobably a right fit for it    Awesome Thank you for that Second question came in fromPriyanka s twitter  and I sawthis this morning as I wascoming in to do this Q A And this is any examples  do wehave any examples of end to endspark Plains for ML They d love to see them Specifically  they re interestedin learning how spark can beleveraged to work on big data onGoogle Cloud    Yeah So thank you for this question I see it came from Prana I saw my twitter this morning So really  what it s all aboutis the serverless Spark MLframework is about you nothaving to create your backendinfrastructure to Spark So that s all taken care of foryou so you can just run yourworkloads  which is  in thiscase  you re trying to run amachine learning Spark job  youcan just get started by notworrying about theinfrastructure So that s one part The other piece of it is theworkbench  which is the Jupiternotebook  but hosted  but italso gives you the opportunityto connect with data or theSpark ML jobs that you mighthave built  and run all of that So as a data scientist  I canjust use my workbench  and thatbecomes my home to kind of getthe data  massage it  connectwith data prog  spSpark ML jobsand then get all thosepredictions right in that onespot    Awesome Thanks  that was a reallycomprehensive answer So now we have a question fromChristopher and it s what arethe benefits of the innovatorsprogram Do you mind if I take this one    Yeah  please So the big benefit is going tobe access to innovators onlyevents We re going to be doing someAMA s we re planning right now road map meetings  potentially and there s also a backgroundthat you can download and use inyour Google Meet meetings Another question came in fromtwitter Awesome So let me read this one Massive focus on the security massive focus on security focuson the developer cloud Isn t this a topic that mattersmore for the managers ofdevelopers than for developsors And I ll take this one  too  ifthat s cool with you    Yeah    So I m a manager  and yeah that s a really good question Security is vital to all aspectsof software development andsecurity needs to be everyone sjob Yeah  managers need to care alot about it but we need to makethe tools so that the developerscan do the right thingautomatically  and that everyoneis participating in making ourcloud more secure And we talked about some ofthose things We talked about our tooling thatcan help you make sure that youput your secrets in securelocations as opposed to puttingthem in code We talked about the SLSA We talked about lots of otherparts  and all of this requiressecure software supply chain but security does need to beeveryone s job Managers can help by teachingtheir developers and enforcingit  but everyone needs to takethe steps to make things moresecure So  yes Good point But everyone s involved Okay This is a great one fromPriyanka s linked in Is it possible to connect to thepublic IP from a Cloud SQL to   I ve been asked this one before Also  is this cross projectset up  Cloud Run service andcloud run SQLs are on    can youdo a project with your databaseand your running of servicesomewhere else Priyanka    You actually can So in this scenario  you haveCloud SQL instance in oneproject  and you have your   you have your compute  orwhatever is calling that CloudSQL instance in another project and you could totally make themwork by using what is called asprivate service access What it does is it connects thetwo together  even with aprivate IP So you don t even have to exposea public IP for your cloud SQLinstance  which is  again  comesback to the security point youwere making earlier Everybody has to think aboutsecurity So you re not exposing withpublic IP  you re just using theprivate IP of your Cloud SQLinstance and connecting it tothe washing machine or whereveryou re running your compute tocall your search from So it s possible private serviceaccess is the service you relooking for to kind of connectthe two together I   Awesome Thank you so much  Priyanka Cool A question from Andy Ooh  this is one of myfavorites So there s a lot of ways to runan application with GoogleCloud It s a huge platform How do I know what I shouldchoose Where do I run my stuff Should I run it on GKE Compute Engine Cloud Run So many good choices    Yeah This is a question we get a lot right And it really just depends onyour situation and thesituations can be a lot  thetype of team  the size of team and the number of    the numberof developers you have  and thelanguages that you might beusing So there are lots of differentscenarios in which you candecide So I ll give an example of afew So  for example  compute engine Like  if you re migrating andyou just want to get fromon premise infrastructure intoCloud  and you just want thespeed  you just want to getthere  I would choose ComputeEngine to just migrate as is andthen modernize later  if needed Sometimes you don t need tomodernize  if you have licensingrequirements and stuff likethat So that s compute engine If you want to work withcontainers  need a little bitmore abstraction    lessabstraction  you can work withcontainers with GKE  and thatgives you a lot more controlover the number you have and theprocessing you re using But if you might be just wantingto run containers  but don twant to manage the underlyinginfrastructure  the nodes andstuff like that  and theregions  just use Cloud Runbecause it s serverless  but itallows you to use your container images and just deploy them Cloud functions is kind of likeeverywhere So you re trying to do    handleone function  or a piece of    apiece of a feature of code thatyou just deploy in that functionas a function  as a code sort ofservice  but it kind of applieseverywhere So I wouldn t say    so Cloudfunction is not like an or  it smore of an and Like it works with any of those it s just more of an extensionand enhancement of your serviceswith serverless So I hope that helped clarify alittle bit of that  but there sa lot that goes in thatdecision    Yeah  and I really liked howyou called out that cloudfunctions is not an or  it s anand Cloud functions is justfantastic It time pieces together That s one of the things I loveabout it And I m just going to point outfor folks that we have sessionson all of these in thebreakouts  so if you want to golearn more about these  go lookin our breakouts and you canfind out more about thedifferent offerings that GoogleCloud has What languages Do Cloud run andcloud function support Let s see if I can do this frommemory GCF These are    we ve got node GS python  Go  java  dot net  Rubyand PHP I got them all Awesome Cloud Run supports any languageor any library or any binariesthat you can put in thecontainer  but if you want touse the source code deploysfeature that Abby showedearlier  that is supported onnode  python  go  java and ddot net and specific versions ofthose languages are supported soplease do go to the Web site andmake sure that the version thatyou need is the one we support    Great memory  by the way    Oh  yeah  I m getting reallygood at naming all thoselanguages So from Selena  when should Iuse GKE Auto Pilot versus CloudRun    I ll let you take that one    You want to let me take thisone    Yeah    Okay    So this pretty much boilsdown to do you want Kubernetesor not If you want Kubernetes  if youwant the enhanced flexibilitythat Kubernetes has  if you wantto have all those knobs anddials that you can turn toreally fine tune everything foryour networking needs or yourparticular load profile  use GKEauto pilot If you have a container and youwant to run it on GCP  andthat s your goal  Cloud Run isgreat Cloud Run is fantastic at that And as we pointed out  you don teven need a container if you useCloud Run source deploys forthose raunings I languages I jumentioned    More questions Ooh  this one is for you This is from Caleb What file formats are supportedwith the Docu stuff    You can do images and PDF s It s really about theunstructured image data So PDF s and images    Awesome Okay Let s see what else we got Ooh This one s from Mark Another one for you  Priyanka Ooh  it s another securityquestion So the you tell us more aboutbinary authorization We covered it very briefly inthe keynote  but it s somethingI ve been hearing a lot aboutand I d love to know a littlemore about it    Yeah So again it kind of boils downto like the whole like securitynarrative that you mentionedthat everybody s kind ofresponsible for for the securityof the entire platform So in this case  with binaryoffice really deploy timesecurity So you re deploying and makingsure that your images orcontainer images  if you reusing GKE or Cloud Run workswith binary  so when you re atyour deployment stage  you canprovide signature authorizationson your images So if    and the verificationand the authorities for those So if they are authorized binary auth will apply theauthorizations and once theimage is authorized  only thenyou can deploy it    Awesome So I just got the signal thatwe re running out of time So this is going tounfortunately be the lastquequestion And this question comes fromWesley Can I use the build integrityfeatures with my on premsoftware    Hmmm Okay Yeah So you kind of can So with Cloud Build  it s reallyany container image  which isbuilt on cloud build You can use both    you can useit in both on prem or on GoogleCloud You just have to use the binaryattestation Cloud Build  andit s on the GitHub page  so youcan check that out But if you re building it with   in Cloud Build  you candeploy it on prem  or in GoogleCloud    Thanks  Priyanka Well  that was a lot of fun  andit was great to hear all thequestions from the audience Y all had some great ones And I want to say just a hugethank you to Priyanka forjoining us and answering so manyof those questions Be sure to join us back over onG co cloud next  as thespotlight will be kicking offshortly  and they have someamazing things that they regoing to show off Thanks for joining us  everyone    Hi  everyone I hope you enjoyed the Devkeynote I m Jeff Reed  VP of Product forApplication ModernizationPlatform at Google Cloud Thank you so much for being withus here today In most of my conversations withcustomers  I found they reaccelerating their technologiedoption through the use ofcloud based services to buildand deliver new capabilitieslike curbside pickup in retail remote diagnostics inmanufacturing  and completelynew experiences to bettersupport their end customers Today I m going to share someexciting announcements that willfurther empower you to digitallytransform your organization We recognize that each companyhas unique Cloud needs  so we atGoogle Cloud are focused onthree areas to support you inwhatever stage you are in yourCloud evolution First  we set out to make yourmigration or modernization patheasy With Google Cloud  you caneasily evolve your existingapplications or build new Cloudnative apps Second  our open platformextends Google Cloud s servicesin engineering practices tohybrid and multi cloudenvironments for consistentservice delivery And third  our planet scaleddistributed infrastructure istransformative and delivers thehighest level performance andavailability in a secure sustainable way One of our primary goals is tomake it easy for you to deployand scale and manage Kubernetesanywhere In the years since Googleinvented Kubernetes  containershave completely revolutionizedIT operations Given our history  it is notsurprising that GoogleKubernetes Engine  GKE  is theleading solution in the market and even Gartner agrees In the recent Gartner Solutionscore card  GKE scored 92 out of100  making it the absolutestrongest strategic option amongpublic Cloud Kubernetesservices Until now  Kubernetes hasinvolved a fair amount of manualconfiguration You have to manage your ownClusters  nodes  YAML files it s a lot With our introduction of GKEautopilot  a new mode ofoperation in GKE  we re makingit much easier for you to useKubernetes Google provisions and managesthe entire clusters underlyinginfrastructure  including thecontrol plane  node pools worker nodes  letting you andyour developers focus on yoursoftware while GKE autopilotmanages all aspects of theinfrastructure In the midst of the pandemic  wesaw a large number of ourcustomers adopting serverlesstechnologies  and that s nosurprise since serverlesstechnologies enables companiesto rapidly develop and deployany application in a fullyautomated environment With services like CloudFunctions  Eventarc andWorkflows  you can easily set upevent driven  serverless workflows that connect in to GoogleCloud  third party SAS servicesor your own applications Serverless is also about runningcomplex workloads at scale whilestill preserving a delightfuldeveloper experience In fact  serverless with CloudRun is about delivering a truedeveloper platform with theflexibility to run any language any library  any binary You can bring traditionalworkloads such as Java SpringBoot  ASP net  and more toServerless Compute now Whether it s GKE AutoPilot Cloud Run or Cloud functions our goal is to make it easy foryou to build and scale apps howyou want and where you want Our second focused area isdelivering an openinfrastructure Cloud It relies on open sourced basedtechnology  like Kubernetes  SGOand K native  delivering theportability you expect It also offers you the choiceand flexibility to build themanager apps across multipleclouds To realize these benefits Anthos operates as acloud backed control plane thatprovides consistent developmentand management at scale acrossboth Edge  on premise  andmulti cloud environments It then enables you to build andmanage global fleets andestablish operationalconsistency at scale Let s hear from my colleague Rae Wang  who interviewedJahidul Khandaker and Suraj Rao the CIO and global head ofadvanced analytics at WesternDigital on their multi cloudjourney    Welcome And thank you for sharing yourinsights with our audience We have been working togethersince 2019 on Western Digital sCloud transformation What initially prompted you tofocus on modernizingapplications and standardizingsoftware delivery across yourorganization    Thank you  Rae We are happy to be here As a result of various mergerand acquisitions  WGC became anintegration of threecorporations  HESD  WDC andSandisk To support very diverseinfrastructure and ITenvironments across the threeentities  switching to a Cloudstrategy became an imperativefor driving to astandardization    Now  Jahidul mentioned aboutthe cloud strategy In 2018  we saw tremendousgrowth in IOT applications fromour global factories thatrequired low latency andhigh speed  on premisesolutions However  now we were faced withthis daunting task of keepingour on premise and the Cloudsolutions synced up This led to exploding solutionsthat provide a uniformmanagement plane across ourhybrid environment After trying multiple solutionsfor over two years  we choseAnthos Anthos gives us the diagnosticsolution that works across GCPand on premise environmentswhile keeping the doors open fora multi cloud future    That is great to hear Go  Anthos So what does hybrid Cloudstrategy mean to WesternDigital    Sure  Rae Western Digital is making apivotal strategy shift to Anthosfor our big data platform We are migrating more than 25business critical applicationsseamlessly to this hybridenvironment with Anthos This move has severaladvantages  a richer userexperience  greater security andenhanced flexibility to managefactory applications Some of these criticalapplications include imageanalysis on millions of images aweek for factory disposition Machine learning will close forreal time factory decisions  andmany  many others    Thank you That s a great strategy and someamazing use cases Now looking forward  what areyour upcoming digitaltransformation goals in the nextthree to five years for WesternDigital    Rae  the future at WesternDigital is very exciting We want to deliver excellenceeverywhere Ultimately  we see Cloudtechnology as an enabler of ourkey business priorities  reducedtime to deliver services rationalize our applicationfootprints  and meet customerdemand for IOT and edgeapplications    Thank you  Jahidul and Siraj I look forward to amazing workfrom this great partnership    Thanks  Jahidul  Siraj andRae Since we announced Anthos backin 2019  we are thrilled withthe reception it has received inthe market In fact  as of Q2 2021  Anthos compute undermanagement grewmore than 500 percent year overyear And today  we are extendingAnthos towards even moreworkloads and more environmentsand in more locations We are announcing Anthos for VMsto support development teamsthat want to standardize inKubernetes  but have existingworkloads running on virtualmachines that cannot be easilycontainerized Once you shift or attach VMsdirectly to the Anthosenvironment  you can leveragedeclarative configuration andpolicy management with Anthosconfig management  andend to end applicationvisibility and security withAnthos service mesh We re also introducing one of myfavorites  Anthos MulticloudAPI  which enables you toprovision and manage GKEclusters running on AWS andAzure infrastructure through acentralized Google backedcontrol play Generally available today The Anthos Multicloud APIensures your team has aconsistent experience to create manage and update GKE clusters regardless of which major publiccloud you re using Thank you for joining us on thepath to revolutionizing Cloudcomputing Now I m going to hand it over toSachin Gupta  VP GM forInfrastructure and long timefriend  to talk about how we reextending these innovations toyour dedicated environments    Thanks  Jeff    Great to see you  friend    It s great to be here inperson and it s great to see youall Our goal in Google Cloud is tomeet you where you are in yourdigital transformation We understand some of yourworkloads cannot move to thepublic Cloud entirely due tovarious factors  such as highamounts of local dataprocessing  low latencyrequirements  or strict datasecurity and privacyrequirements But as you ve heard yesterdayfrom Thomas  we re working tohelp you solve some of theseconstraints With the announcement of GoogleDistributed Cloud  we reextending our infrastructure tothe edge and to your own datacenters This announcement allows you tofurther digitize your businessapplications by ensuring theyhave the speed  intelligence andprocessing power in managedheterogenous environments  andwe re approaching this from adifferentiated standpointrelative to other cloudproviders First  we bring Google s AI andAnalytics Solutions closer towhere the data is beinggenerated and consumed toharness real time insights Second  Google Distributed Cloudis enabled by Anthos It helps you to build and runapplications on GKE clusters andvirtual machines anywhere with aCloud backed control plane forconsistent management at scale And third  our Planet ScaleInfrastructure delivers thehighest level of performance andavailability on the most secureand sustainable platform Google Distributed Cloud is afully managed  integratedhardware and software solution meaning you don t have to worryabout the underlyinginfrastructure and can focus onyour applications and businessinitiatives We aim to simplify operations leveraging Google s expertiseand track record in areas likescale deployment  fleetmanagement and site reliabilityengineering This allows you to focus on yourbusiness priorities and leavethe complexities to us Google Distributed Cloud isdesigned for running sensitiveworkloads that meet sovereigntyrequirements and offers private5G LTE solutions for enterprisecustomers There are four deploymentscenarios depending on thecustomer s need   The Googlenetwork edge  the operator edge the customer edge and customerdata centers RThe first use case I ll talkthrough is at Google s networkedge  which is designed forsingle and multi tenant usecases  leveraging over 140Google network edge locationsworldwide Next is the operator edge This is owned by communicationservice providers for bothsingle and multi tenant Clouduse cases As I mentioned before  you llbenefit from 5G LTE servicesprovided by our operatorpartners It can accommodate emergingservices and applications withstringent latency andreliability requirements For example  online games andgame streaming depend on lowlatency to preserve the end userexperience Then we have the enterprisecustomer edge These are customer owned edgelocations such as retail stores factory floors or branchoffices  which require localizedcompute and processing directlyin these edge locations Next  the customer data centersare customer owned facilities orco lo facilities and are set upfor single and multi tenanthybrid scenarios It s also ideal for life cyclemanagement of virtual networkfunctions for communicationservice providers that reside onpremises  such as Cloud nativebuildout of private 5G networks Google Distributed Cloud alsoincludes a hosted mode to runsensitive workloads Hosted mode helps you meetsovereignty needs by addressingdata residency with strictsecurity and privacyrequirements  all whileproviding you with a way tomodernize on premisedeployments Customers can manage thisdirectly or host through adesignated and trusted partner The good news is that hostedmode does not requireconnectivity to Google Cloud atany time to manageinfrastructure  and uses a localcontrol plane for operations Upgrades in patches are offeredby Google  and verified by thetrusted partner To learn more about howcustomers are leveragingGoogle s Distributed Cloud  I dlike to welcome Rasesh Patel chief product and platformofficer  AT T Rasesh  welcome to Next    Thanks  Sachin  it s so goodto be here with you today    Rasesh  why don t we digright in and have you tell usabout how you re looking toleverage Google s distributedcloud for your edge andcomputing needs    You bet I d start with saying AT T andGoogle have similar goals whenit comes to edge compute We both want our businesscustomers to build and runmodern applications close totheir end users By moving compute workloadscloser to the user  we canreduce latency to levels thatwill allow for a whole new rangeof mobile experiences thatweren t possible before Sometimes this compute will beon the network edge Sometimes it will be at thecustomer premise But regardless of where thecompute workloads occur  this isnot something AT T s going to doalone    That is tremendous It is really fascinating howcommunications has been able toinfluence business revolution and it looks like you re poisedto do it all over again So what kind of new businessoutcomes do you think we canbring together with 5G and edge    You re absolutely right Sachin Our goal is to create net newbusiness services and customerexperiences Let me give you some industryexamples In retail  services includingstreamlining automated inventorymanagement  predicting andmanaging queues  even enablingcashierless checkout optionswill come to life In healthcare  we see secure multi gig connectivity for alldevices within a hospital  theadvent of remote patientdiagnostics and care  and rapiddata transfer between field baseemergency medical services likean ambulance  and hospitals And in the entertainmentindustry  we re enhancingin venue experiences forconcerts and sporting eventswith solutions ranging fromimmersive AR and VR experiences smart parking  ticketless entryto contactless food and souvenirpayment    Last question What are your goals for AT T andGoogle partnership in the nextthree to five years    Well  our work togetherbrings market transformativecapabilities to businessesacross many industries 5G and fiber based edgeconnectivity and compute withGoogle s powerful ecosystem thatincludes maps  voicerecognition  AI  Android andmany other capabilities  enablesthe development of these nextgen experiences in anaccelerated time to market And we re looking forward to alot of expansion We re bringing our network basedsolution with Google Cloud toover 15 major markets in thenext several years We have plans to roll out theservices in major metro markets including Chicago  Atlanta Dallas  Miami  San Francisco andmany more So stay tuned    Thank you for joining us Rasesh  and we look forward tocontinued success and deliveringjoint value to our mutualcustomers Thank you again  Rasesh    Thanks for having me    Everything you just heardabout Google Distributed Cloudis made possible by Google splanet scale infrastructure To ensure you re successful  ourinfrastructure delivers severalkey differentiated benefits including a global networkconstruct  performant andcustomizable compute services and reliable and secure storage First  at the core of thisinfrastructure is the world slargest and lowest latencynetwork with 27 regions  82zones and 146 points of presencelocated in more than 200countries all interconnectedwith 16 subsea cables It enables companies like majorLeague Baseball  Wayfair and1 800 flowers to quickly migrateexisting enterprise workloads toGoogle Cloud and will continueto invest in our global networkand global reach This year alone  we have alreadyadded four new cloud regions Warsaw  Delhi  Melbourne  andToronto Moreover  with simplernetworking solutions such asNetwork Connectivity Center  youcan easily connect ST WANs VPN s and interconnect with acentralized management model andmonitor the network with networkintelligence center And with private serviceconnect  you can connect at theservice layer withoutconfiguring the underlyingnetwork Second is our performant andcustomizable compute platform Compute engine allows yourapplications to achieve higherreliability  security and scalewithout any of the operationaltoil We recently announced a new VMfamily  Tau VMs  which isoptimized for scale out  digitalnative workloads Tau VMs offer 42 percent higherprice performance thanalternatives from any otherleading cloud provider As of today  I m also excited toannounce the preview release ofnew Spot VMs With Spot VMs  you can useexcess compute capacity atdeeply discounted rates  and ournew Spot VMs offer bettersavings and more predictablepricing than alternatives fromany other leading Cloud Third  our business businesscontinuity and storage options Our strategy is about tenderingyour storage to your workload sowe can meet your price andperformance needs To advance our storage as thebest option for globalenterprises  we recentlyintroduced three important newservices The first is backup for GKE which is an easy  Cloud nativeway for customers to protecttheir configuration and datarunning in their containers The second is filestoreenterprise  a Cloud Nativemanaged NFS storage solutionthat offers 99 99 percentavailability SLA  and is idealfor running enterpriseapplications such as SAP andGCP And the third is additionalrobust business continuityfeatures for cloud storage thatextends the unique  single namespace model we have with dualregion buckets You now get more choice in whereyour data is stored with customdual region buckets  and theoption of a new market leading 15 minute RPO SLA We re passionate about helpingcustomers continually evolvetheir approach to modernization and bring more of theirapplications and data to theCloud Google Cloud is planet scale available wherever you need it while also providing cuttingedge innovations in performanceand security Our goal is to make your journeyto Cloud easy by offeringtransformative capabilities tohelp you innovate faster andsave money through an openapproach that enablesflexibility and choice Next  I m going to turn it overto the demo team to bring themagic of Google Cloud to life So stick around  and after thedemo  we ll be taking yourquestions in a live Q A See you then    Hey Welcome  everyone I m Richard    Hi I m Vitia    We re really glad you joinedus here today We re going to have a lot of funtalking tech  and I wish youcould be here with uspersonally  but we re live  atleast  and it means we can kindof say anything  I guess I couldsay Google should bring backGoogle Reader  and I m stillworking here  so that s awesome If you have some other hot takesand want to share some feedback put it in the chat We can see that live So keep it coming    If you can t see the liveinteractity  please click on theblue join live interactiveexperience button on the Website and we ll see you on theother side From those of you just joiningus from the spotlight session you heard about some of our newproduct announcements  includingAnthos for VMs  Anthosmulti cloud API and the Googledistributed cloud    Awesome In the next 15 minutes  we reactually going to show you allof these in action  which isgreat So here s what we re going to dotogether First  we re going to migratesomething and demonstrate how toget some new value from an oldsystem by moving VM basedapplications to a newercontainerized platform And we ll start by showing youhow to run a legacy app in aserverless platform So buckle up    Next  we ll create We ll build a new experience forcustomers through our AI poweredsoftware  and then we ll deploywith some brand new deploymenttools unique to Google Cloud    Love it Finally  we re going to beexpanding our deploymenttargets We re not just shipping to cloudanymore  but extending thatawesome path to production toother public clouds  and evencloser to your customers at theedge All right So let s bring some things up shall we And now work at Cymbal shops So let s pretend that I am thedirector of IT at this company Cymbal shops  and like many ofyou  our global business hasbeen shifted because ofincreasing demands online  andat the same time we have someproblems  so old software wasnever meant for all the customerloads we re putting on it now It s holding us back We re trying to offer mobileexperiences  real time stuff  wehave to do it as quick as wecan  and look  I can t rewriteeverything So how do we reduce some costsand add some capabilities tothat existing software  a littleeasier  without too much effort So I want to start with a smartmigration approach  you know  Iwant a repeatable  efficient wayto move old systems to newerstuff  maybe even a serverlessone But Cymbalshop needs help fromour friends at Google Cloud Help  you re my only hope    I got you  Richard So our fit assessment tool which you can see here  isinitiated from the Google Cloudconsole It helps us understand what canbe automatically migrated andwhich workloads are going to bemost successful in their newhome It s unique to Google Cloud andsaves you time By pointing our migrate serviceat a set of over 2 000 virtualmachines in our on premenvironment  we just generated afit assessment for the workloadsrunning in each of those VMs And remember  these VMs could berunning on prem  in GoogleCloud  in AWS or in Azure So now  let s take a look at theresult of the assessment See here  we have a graphicalview of our most likelycandidates for migration This tool is particularly goodat detecting compatibility forolder java applications likejava  EE application servers Now  that we ve assessed  let smigmigrate Using our migrate service  I mtaking a java app  running onWeb sphere in an on prem VM  andgenerating a container image That can run on any of ourcontainer services  all viaautomation And  you can see here that theclassic java app is now servedup by our only pay for what youuse serverless containerproduct  Cloud Run    Wow I love that So this is great I want to make sure we know whatwe just did here I mean  I don t have to manageall of these partially utilizedvirtual machines anymore becauseyou containerized it  and I veoffloaded some of themanagement That s awesome And you ve given me access tobrand new functionality andsaved me money by running in amodern platform like Cloud Run so you just took a classic javaapp and ran that on a serverlessplatform That s pretty wild stuff If you like that  tell us in thechat that you have a need forputting some of your oldersystems in these newerplatforms All right So next up  let s create somenew value and get that to GoogleCloud So SCymbal shop has some problesthat we have to solve It needs some seriousmodernization to support all ofthose sort of new customerexperiences and keep uscompetitive So this means upgradingfunctionality and even changinghow we could deliver it toproduction So curbside pickup  super hotright now How do I add functionality thento count how many customers didcurbside pickup each day That seems really important Can you help me with this one    Yes  of course  I can So Richard  what does yourcurrent app consist of    Well  thank you for asking Our current app has a MongoDBdatabase with multiple servicesthat track orders and pickups We actually want to add newservices and data to capture andanalyze that curbside pickupinfo  and to make that happen we could look at historicalfootage from the past day andcount the pickups and do someanalysis later    This looks like a greatopportunity to do two things First  we re going to add GoogleCloud AI to process videofootage and capture metrics Second  we ll containerize theseworkloads and put them on acontinuous delivery pipeline sothat it s easy to keep makingchanges    I like that So if you do this  then weactually know how many customersare doing curbside pickup  maybeeven how long they were waiting and I think you re going to fixsome of my path to prod and makeit easy to kind of package andkeep changing the software overand over again I love that    Yes Absolutely Check it out So Google Cloud vision API candetect vehicles in this footageto determine how well curbsidepickups have been going at eachlocation So now  let s write some code and deploy the updated app toGoogle Cloud This is exciting  and I m hopingall our audience is excited too I m using visual studio codehere  but I could easily useintellij or any IDE And using the no cost cloud codeplug in  I can easily browseGoogle Cloud services  add themto my app  and then code andtest this container based app    You are the fastest coderI ve ever seen That was remarkable So that was pretty simple  too I like that So now that everything isworking locally  how would Ithen deploy this to the cloud    Let me show you First  we need to package upthis app into a container You probably want yourdevelopers spending time writingcode  not docker files  and wecan help with that I ll show you how we like topackage containers  and whileI m doing that audience  tell ushow do you package yourcontainers So here is our poll  and we havefour options in our poll  so I mhoping you would actually pickit up    Awesome    So Cloud Build is aserverless build tool that manycustomers use for continuousintegration We also now support industrystandard cloud build backs topackage up our app into acontainer image automaticablyand send it to artifact reregistry It stores your container imagesin regional repositories andhere you see that weautomatically scan forvulnerabilities    That s neat What else do we use artifactregistry for I haven t seen as much aboutthat Can you tell me a little bitabout that    Yeah So while we use artifactregistry for storing dockercontainer images  Richard  wecan also use it for storing allyour language specific artifactsin one place For instance  we just went GAwith java  no  js    That s pretty cool So while we re waiting for thepoll results  looking at some ofthe chat  I saw some questionsabout how do we do day 2management of migratedworkloads I think the migrate toolinggives you a container image thatnow I could download to my desktop  I can run in differentplaces I have different ways to runthat Some other questions about again  thinking about how do wemigrate to GKE and Cloud Run I think it s pretty cool thatthey can take an app and run itin either one  which I think isgreat  and we re seeing the pollresults come in    Yeah Looks like the results are in And it s amazing that a lot offolks are already automatingtoday and they are automatingvia docker build commands in CIpipeline That s excellent So now that we have a containerin the registry  I think it stime to deploy it So to deploy our app  let s usethe new Google Cloud deployservice  a continuous deliveryservice for deployingcontainerized workloads While the service iscontainerized and could run inGKE or Cloud Run  the app isfairly coupled to our MongoDBinfrastructure running inKubernetes and is part of ourmulti cloud strategy So let s target one of our GKEauto pilot clusters GKE auto pilot is the fullymanaged service where GoogleCloud provisions  scales upgrades and troubleshoots thecluster for me Here we see a deploymentpipeline that helps us managerelease candidates andenvironments Cloud deploy helps us managepromotion and rollout acrossthese environments  and once theapp is deployed to GKE autopilot  you can start using itfrom each retail store that youhhave You kick this all off and alsoyou manually approve finalpromotion to production In fact  I m going to havesomeone drop the URL in the chatwindow now so that you can seeit for yourself It all works just like magic    That s cool That s great So it s awesome So from development to packagingto deployment  I thinkpersonally this is the best setof integrated tools for servicesand building containers thatI ve seen  so I think that sawesome So we re also seeing some otherfolks in the chat talking aboutsome of these components as welland what they re seeing formigrating the apps  andhopefully clicking links andtrying to break the app we justdeployed All right So now that we ve just takenthat existing app  we added AIfunctionality and wedramatically changed theknowledge that each of ourstores has about the customerexperience That s pretty cool    Indeed  isn t it  Richard What else do you want to throwmy way    I think we got some time So we ve acquired a few otherretail chains who use differentclouds I mean  nobody s perfect  right So at the same time  this appneeds to run in more places sothat each of those stores cananalyze curbside pickupbehavior  regardless of whatcloud they re using  so how do Irun this app everywhere    Yeah  that s a greatquestion  Richard  and we dohave a solution for consistencyacross any environment Check out how we do it throughAnthos With the new multi cloud API  wecan provision these clustersright from the Google Cloud CLIor console in other clouds including AWS and Azure  whereyou already had someapplications installed See here that I m using a singleG cloud command to create a GKEcluster on Azure Here  we ve deployed GKEclusters to Google Cloud  andMicrosoft azAzure I m managing it all from GoogleCloud  and I m even able tocentrally deploy and viewworkloads to any of these GKEclusters    That s wild Thanks So you re actually putting GKE I think it s the best Kubernetesin the public cloud anywhere Iwanted  and that s reallypowerful stuff I love that new Azure support And again  each store manager whatever cloud they re using canactually see the support at theend of the day to know how theyperformed on curbside pickup That was the goal All right So now you re making me thinkthat we could probably do more We could expand some of ourthinking here and maybe respondto the customers  demands inreal time Could we evolve from thatanalysis of parking lot footageto maybe improve the experiencein real time By that I m thinking  can I runthis AI model against livecamera footage instead ofrecorded stuff and maybe be ableto do something with it as ithappens But of course  as always  newchallenges emerge  if we thinkof something like this  so offthe top of my head  we d have tosomehow customize this AI model right Because now I have to identifythe number of cars in motion how many are waiting  is thecurbside lot picking up  and Iprobably want to operationalizethat model and move it closer tothe store  because I mprocessing data in real time  solatency matters And then I want to integrate allthat insight I m getting withthe existing in store systemsfor the managers to use so theycan move their employees around So that s a lot of stuff I don t know Can you help me with that    We can So I m calling in my colleague GabrGabrielle He has been working with one ofyour on prem locations  Richard in Austin  and he can explainhow we improve your software foredge scenarios Hi  Gabrielle    Hi Hey  everyone Glad to join you  Richard By the way  congrats on your newrole What you re asking for can besummed up with Googledistributed cloud and the edge It builds the best of GoogleCloud to the edge It consists of a fully managedCPU  GPU optimized platform witha common set of Google interpretapplications It can be deployed anywhere As an example here  at some ofyour stores So let s get going We re going to add real timeintelligence with AI models  andprocess locally to reduce videolatency Let s get this running on theedge in each store So we start out by first usingGoogle Vertex AI  so we trainthe engines and the visual AIobjects  recent specificationmodels in the central cloud Here  we are monitoring theprogress and accuracy of themomodel  and as you can see  thetrained models cannot    thosecontainer images can be ready tobe deployed anywhere  includingin the edge In this case  we ll be targetingfive different Cymbal stores From the Google console  we canchoose the Kubernetes clustersrun specific edge locationsacross all the single stores Once we ve picked thoseclusters  we can deploy thevisual AI model we created toeach edge in the store  andmanage those edge clusters justlike any other GKE clusters The Cymbal operations team hasthe same familiar experience only now  Cymbal can alsoleverage the GPU optimized tocloud to achieve betterperformance and lower latency And as a fully managed service we see that Cloud comes with thesame integrated Google Cloudoperation and managementcapabilities that you re usedto Here  you know  Cymbal canmonitor the health and state oftheir store deployments  managecapacity scale  all of theseusing a familiar Google consoleand backed by Google SREpractices So as you can see in the videohere  a real time analysis ofcurbside service level from thelive footage led to real timeinsight for single shops tobuild a better customerexperience at each store Specific provision model in theedge is recognizing cars motion to recognize when the curbsidespots are filling up  orcontinual time So what do you think  Richard Back to you    This is great You re amazing  Gabrielle That s    I can t believe youbuilt all that all by yourself So this actually gives all ourstore managers really newinsight into the real timecustomer experience  lettingthose stores allocate staff andpeople based on real timecustomer demand That s awesome So for the audience out there I m actually interested  whatsort of apps do you think aboutrunning at the edge Is that a real use case you reconsidering What sort of things might runthere All right So I do love what you built forme I don t want to be  you know greedy  but I want a littlemore This app isn t an island  right You re doing some cool AIcontainerized based stuff  butthere s a lot of things alreadyat that store location  right Our notification services oryback office  it s all in VM s So now that we have to integrateour new AI customer service app these VM based systems  am Isigning up for completelydifferent management experiencesacross containers and virtualmachines  or can you dosomething for me    Oh  absolutely not We re going to provide you thesame experience  with the newAnthos for VMs We can actually help you bringthose virtual machines into theAnthos platform and manage themthe same way you managecontainers So see here  I ve moved severalvirtual machines under Anthosmanagement  which now gives me astraightforward way to move andmodernize existing apps at theedge    Wow That s awesome There s nothing you can t do GrGabrielle  that s great So what s powerful here is thatall the Google powered fleetmanagement in Google Cloud other clouds  at the edge  isall based on Anthos with oneopen control plane for whereverthose workloads run This simplifies our operations alot That open foundation makeshiring developers a heck of alot easier So last challenge for you Cymbal has this growing Europeanpresence  but they operate onrestrictions on datasovereignty I don t want to sacrifice allthe amazing capabilities thatyou ve showed me today  or isthis a lost cause  or do youhave something for me    Yes  we have something foryou As we announced yesterday within Google Google Cloud  wecan use a single hardware andsoftware stack from GoogleCloud  but now local compute and in local control plane It runs not only our containerand VM workloads  but also thedata and application services wecare about We could run these as an examplein the regional holster  fullyair gapped and have local storesconnect directly to that One open modern platform fromwherever you want to operate    I m sold You ve done it So it s pretty cool So same Anthos control planetech  but also local services inthis fully air gap setting That s great stuff Thanks  Gabrielle Appreciate you joining us here Did you like that    I did  actually And Gabrielle  that was amazing and I m sure audience thinks soas well Over the last 15 minutes  we veshown you how Google Cloudoffers a world class experiencefor building new software ormodernizing what you have Our solutions forcontainer based apps are secondto none  and they re now makingit possible to extend thoseterrific services to whereveryou need us to be    All right Thank you all for joining us This was terrific stuff And thank you out there for thechat and the engagement It was awesome to see that So now  stay tuned for the liveQ A coming up next We re going to talk abouteverything from the spotlightall the way through to thisdemo Get your questions in there  andwe ll be answering it live Thanks so much Bye bye    Bye    All right Hi  it s me again In case you didn t tune in forthat live demo that just wrappedup  I m Richard    I m Sachin Richard  it s nice to see you inyour new role    Yeah  thanks for hiring meback  I m glad to be back So our friends from the livedemo are going to be stickingaround to answer any of thequestions you may have  so youmay see their faces throughoutthe show as well  so let s jumpin with a little poll What gets you fired up abouttech  using serverlessplatforms  running GKE anywhere How about continuously buildingsoftware  or heck  even justhaving breakfast for dinner Throw your results in there I d love to see what you have tosay So while we wait for that  are areminder  this Q A is completelylive You can engage with us directly ask your questions  say hi inthe chat  yell at us  certainlySachin  not me  I m doing mybest If you can t  though  go back tothat Web site  and we ll waitfor you over here All right Now time for some poll results Let s see what we have Anything coming so far  let ssee what fires people up What fires you up  Sachin    Well  besides breakfast fordinner  it s all about Googledistributed cloud The ability to have thatconsistent management runninganywhere  meekt our customerswhere they re at  fully managedsolutions  it s just absolutelyincredible  and with Gabrielle that was just fantastic    Yeah  it s a big deal Well  looking at the results sofar  you all are just animals Eating breakfast for dinner So good job    I was right    Yeah Live your life That s great So let s get started with ourfirst question This one comes from Jessica So when would I use Anthos onbare metal infrastructure versusthat Google distributed cloud atedge  how do I make the decisionbetween the two    Yeah  that s a greatquestion Thanks for that question So first of all  let me just goback to that Cymbal shops retailexample If you re a customer  you haveyour own hardware  it scustomized for that environment perhaps  and you re just lookingfor a stack on top to run Anthos bare metal is a greatoption But if you re a customer whereyou just want to consume thewhole stack  hardware  software as a service  and you want toleave all that complexity to us then the Google distributedcloud product is a greatsolution for that and so we wantto be very flexible about it You get to choose based on whatyour needs are    It s nice to have thesoftware based option or thesoftware and hardware    Exactly Exactly Because  you know  different      The same control plane    But the same control plane same experience Deploy    write ones  buildones  deploy anywhere    Yeah  that s a big deal So this one is going to be foryou  Sachin  as well So this one comes from Mike Wl  you know  Val is asking thisquestion  let s switch questionsup I ll give you a breather I wore you out on that one So are there other managedservices that people might wantto adopt as they become morecloud native When did they start picking upas they start making this moveto cloud    Yeah AbsoAbsolutely  Richard So as you deploy moremicro services  you can easilyleverage our fully managedserverless cork strationproducts like work flows orevent to arc so that you canconnect as well as coordinateGoogle Cloud services such ascloud functions  Cloud Run  HTTPbased  API services  includingthird party services so that youcan all tie them all up in toserverless work flows  and thisreally helps you address thosetype of use cases which requirebusiness critical  missioncritical work flows so that youcan actually have in built errorhandling  retries  observabilitybuilt in so that everythingactually executes reliably andflawlessly    Love it Thank you Good Next one  this one is for you Sachin Buckle up here So Robert from Liberty Mutual isasking us about kind of the OSsupport for Anthos So when I think about  you know we know Anthos spoerpts windowssxn Linux  which is great  sohe s curious about where are weat with some of the operatingsystems    This is consistent with theAnthos that you have from asoftware only point of view  butwith Google distributed cloud we just carried it forward We re certifying that solutionon top of hardware that theyprescribe And I think there was a questionthat I was seeing in chat  whichis about  hey  you know  whatkind of hardware is certified So we ve gone in  we havehardware from Dell  from HPE we ve got GPU s  and so there sdifferent hardware for differentscenarios  and then on top ofthat  whatever Anthos provides we can bring to bear    You can run    we support allof those as well  so we try togive you a good set of choicesthere Awesome Let s do it So are you seeing any customerpreference so far for GKE autopilot versus GKE standard GKE standard is amazing  it sgot all the knobs and dials Auto pilots just turn key Ready to go Are you seeing any change inpreferences    First of all  we ve seenmassive adoption of GKE autopilot  so it s really opened thedoor  I think  for many  manycustomers who wanted to get thebenefit of Kubernetes andcontainers  but wanted it to besort of automatically set up forthem But you still have customers whodon t just want that KubernetesAPI They actually want to have anexisting cluster or to have anenvironment that s fine tunedfor their needs  very  verylarge scale deployment where GKEworks great And so I think with thecombination of GKE and GKE autopilot  we can now cater to anyof those needs    Yeah  I guess we ve actuallyseen a lot of auto pilotcustomers or just new GoogleCloud customers as well A great first foray in toKubernetes    Exactly Exactly They can get going so quickly I mean  again  the demos justnow were fantastic    Yeah  even I can use it Great stuff More stuff  Sachin How do I choose which containeroriented compute service to use Now  we don t necessarily havethe 17 ways that other cloudsoffer We have maybe fewer  morefocused services  but there aremore choices  not just one wayto run a container  so howshould one make a decisionthere    We do keep it simple  butthere are some choices If you have something that scompute heavy  where it s a lotof self managed software thatyou have  self managed servicesthat you re creating  it couldbe that you have your owndatadatabases  it could be a middleware messaging That s where we think GKE isbest But if you re a consumer of alot of the managed services andyou ve got some adjacent codethat complements those services then Cloud Run is fantastic And so not the 17 locuses likeyou mentioned  but there are fewchoices based on how you reusing that containerizedenvironment    Yeah That s a crisp way of thinkingabout it Good stuff Spencer is asking us  you know Anthos for VM s is interesting We announced that  and thatseems wild We ve just been talkingcontainers at this time Now we re talking VM So what in the world is the usecase for that Why in the world would I thinkabout having Anthos managevirtual machines    Once again  great question We ve been very clear aboutthis We want to meet customers wherethey re at  and while customersare looking to modernize  andwant to be on GKE  onKubernetes  containerized  theydo have some softwarecomponents  some applicationsthat require the environment And so the ability to have thatone consistent management plane and have the sort of loweredcost running have M s or containers on top ofKubernetes  that s a verypowerful combination So Anthos for VMs is just whilethey re looking to mentorize whatever pieces of software theyhave that requires software they can bring it alongconsistently as they do thetransition    It seems powerful to getmaybe the operational toolchain  just treat the VMcontainers the same way  thatway they can simplify managementand hiring and those sorts ofthings    Yeah Exactly If they re going to be therenext to the containers  that sgreat  but that consistentmanagement reduces theoperational cost    This one is going to go toVidia  so Alex is asking usabout Cloud Run  and look thisis something that we talk a lotabout We love Cloud Run Cloud Run is terrific So what s new and interestingthere What s been happening in theCloud Run world    Yeah Thanks  Richard So here are some new featuresthat might be interesting Number one  to further secureyour Cloud Run environment  wejust spent GA with binaryauthorizations so that you candeploy your trusted containerimages Number two  you can now run evenmore workloads  for instanceworkloads that may needbackground processing  and weoffered additional always on CPUallocation controls  which comeswith new pricing There are no requests fees memory and CPU s about 25percent lower priced The third feature that might beinteresting to call out is thatwhile Cloud Run free tiercontinues to exist  we also nowrecently introduced committeduse discounts so that you canactually get up to 17 percentdiscount for over a one yearcommitment    That s great Awesome Thanks  Vidia Yeah I think some of that committeduse stuff is terrific Same with the security features I mean  Cloud Run is one of thebest security oriented justcontainer run times out there in addition with GKE Another question  this one isfor you again  Sachin I m glad you re ready Vladimir is asking us  what isthe benefit of doing Anthos edgeversus Anthos on VMware And I know those are evendifferent to start with as wethink about the question  couldyou do on VMware at the edge You know  how do you think aboutthen our distributed cloud edge I guess kind of help demystifythis    If I think about thequestion  it feels a little bitsimilar to the previousquestion  which is what are youactually trying to do If you re actually trying tomodernize  then the Anthosenvironment  where we providethat simple management plane built on Kubernetes  where  youknow  you can run containerizedapplications  but you can alsorun VMs now  that s a you re ona path to modernization That s what you re looking for You re looking for operationalcost reduction  simplification but you want to be into thissort of new world in thecontainerized world  that swhere    but there are going tobe other options on prem thatyou can run And then similar to  you know perhaps other solutions that maybe out there where it s not justa software layer  you canpackage the hardware and thesoftware completely together that s where Google distributedcloud  that s built on Anthos comes together So are you looking to modernize Is it just a pure legacyenvironment that that s how it sgoing to remain If you re looking for modernizeand you need the flexibility ofhaving MV s as well  Anthos andAnthos for VMs is perfect    And obviously you can runright on top of that andeverything is amazing so again Ilike the flexibility that we llwork with what you have    And if you want to run thatVM footprint inside our GoogleCloud regions  for that we veGoogle Cloud VM region  and wesupport that And so we re very flexible basedon what customers are trying toachieve    Yeah Awesome Another one for you  Sachin This one I think is interesting So I thought Anthos was alreadymulti cloud What are we talking about with amulti cloud API Weren t we already doing that Or what s new with a multi cloudAPI    Yes Look  Anthos has beenmulti cloud for over a year now right So the multi cloud API is reallyabout instead of having to bringup your clusters  your GKEclusters  the control plane aswell in every single cloud  youcan now just run that controlplane in Google Cloud  and thenmanage those clusters that maybe sitting in  you know somewhere else  in a very  verysimple way through that samecontrol plane  and so it sfurther simplifying how youdeploy those clusters and howyou manage them  and bringingyou the power of GKE and Anthostogether    Yeah No  I love that demo of justshowing a single G cloud commandbecause for someone today youcould deploy Anthos on AWS  butyou first stand up thatmanagement cluster It wasn t hard  but can Isimplify that  can we take moreresponsibility  and it lookslike we have  so I think it sreally just a simplificationstory Awesome This one is for me somehow So awesome So how do I choose between GKEauto pilot and Cloud Run Look  auto pilot is amazing It feels like serverlessKubernetes  and that s kind ofmagic on how that behaves  andit s still Kubernetes undercover There s no extra extensionstuff  this is just GKE And Cloud Run is also kind of aserverless environment So if you look at the differencebetween the two  really it sabout  to some extent  do I wantKubernetes If I m deploying marketplaceofferings  I m deploying adatabase  I m deploying statefulworkloads  auto pilot is goingto be amazing there Right So that one is pretty powerful And then at the same time  ifI m in Cloud Run  and as youmentioned earlier  right  ifI ve got a lot of managedservices  I m using spanner Pub Sub  I m doing AI andputting some code in the gaps toconnect the dots to add some APIend points  Cloud Run isamazing So when I look at this  when Ilook at customers  it s often acase where if I want Kubernetes GKE is clearly a great choice If I m building compute intensesystems  as you mentionedearlier  GKE is amazing  andauto pilot is awesome  and whenI m just trying to run somecontainers and building thingsthat kind of connect dotsbetween other managed services Cloud Run is best in class    Yeah It depends on your startingpoint  I guess    Yeah But again  not too many choices but just enough Awesome This is another one for Sachin This one is coming from FordMotor Company folks Is Anthos open source Is there any support available Is this just DIY  live yourlife  or are we actually behindthe product helping people  thisis commercial offering Help clear that up    Yeah  first of all  in termsof open source  it s built onKubernetes And so built on open sourcecomponents  so for sure  itenables our first party softwareon top  but also third partyopen source software that canrun in that containerized or VMenvironment As far as support  it s been   we now have more and morepartners  more and morecustomers trained on Anthos Anthos is the foundation ofGoogle distributed cloud aswell  and so we continue toinvest heavily to ensure thatthe training  the enablement the skills  the capabilities get built up in our customers in our partners  to continue tosupport those deployments I don t know if you want to addto that    No  it s a great answer I mean  we see so many peopletake all this amazing opensource and build platforms which can be powerful  andAnthos is actually 30 somethingodd open source projects thatcome in to one curated integrated  tested  packagedcommercial product  as you say and the multi cloud support is abest effort We test this and certify it onas zur  on Amazon  on prem  onmetal  so I think that s prettypowerful stuff that  you know  Ithink some people can still geta little bit like what in theworld is Anthos  and I thinkit s important for us to remindpeople that Anthos is really away to build and managedistributed fleets ofinfrastructure  and so what apowerful way to do that in asupported fashion    Yeah  hybrid  multi cloud with hardware  softwaretogether    How do I manage a bunch ofGKE clusters all over the place That s a really powerful hopeful  simple way tounderstand this sort of thing Have you continued to see thatuptick as well in Anthos I know we re starting to seesome of the growth there Are you happy with where thingsare going    Yeah  absolutely We continue to see more customertraction  and now with Anthos Anthos for VM s  Anthos for baremetal    Putting in distributed cloudis powerful story for customers because I can make oneinvestment in my skills and nowI can take advantage of this ina lot of places    Yes    Awesome So next we want to go to Fabian So Fabian is asking  what containers are all over theplace  should I just opt forCloud Run instead of App Engine Wlees the future of App Enginein this context Vidia  what have you got for me    Yeah So App Engine is obviously avery successful product for usat Google and serverless However  we see containers asvery  very strong positioned forthe future of development  asyou could see with all the demosthat we had  and with technologysuch as Cloud Run  as well asGKE auto pilot that has come into play  it has got the benefitsof portability and betterintegrations with the rest ofGCP  as well as the cloud So that s the way we look at it    That seems like a goodanswer I mean  you know  Google Cloud sgot a lot of these stableservices  a lot of customers onApp Engine  but it s nice tohave alternates as people arelooking for other integrations things like that Cloud Run is continuing to be abig investment That s awesome Sachin  this one s for you Security questions I mean  I was personallyimpressed with a lot of it andenjoyed a lot of the keynotesfrom yesterday as we looked atall the investments we re makingin security here If you re trying to make achoice on security  I thinkGoogle Cloud is your bestchoice  and not just because Iwork here But how secure are some of theseclusters out of the box We re thinking about  I massuming this refers to Anthos You know  when we stick eitheran Anthos cluster or a GKEcluster off prem or even just aGKE cluster sitting in GoogleCloud  how do you think aboutthe security store and howsecure is this by default    By default what we re tryingto do is turn on all the bestpractices that we recommend sothat the communication channelthat you described is secure ifcustomers want to manage thatwith their own case  they can but we want to make sure thatall the best practices  so thatif it s    the data is intransit  the data is at rest then by default we can keep thaton for customers I mean  you should add some moreAnthos specific  maybe securitycapabilities that we turn on    Yeah  as you say  there s alot of important things aroundOS hardening  how we distributebytes They re coming from us You can trust that as well thatwe re putting them through thepaces  but then with auto pilot arguably one of the best partsof auto pilot is that we justenable all the security stuff bydefault  because I don t knowwhat I m doing as an end usermyself when I send up stuff  soif you re turning on the rightsort of encryption at restsettings  you re tongue on allthe right access  you re turningon continual updates forpatching  I think that s prettyawesome So in a lot of these cases we redefaulting to secure    I think one of the questionswe get is how do you handle thisin the air gap environment Like  so how does the customerget software updates  how dothey make those secure And so what we do is we takethat software  we publish it toa repository The customer is able to downloadit outside their air gap  checkit for any vulnerabilities  youknow  test it out on their own and then move it into the airgap  and then the platform willautomatically deploy it  and sothere s some additional checksthat they re able to add whenthey have those very strictsovereignty data privacy needs and they need to be able tocheck any piece of softwarethat s coming down    Powerful stuff All right Last question for you I could ask you questions allday  but I m being told I haveto ask you a last question    Okay    So John wants to know aswell  so I m assuming thisrefers as well to distributedcloud Whose hardware is this and whodoes it run on  and do youmanage the hardware platform aswell Let s at least refer todistributed cloud Are we managing the hardware Is it coming from Google Are we making the kits  or is itcoming from partners Lay that out for us real quick    Yeah  so it s prescriptivehardware that we provide  and wemanage Right And so in terms of how it ssupported  again  that s donethrough us We can do this through partnersas well  and so what happens isin a    for example in air gap in the hosted mode  you may needto have a trusted partner    orcustomer may want a trustedpartner who s actually doing theoperations in terms of hey  I madding a rack  or adding morehardware components  but thesoftware that they providemanages that hardwareinfrastructure that s deployedon prem  and the beauty of it isthe controlled plane there iscompletely disconnected fromGoogle Cloud as well So you have both those options you know  the edge mode whereit s connected  we can do itcentrally across many sites  orhosted  where the control planeis local  but we still managethe hardware    Super powerful stuff Love that So thank you  Sachin Thanks to all of our guests whowe threw questions to on thedemo set So it s been awesome hearingfrom you all  getting all thatfeedback Be sure to hop over to the cloudNext site and stay tuned forPhil and Sunil talking aboutsecurity Thank you    Hi  there Thanks for all the participationin our Q A  and I hope you reenjoying the show So welcome to SecuritySpotlight My name is Sunil Potti  and I mthe VP and GM for Google CloudSecurity Over the last few months  we veseen some of the most damagingcyber attacks in history  youknow  against public utilities private sector companies government agencies Causing many organizations torealize they are at a pivotalmoment in their securityjourney Of course  this is not news toyou know most of you in ourindustry  but let me put a fewthings in context with asobering statistic Did you know that the cost ofcybercrime is now estimatedbetween 2 and 10 trilliondollars a year For the past two decades  Googlehas made security the cornerstone of our strategy This has resulted in Googleenabling more users to be safethan anyone else in the world locking malware  phishingattempts  spam  cyber attacksglobally  and not just for a fewthousands of Web sites  but forbillions of users and millionsof Web sites globally So that being said  we alsorealized that this journey  youknow  will require enduringcommitment over the comingyears And that s why Google iscommitted to invest  10 billionfor the next five years to helpstrengthen cyber security forenterprises  consumers andgovernments We want you to be able to useour Google Cloud Security magicto protect your enterprise We make that possible in twoways First  we provide the industry smost trusted cloud foraccelerating your digitaltransformation efforts  andsecondly  we bring trust to youwith security products that meetyou where you are  and bring ourGoogle security magic to youron prem  private and multi cloudenvironments And it s just not about havingbest in class securitycapabilities It s also about how they areactually delivered in aconsumerized fashion But at Google Cloud  we take adifferent approach We are driven by a vision ofinvisible security  wheresecurity technologies areengineered in  so you getopinionated full stack coverageof security controls  securityoperations as a silo disappears so niche security talent getsdemocratized And last  but quitesignificantly  sharedresponsibility in a world ofpublic cloud evolves to sharedFate  where we  as yourprovider  have true skin in thegame  and mutual securityoutcomes So a great example of what Imean by invisible security isthe all new  built in dataprotection  automatic DLP forBigQuery It s a game changing capabilitythat discovers and classifiessensitive data for all of BQproject across your entireorgwithout you needing to do asingle thing If you take a full step back andinternalize what invisiblesecurity could mean for you  asan enterprise  a serviceprovider  a partner  or whoeverit is  there are three areaswhere Google s insight andexperience can make a meaningfuldifference One  protecting your employeesand assets Two  protecting your IP Three  protecting your users andyour brand Let me spend a bit of time oneach of these First  one of the most overusedbuzz words in cyber securitytoday is zero trust Why not The core of the zero trustapproach is the idea thatimplicit trust in any singlecomponent of a complex interconnected system can createserious security risks What does that mean It s that trust needs to beestablished via multiplemechanisms  and be verifiedcontinuously So here at Google  we haveapplied a zero trust approach tomost aspects of our operations We ve implemented zero trustaccess with our BeyondCorpframework  shared our use casewith the entire world  anddelivered BeyondCorp enterprise a world class solution thatincludes integrated threat anddata protection We ve had great reception to beon corp enterprise since thelaunch earlier this year  andtoday we re deliveringcapabilities that expand thesurface area for zero trustaccess to cover all of yourapps  both modern  as well aslegacy So as a point of note  rightafter this session  the teamwill be doing a live end to enddemo of BeyondCorp Enterprise toshow how you can make accesseasier and more secure theGoogle way So don t miss it Next  protecting your assets iseasier if it s intentionallydone upstream  shifting securityleft And so zero trust goes beyondaccess to protecting yourworkloads as well and your IP And how Google operates ourproduction environment is inthis way  the way software isconceived  produced  managed andinteracts with other software We call that approachBeyondProd  and we ve alreadyproductized many capabilitiesfor you So looking at BeyondCorp andBeyondProd  you realize thatzero trust is much more thantools Zero trust is how we envisiondesigning  deploying andoperating safe environments soour daily lives  as anenterprise or consumer  can gofrom a world of being on theedge to being safe And safer with Google But you can t lose sight of thefact that while zero trustapproaches help  you know address the preventative side ofyour program  robust detectionand response capabilities areneeded to complete theEquEquation And this is why we invest inchronicle  and wecontinue to amplify it as ourfoundational security operationsolution And we are excited to announcestronger integration betweenChronicle and Security CommandCenter  SCC on GCP withcentralization of alerts  theaddition of cloud asset andmanagement  and user context toenrich investigations Next  let s talk a little bitabout protecting yourintellectual property So for organizations pursuingdigital transformation  code isyour IP It s the heart of your business The foundation of securing yourcode is a secure software supplychain From the time developers startwriting code through your CI CDpipeline all the way throughdeploying and operating inproduction So at Google Cloud  we believetwo things are foundationallynecessary for doing this in anenduring fashion One  industry wide frameworksand standards are a must  butthey also need to becomplemented by full stackmanaged services that implementthese standards and makeadoption easy So for the first  Googlecofounded open SSF acrossindustry forum for open sourceand supply chains softwaresecurity Projects such as SLSA  anend to end framework forEnEnduring the integrity ofsoftware artifacts And second  we have the all starGitHub app  right  in time forthe fall season For continuous enforcement ofbest practices for GitHubprojects And then last but not least open Source Score Cards  whichprovide a risk score for opensource projects And this is just a few waysGoogle is contributing toindustry wide standards And on Google Cloud  with ourinvisible security approach  weprovide these tools in a fullymanaged environment  from codeto build  deploy  run andoperate that implement thesestandards by default And let s not forget  you alsoget a consistent way to defineand enforce policy for a zerotrust software supply chain thatestablishes prominence andprevents modification ortampering And we re building this zerotrust software supply chain withnew additions across the board You can learn more about all ofthese launches in our breakoutsessions So to add some more  you know context to this with new voicesin our conversation on thesetopics  I d like to bring onMurray from Facebook  and Steph who leads our cloud securityuser experience team Steph and Murray  over to you    Thanks Sunil  and welcome Murray Can you tell us about your roleat Facebook and what that workentails    Sure I m a manager supporting theCloud foundation team withinFacebook Cloud foundation is chartered tohelp teams with a need to usepublic Clouds for theirworkloads so that they can do soboth quickly and safely    Can you talk about howFacebook is leveraging zerotrust  secure supply chains orother modern cyber securitysolutions    Facebook focuses a lot ofengineering effort on securesupply chains This extends to our use of theCloud as well  so our workloadsthat will run externally haveall of the same requirements asthe internal ones in terms ofsecurity  vulnerabilitymanagement  security logging monitoring  tight access controlto resources  access auditing data deletion requirements penetration testing  mandatoryreviews of anything with privacyimplications and so on One approach we re taking isthat our teams can t use anyUI s or directly control theservices they build in thecloud They have to go through our toolsuite to do it Some of our suite usesopen source    a common opensource tools  which means thereare vast online help resourcesavailable to enable our teams todesign and build their services Then  we provide our ownextensions so that these tools integrated with our existinginternal development work flows logging systems  dash boards  etcetera It also allows us to impose ourown guardrails and bestpractices It s infrastructure as code inthe truest sense Naturally  we already have aninternal CI CD system that sused to update our internalservices on a regular basisafter appropriate code reviewsand automated testing    So I d love to wrap up byhearing about your experiences what your team uses to keepFacebook fortified andcontinually strengthen itssecurity posture  and two partnering    One of the capabilities thatour tool chain allows is toinspect what the deployment isgoing to do before it happens which gives us an opportunity toimpose best practices or errorchecking and stop problems fromgoing live Some of these rules shield usfrom accidental securityexposure Some look for weak deploymentpractices  such as a lack ofredundancy  and still otherslook for avoidably expensivepractices These rules applied here arebased on both our ownexperiences in policies  andthose recommended by our expertpartner teams supporting us This allows us to be agile andrespond to incidents when theycome to our attention    Thanks  Murray and Steph Interesting to see how theseideas are implemented  and I mexcited to see where we llcontinue to go together Okay So let s move beyond ourinfrastructure for a moment andthink about the big picture The software that runs yourbusiness delivers value to yourcustomers and your users The security you re building isdesigned to protect the usersand build trust between them andyou Security incidents erode thattrust  and eventually damageyour brand So now Google keeps more userssafe online than anyone else onthe world So let s see how we areadvancing our efforts anddelivering all thesecapabilities to enterpriseswherever you are Eradicating phishing  as well ascompletely stopping fraud Period And in this area  safe browsingis the gold standard Our program to protect usersfrom dangerous sites or files isunprecedented in its reach andcapability Its capabilities are invisiblybaked in to browsers on fourbillion plus devices Last year alone  we advanced thestate of the art with enhancedsafe browsing  an optional butmore advanced level of securityfor safe browsing the Web  andby sharing real time browsingdata in a privacy preservingway  we were able to deliver a35 percent additional reductionin phishing attacks and malwarethat reach users These kinds of numbers and thiskind of efficacy has not beenpossible without leaning in onour heritage  and our corecompetencies in providingsecurity as a built incapability to consumers  as wellas enterprises wherever theyare And since its introduction inApril 2020  in addition to safebrowsing  millions of users havetaken advantage of this extraprotection in a very rapidfashion So our ultimate goal is toeliminate phishing as a threat Period Bringing safe browsingcapabilities into yourenvironment through Gmail BeyondCorp Enterprise  and manyother products So in addition to targetingusers directly  attackers viewWeb sites as the first and ofteneasiest place to commit fraud And over five million sitestoday use Recapture  our fraudandbot management solution thatstops attacks like potentialstopping and account takeovers With Recapture Enterprise  youcan do this frictionlesslywithout user challenges tomaintain an optimal userexperience  which is justanother example of invisiblesecurity And over and above this  wecontinue to bring together thebest of Google s capabilities toenhance fraud prevention So a new integration betweenCloud Armor and RecaptureEnterprise that detects andstops bot activity at the edgeis now in preview So let s hear more from BrianLozada  the CSO at HBO Max onhow they re actually usingRecapture Enterprise to protecttheir users  and therefore theirbrand    Thank you  Sunil We chose Recapture Enterprisebecause we wanted to offer ourcustomers security againstmalicious actors whilecontinuing to provide africtionless customerexperience With recapture enterprise  weenable our customers through ourlog in and registration processwithout requiring them to engagein any kind of a challenge It s a win for everyone Our development and productorganizations can continue tofocus on creating customercentric experiences and ourcustomers can easily use ourservices The use of Recapture has helpedus in our mission to secure thecustomer experience  allowingour customers to feelcomfortable while enjoying ourplatform Thank you    Thank you  Brian So we want to provide as muchassistance as possible in yourdigital transformation  or yoursecurity transformation As Thomas mentioned in hiskeynote  we are excited tolaunch our new Google cybersecurity action team to help And to talk more about this  I dlove to kind of bring on PhilVenables  our Cloud CSO  who sleading this effort to tell youmore Phil    Thanks  Sunil So the Google cyber securityaction team marshals expertsfrom across Google to form whatwe believe will be the world spremier security advisory team It has a singular mission supporting the security anddigital transformation ofgovernment  criticalinfrastructure  enterprises small businesses  consumers andsociety overall To deliver on this mission there are many ways the GCAT canhelp you today  with strategicadvisory services for yoursecurity strategy  includingtransformational workshops andcontent like our CSO guide tosecurity transformation  and ourframework for increasingoperational resilience forfinancial servicesorganizations With trust and complianceservices driven by specialistswho continually obtain the mostimportant global compliancecertifications for our products and map them to industry controlframeworks for you We are full spectrum  customersecurity and solutionsengineering that delivers provenblueprints and architectures fordeploying our products andservices securely in accordancewith your regulatory regimes  aswell as comprehensive solutionsfor areas like autonomicsecurity operations  cyberresilience  zero trustarchitectures  and many morewe ve got under development And finally  with threatintelligence and incidentresponse services  includingthreat briefings  preparednessdrills and rapid responseengagements  so you can stay ontop of the evolving threat andsecurity landscape Another way we re working tohelp strengthen and transformsecurity is by working withindustry and the public sector Google Cloud joined as aninitial partner for the U S Department of HomelandSecurity s Joint Cyber DefenseCollaborative  and thisinitiative will strengthen ourcollective security posture bypre empting and reducing theimpact of cyber threats throughincreasingly strong partnershipsbetween the public and privatesectors  and as Sunil mentioned the White House CybersecuritySummit in August  we announcedthat Google will invest  10billion over the next five yearsto further strengthen cybersecurity The areas addressed includedexpanding zero trust programs securing a software supplychain  enhancing open sourcesecurity  as well as training100 000 Americans in skills including data privacy andsecurity through our Googlecareer certificate program And this builds on the  100million we ve already committedto improving open sourcesecurity  focused on effortslike Lenox kernel development work like metrics tracking  andthe coordinated vulnerabilitydisclosure through the OpenSource Security Foundation So with that  Sunil  back to youto wrap up    Thanks  Phil Great stuff As you can see  our commitmentto improving security forcustomers in the whole ecosystemis substantial And more importantly  it senduring In products  in resources  andmonetary returns We are making a meaningfuldifference with our long rangevision and commitment ofinvisible security  deliveringcontinuously around zero trust safely securing your softwaresupply chain  and deliveringuser protection services toprotect your users and yourbrand So to close  I mentioned a fewnew products  and I didn t havetime to cover all the otheramazing innovations that arecoming out of our teams And we are making many more suchannouncements here at Next So please see our securityexperts  fellow customers andpartners in our track sessionsto go deep on the topics andproduct that matter most to you So don t forget  the live demoof BeyondCorp Enterprise iscoming up next right here followed by a live Q A  where wewill be answering your topquestions Thanks again  stay secure  andhave a great rest of Next   Ly  everyone I m macro    And I m tanesha    In the security spotlight  weheard about the importance of azero trust approach to security    And in our demo today  wewill show you how to implementzero trust access to be moresecure and more productive And a quick shoutout to ouraudience This is truly live Dive into the chat We want to hear from you Tell us what you do and whereyou re joining from We can see you Check it out  macro    Sure enough  we can seeeverybody down here  and we reso excited to interact and talkto you all today If you re still hanging out inthe next event Web site  clickon the blue button that saysjoin the interactive experienceto activate these features So let s get into it We re going to cover threethings One  how your entire workforcecan securely access legacy andmodern applications withoutexposing your network toattacks And two  how you can get betterprotection against threats anddata loss right from within yourBrobrowser And three  how you can gainvisibility in the unsafeactivity even if users aren t ona corporate network or deviceand we re going to do all thiswith beBeyondCorp enterprise This is Google Cloud s zeroaccess trust solution With BCE  you can use Chrome toaccess apps and resources In the background  Google snetwork protects and proxiestraffic  enforcing accesspolicies These policies use factoriessuch as identity  device info location and third party signalsto authorize access to apps anddata that you need to do yourjob    That s awesome  Marco So let s get to the good stuff Think about it How easy is it for yourdevelopers to work remotely andsecurely right now We know it s a tough task We know that making life easyfor developers will alsoprotecting your code iscritical That s why our new BeyondCorpEnterprise feature clientconnector is so important It enables zero tech access tolegacy thick line applications    Yeah I think a lot of people run intothat today So let s take a look at howdevelopers would use this We set up this laptop for theCymbal group    By the way  they re afictional company Cymbal has been around since the1970s  and they haven tmodernized all theirapplications Sound familiar One of their main developerapplications is still hosted ina private data center    Yeah  I see this quite a bit Until recently  Cymbaldevelopers didn t have a way toaccess this application remotelywithout a VPN The security team had constantconcerns that remote users couldexpose the network or evenworse  their source code toattacks or hackers But now  using BeyondCorpEnterprise client connector they can access client serveraapps without a VPN Let me show you how users wouldconnect via SSH to their servercom pos Torre  and let me justsay  don t blink or you mightmiss it It s that simple So let s go ahead and swing onover to our developer machine From here  I can simply open upGoogle Chrome Once Google Chrome has actuallyloaded  you can see I veauthenticated to my applicationrepository already I simply click on the end pointverification extension and fromhere I click on startconnection My end point is being posturedin the background and secureconnection is being made to GetLab From there  I can simply  then once it s turned green  ofcourse  minimize my browser  andthen on the left hand side  I mgoing to go ahead and open up myterminal application This is my thick clientapplication I m going to then go ahead andrun a Git clone  and that sactually going to pull down thecode from Git to my localmachine And there s my applicationpulled down locally on to my endpoint I know that probably seemedpretty straightforward  but whatyou might not have noticed wasfirst  there were no clients It s all in the browser youprobably already had open Second  in the background continuously validating theidentity and the device and abunch of other factors is ouridentity ware proxy And third  our TCP proxyseamlessly and securely forwardsall the traffic The fact that developers aroundthe world with do this remotely simply  and securely without allthe hassle of a VPN is prettyawesome    And  with BeyondCorpEnterprise  workers are onlyallowed access to applicationsthey re permitted to use So we prevent unwanted lateralmovement across the network So  audience  we want to know would you use this method forremote access in yourorganization    Awesome Now  let s see how yourworkforce can securely accessmodern Web applications evenfrom noncorporate devices Let s look at how Cymbal does itfor their extended workforce their call center contractors    Meet Rhonda She s one of thousands remotecontractors who help withCymbal s 24 7 support during theholiday shopping season So security community  can youshare in our chat  in the pastyear  have you onboarded newtemporary workers Did you send them a laptop ormake them install software sothey could be more secure    Looks like that would be ayes Contractors also tend to be morevulnerable to attacks Last year  44 percent oforganizations experienced abreach  and 74 percent of thosebreaches were the result ofgiving too much privilege accessto third parties So for Cymbal  keeping thoseusers off the corporate networkdecreases the risk of beingexposed to attacks like raransomware  and here s whereusing BeyondCorp Enterprisecomes in again for Cymbal Contractors and other employeeswere able to BYOD  onboardquickly and use a device thatthey feel comfortable with whileBCE layers of protection keepthem secure    That way  Rhonda can havesecure access from her owndevice with clear separationbetween work and personalactivity Let me show you exactly what shesees as she begins her day She first navigates to herGoogle Chrome browser Then she navigates to her Cymbalcall center application We ll enter in our credentials We ll then do two factorauthentication with our timedsecurity key Now that we ve passed two factorauthentication  we are in ourCymbal call center application You saw Rhonda log in to Chromewith her Cymbal credentials This is what we call aBeyondCorp protector profile It extends threat and dataprotection as soon as she logsin As you can see  our agentlessapproach means no additionalsoftware She gets right to work on herown device rather than pickingup a laptop from IT and waitinghours while everything getsconfigured  therefore savingtime and money    Awesome to note And the thing about zero trustaccess is that we don tautomatically trust Rhonda justbecause she has log incredentials Authorization is continuous  notjust when she first logs in BeyondCorp Enterprise enforcesCymbal s contractor accesspolicies based on her identity and contextual information abouther device and location  as wellas the fact that she sauthenticated with her tightenkey So security community  we wantto know  what do you all use formulti factor authentication SMS code  security keys Maybe an authentication app Or maybe something elsecompletely Hopefully it s not nothing    For me  it s my security key Every day Especially since I don t alwayshave my phone on me    Yep Same here They re so easy to use  and theyprovide the strongest protectionagainst phishing Regardless of what you use  someform of multi factorauthentication is critical forbasic security hygiene  and wehighly recommend it  especiallyfor remote access So let s take a look I don t know if anybody hasanything Looks like most people use let s say  security keys B Awesome That s what we love to hear So let s get back to Rhonda  whojust authenticated to the app and explore how we can integratethreat and data protection rightfrom within a browser    Great Let s see her get to work andsee how BCE protects Cymbal their customers and Rhonda withease It s been a really busy day She doesn t have enough time tofinish processing a batch ofcustomer refunds before her nextcall She wants to save the customercredit card information to alocal file and do the refundswhen things slow down  butsaving this sensitiveinformation to another locationis against Cymbal s securitypolicy Let me show you her experience Let s navigate to our creditcard file  and let s downloadthis so Rhonda can do therefunds at another time As you can see  she s blocked You can see a message appear atthe bottom that prevents herfrom doing so    Yeah Exactly You can actually see where thecredit card PDF has sensitivedangerous content inside of it And I ll show you a policythat s been configured forCymbal that will detect riskybehaviors just like these So let s go ahead and look ather administrative console here admin Google com Over on the left hand side  youcan see the security menu  andif you scroll down underneathsecurity  you can actually seedata protection And we re going to go ahead andauthenticate  obviously  asadministrators securely  solet s go ahead and type in ourpassword And once I ve authenticated I ll be able to go ahead andlook at my data protectionpolicies You ll notice here where it saysmanaged rules  so we re going togo ahead and go on the managedrules  and we can actually seethe credit card detectionpolicy I ve also got detectors forthings like Social Security oreven detecting code that s beencopied or pasted or shared bythe developers So as the admin  you can decideif you want to automaticallyblock user actions  as Cymbalhas done here  or if you want totrigger a warning to the userinstead    And in addition to thingslike credit card numbers  youcan also set specific DLPpolicies to detect file type assource code So that way you can protectsensitive information and codeby monitoring  controlling oreven blocking what people uploador download    Yeah Exactly We can use very granularpolicies for this type ofinformation in order to protectagainst exfiltration So we just showed you some ofthe types of data protectionpolicies you can set up You can also customize accesspolicies and the changes takeeffect in real time This is a really big deal because you get continuouschecking of whether a user is ina policy giving you up to thesecond security controls In fact  let s make an updatehere in real time for Rhonda So  okay  community  we re goingto ask you which policy weshould change Is it her location Maybe her operating systempolicy  or whether or not she son a corporate owned device Please chime in here We d really love to understandand hear what you guys deemimpoimportant While we wait for the results tocome in  I will say that I seethese kinds of things all thetime with my customers They want to be able todynamically change policiesdepending on theircircumstances In particular  some customersare really interested incontrolling for location perhaps limiting access to onlycertain countries Additionally  if a company isgoing through an org change ormaybe a merger  being able tochange access policies forselect groups of users ordepartments in real time iscrucial  and beyond these threechoices in the poll  there aremany other ways to customizeyour access policies based onwhat your organization needs So let s go ahead and look backat the polls And looks like corporate ondevice So awesome I love it And thanks for everybodycontributing Let s go ahead and do this rightnow So we re going to go ahead andjump on over into our GCP cloudconsole  and from here we cansee our identity ware proxy We can actually see our CymbalGit lab for developerapplication We can also see our call centerapplication And again  these applicationscan be anywhere  and for thisdemo  they re in GCP We re going to go ahead andselect the call centerapplication and down at thebottom  we can see Rhonda overon the right hand side and herapplication access policy Let s go ahead and edit that and we re going to go ahead andremove the existing accesspolicy that s allowing accessfrom Europe and the U S   andwe re going to go ahead anddelete that one  and we re goingto add a new role We re simply going to go aheadand select cloud IEP  IEP Webapp user and from there we regoing to go ahead and selectaccess policy  which everybodychose  and that was requirecorporate device  right Okay I just wanted to make sure Iremembered that correctly Let s go ahead and save that inplace Click save And when we save that policy it s actually going to bepropagated across the worldwithin a very short period oftime All of the proxies acrossGoogle s global network areimmediately updated So the next time any user triesto access a protective resource they re evaluated against thatnew policy set We mentioned continuousauthorization earlier  and thisis a key part of that Authorization is not a one timeoccurrence So even if you begin a newsession that doesn t mean you llhave perpetual access    Exactly And for companies like Cymbalthat employ hourly and tempworkers  the ability todynamically update accessconditions is important For instance  they could setthese policies so workers onlyaccess applications andresources during their shifthours  or working days  or onlyallow access from    only allowaccess from specificgeographies They also may want to requirethat devices have the mostup to date operating systems and security patches So this is an importantcondition to manage  especiallyfor all the contractors usingtheir own devices    Yep Exactly So because Rhonda doesn t needtmeet that condition which I justupdated  she will no longer beable to access the call centerapplication Now  I think we ve all facedthis  and it s one of the mostfrustrating things about remoteaccess  especially when you reusing the VPN You think you should have accessto something  but for somereason  it s just not working    It s the worst So let s see if our real timechange worked So when Rhonda tries to open anew task in the call centerapplication  let s take a lookat what she sees Let s navigate back to our home And she s denied access as aresult of the change Marco justdid But once again  BCE has Rhondacovered She can report this error usingour new feature  the BeyondCorpEnterprise policy troubtroubleshooter  which informsend users that they re blockedand tells them how to get helpquickly So as you can see here  she willfollow the prompt to E mail toadmin to get help Let s go ahead and E mail ouradmin Our admin is now notified of usbeing blocked With BeyondCorp Enterprisepolicy troubleshooter  adminscan quickly fix issues that areblocking users  keeping themproductive    Yep You bet Let me show you what the Cymbaladmin would see on the otherside of this request  and howthey can unblock users likeRhonda So we re going to go ahead andswitch gears Let s go over to G mail  anotherGoogle application  and it lookslike we got a notification forcredit card detection  which isgood So we know if Rhonda is takingsome interesting actions withinher end point Oh  and it looks like we justgot an E mail from Rhondabecause she s actually beenblocked to an application There s the link that she sentover  so let s go ahead andclick on that and we llautomatically be logged directlyinto the Google Cloud platform So from here  I can actually seethe different policies andbindings that are in place  solet s go ahead and select thecall center application and overon the right hand side here  wecan actually see the grantedconditions  or the denialsthemselves So let s go ahead and look atthe binding details So interestingly  here we cansee that Rhonda failed to meetany of the access levels andsure enough  it s requiring acorporate device  which was notgranted So I d normally go back andblock her by updating thatpolicy  so her access levels areevaluated like any othercontractor  but we re going tokeep moving  just in theinterest of time    Sounds good That was so easy  by the way    Yeah Super easy Now  let s look how BCE can giveus better visibility into unsafeuser activities  whether they reunsuccessful access attemptslike we just saw  or otheractivities across the apps thatBCE protects Let me pull up the securitydashboards in Chrome and giveyou all a look So we re going to jump back overinto our Google admin consolehere and in the same securitymenu on the left hand side  wecan actually click ondashboards Wait for those to load out for asecond So something to make note ofhere is that with Chrome dataprotection  you re actuallygoing to get a whole slew ofdifferent information We can see Chrome high riskusers  we can see individual DSPincidents  if we wanted toscroll down a little bit  wecould actually see how manyusers are forgetting their passwords  if I can figure out howto use the track pad here So we can see user log inattempts  we can see  forexample  messages that areencrypted  but what we reinterested in is whether or notthose credit card numbers aremaking it through So let s go ahead and drill in alittle bit in to one of thesereports So we can see every single fileuploaded  file downloaded  Webcontent upload  for example  andwe can actually see every singletime that this took place forour Social Security detection as well as credit carddetection  and if we wereactually to drill in on creditcard detection here  I thinkthis is really cool  so I justwant to show everybody realquick here  since we do havelike another minute  and that isall the sensitive data transfersthat are taking place  blocked detected or otherwise So even if your organizationisn t using all corporate owneddevices  you can still monitorsecurity events and investigatethose alerts    That s awesome  Marco Audience  so what do you think Let us know in the chat We definitely know this issomething of interest In the last 15 minutes  you veseen how entire workforce canaccess modern and legacyapplications securely  how youcan improve threat and dataprotection  and how you can getbetter visibility in to riskyactivity even on unmanageddevices    Yeah Our call goal  right  at the endof the day  with BeyondCorpEnterprise is to make yourexperience more productive andmore secure  and our team looksforward to supporting you onyour zero trust journey    Thanks so much for joining ustoday and participating We have a live Q A coming upnext to answer all of yourGoogle Cloud security questions As well as any questions youmight have had from the demo So please stick around    Yeah Please do And thank you all for joining We ll see you all soon    Bye    Awesome Thanks so much Tanesha andMarco That was an incredible demo It s always fun to see ourproducts in action and I m surebetween the great spotlightearlier  the demo  our audiencehas some great questions linedup So thank you for joiningeveryone today My name is Iman    And I m Sri I m so thrilled to be hereanswering your questions livefrom our fabulous studio here inSunnyvale    Awesome We re also joined by the teamthat just took you through thatlive demo just a few minutesago  so you may see them pop upto help answer questionsthroughout the shoI  show Okay Let s jump right in with alittle poll Tell us what got you mostexcited today Is it our trusted cloud Is it our zero trust philosophy Is it our  10 billioncybersecurity investment or isit the cybersecurity actionteam And by the way  I m waiting forthese poll results  so while wewait for these results  just areminder  this Q A is live  soyou can engage with us directly So Steve  Brian  Danesh  I mwatching all of you right herelive on the chat So if you can t  just go back tothe next event Web site andclick on the blue join theinteractive experience button And as we give you some time tosubmit your poll  I want to hearthe scoop from you  Sri  so tellus what you re most excitedabout    I am really excited about thezero trust model I feel like we re kind ofadopting it  and we re seeing alot of people move toward it So      Yeah Yeah I mean  we ve been doing thisfor so long  right You know  we re taking anapproach that we ve pine nearedand we re putting it out in theworld and we re watching a lotof customers sort of adoptingthis approach now    Yes Absolutely    Awesome Awesome    Oh  looks like it is    It looks like it is  yeah That s right That s right Looks like a lot of people arereally excited about zero trust That s amazing Okay Cool What s the next We ve got our  10 billioncybersecurity investment  butzero trust is definitely leadingby a pretty big margin All right So let s go ahead and getstarted Let s kick off our firstquestion Yeah    All right Sounds good    All right Cool So I guess while we re waitingfor questions from the audienceto populate  the question I havefor you is why is securitydominating the headlines    You have to be under a rockto not  like  you know  noticeall the stuff that s happeningaround us  all the IPO s  withForge Rock and Dark Trace andwe re seeing presidents andprime ministers commit a largeamount  a giant amount ofdollars  including our newpresident  to security And so I think there are twothings here Typically  in industry  we llsee a lot of movement becausethere is a technology shift thathappens  and we re seeing that too  with the digitaltransformation  you know  theway that we secure ourenvironments has changed  and sothere s a lot of movement that shappening in this industrybecause of that Added to that security has thisother thing  where it s alsodriven by the people who areattacking the  you know  theattacks that are happening  andwe re seeing a huge amount ofattacks happening today And as a result  like  forexample  we ve had    I mean this is really sad We ve had our first death thathappened because of a ransomwarethis year We ve had a pipeline companyhave to pay a ton of money for aransomware We have had manufacturingcompanies get disrupted in theirmanufacturing company  basicallyhad a disruption prettyrecently And so we re seeing a lot ofthis happen all around us And that s kind of adding more you know  focus into this area where people are starting torealize that this is animportant area that needs a lotof investment because otherwisebusinesses are going to sufferbecause of nations are going tosuffer  and that s I think partof the reason that we re seeingthis doubling of thatexponential growth    You mentioned a couple greatthings Like  I noticed that the cloudevolution has allowed a lot ofbusinesses to sort of transformthe way they work  the way theyoffer services and products totheir consumers  and we as humanbeings are now more dependent ontechnology now more than ever and so I always like to saythat  like  you know  cybersecurity is going to start to beseen as a service oriented role an honorable role in societybecause we have customers whobuild healthcare systems on ourcloud  industrial systems on ourcloud  you know  offer all sortsof services  right And as we become more and moredependent on technology  we needto ensure that we re alsoadvancing the state of howsecure our infrastructure is sothat we can protect ourcustomers and protect people atthe end of the day So looks like we got a questionfrom Jeanette  which is whatmakes Google s cloud more secureand trusted than itscompetitors    So Google actually startedbuilding a lot of these securityproducts for itself  right So Google is known as a verysecure company  and so whatwe ve done recently is takensome of the work that we havedone to secure our owncorporations or our owncustomers  like  you know BeyondCorp that we just saw thedemo for  or recapture  andproductized it  and the visionhere is that every customer canbe just as secure as Google is But add to that this idea ofinnovation that s there atGoogle So  for example  when we decidedto go in to the federal spaceand do a fed drop certification instead of just standing up acouple of data centers nearNorth Virginia and saying  like let s do that  let s    youknow  this is a set ofcustomers  let s give them acloud that is compliant to theirneeds  we decided to make all ofGoogle Cloud the    so now acustomer in Brazil can leveragethose features just as much asif every customer might And so I think there s thisleaning in to the forefront ofsecurity that we re now seeingwith the trusted partner cloud For example  we had a couple ofannouncements last week We had an announcement  andwe re sort of saying that youccan  a lot of nations want to beable to own their owninfrastructure and their owndestiny And so we re taking the GoogleCloud and kind of making itavailable to them in a way thatthey own it and they operate it and so that s another thing thatwe re doing that s kind ofpushing the boundaries And I think this is just thisidea of pushing the boundarythat is what makes Google reallyspecial    Yeah I think you mentioned somethingthat s really important  whichis like because we own all ofour infrastructure and we veserved billions of users allaround the world through G mail through Search and through allthis  it s kind of a no brainerthat we also allow enterprisesand small businesses to also nowrevamp the way that they dotheir operations on our cloudand on our infrastructure  andso we are able to do globalchanges like that with the fedramp certification  where we domanage all of our infrastructureend to end  which is a big  bigowning of this It allows us to govern our cloudmore securely  more efficiently and also to pioneer reallysignificant changes for ourcloud and all of our customersworldwide    Hundred percent    Yeah All right We got the next question  whichis from Azumi The question is  which operatingsystems does BCE support Does BCE support third partyclouds or on premiseapplication And I believe this questionwould be probably best suitedfor Marco    Yeah  I can answer thatquestion So the beauty of BeyondCorp  andI would just say based onobviously the breadth and widthof our Chrome browser is that wecan support any operating systemat this point  so whether it sChrome LS  obviously  which welove and hold near and dear toour hearts  or windows or Mac OSor Linux or android or iOS  wesupport all of them So your users aren t going to belimited to a particularoperating system  which isgreat  especially when you arean organization trying toenable  right  enable youremployees and your contractors On top of that  we supporteverything from thick clientapplications  right  toserver based applications Web based applications It could be  you know  HTTP TCP based protocols There s no limit at this pointin time So kind of imagination is thelimit    Yeah I mean  as someone who probablydoesn t get as much exposure toBeyondCorp and getting hands onas I used to  I really lovedyour demo  and so that wasreally super helpful  and Ican t wait to go back andrewatch it We ve got another question fromTara  which is in the spotlight Sunil spoke a lot aboutinvisible security So two questions This is two pronged If security is invisible  doesthat mean that I can t changethe settings or decide whatsecurity I want And two  does invisible securitymean that security is free    It s a great question I think I would like to stepback and see where the problemis  right So the problem we re trying tosolve is that there is an alertfatigue that we re trying todeal with  right There s  you know  mostcorporations can t even look at forget about processing  about50 percent of the alerts thatthey get So you re starting to see allthese alerts coming in You don t look at it And there s a lot of work beingdone to bubble up the moreimportant alerts  and put themin front of the sock and soforth  but that still means thatthere are 50 percent of thealerts that you don t even lookat  right So the idea is  are there thingsthat we can do to preventcertain type of action  forexample  that might not bekosher action without creatingfriction in the business So based on the business  ownrules  or policies that you canwrite  are there policies thatyou can write  what can youbuild into the platform Anyway  bringing down the numberof alerts and number of thingsthat you want to watch  and howdo you actually guide things tohappen And so this idea of moving fromjust all this risk bonding tothings  to maybe preventingthings  maybe guiding people inthe right direction A great example is there was acustomer who once told me thatjust by sometimes popping up aquestion  like  you know  isthis something that you want todo You ll find that a large numberof people will just not do it right And so you can    there are alot of different ways that youcan prevent certain things fromhappening  and that  I think  isthe core to what invisiblesecurity is about So it is based on things thatsomeone might configure  andit s based on the business logicthat that company wants to have and it s not something that justinvisible in the sense that youlose visibility into yoursecurity It s invisible in that  youknow  there are things that youdon t need to do and they re notcoming at you all the time    Automate as much as possible It s like no security personever said that their job iscompletely stress free And it s easy  right And so  you know  I always liketo say that  you know  we ashumans  we do security in oureveryday lives When we leave the house  we putour seat belts on  we lock thedoors We ve been taught over all theseyears of evolution  how to besecure  right Some of us more than others right And then when we get into ourdigital lives on the cloud  wehave to relearn all thesethings And some of them we have tobuild into our infrastructure sothat they become part of ourbackground processes  and someof it comes through userenablement and some of it comesthrough the amazing productsthat we serve Security is the core of how weadvance all of the amazingthings that we re doing athumans in to this new worldwhere everything istechnology driven and whatnot We got a question And this one is for Tanesha probably I would say  Tanesha  whatapplications does BCE support    Thanks BCE supports all applicationsfrom IS to SAS to thick clientapplications as well And like Marco mentioned  wesupport HTTP and HTTPS  in ad toTCP protocols like SSH and RDP    Awesome And I see a lot of great chatterfrom the audience  Robby  Joey thank you for engaging All right Cool So we got another question fromjean Mikael  sorry if Ipronounced your name wrong Can you only use the Chromebrowser for BCE And then how do you make surethe Chrome browser is in use isthe approved one  the Chromebrowser that s in use is theapproved one  not another Chromeinstance  that could be a man inthe middle    Yeah  that totally makessense So this is kind of the beauty ofcontext policies We can force these policies downto the browser level  versions for example  what kind ofbrowsers able to access If it s a corporate managedbrowser or device  you can wrappolicies around all of that  soyou can get very  very granular And I would say the other sideof this is we want users to beable to use what browser theywant  right  for personalaccess I think that s great And we should continue tosupport that if that s what theywant and if that s what thecorporation wants But as far as accessingcorporate resources  it s justlike anything else  right You want to be able to manage itand we re not going tonecessarily just trust anything So having a trusted Chromebrowser for accessing corporateapplications works great for ourcustomers  and it s just thebest method to kind of moveforward So hopefully that answers yourquestion    Thanks  Marco Yeah All right We got a question from Robert and this is how is Google s ZeroTrust Model different than othercompeting hyper scalers    So I think zero trust modelis    so Google s Zero TrustModel  and I think there s aworld that s adopting the ZeroTrust Model There s only one model There isn t a Zero Trust Modeland someone else s trust model And the Zero Trust Model is verysimilar to maybe I think ananalogy is the things thathappen today as we got into thestudio We    you know  we ve all been you know  vaccinated and testedmultiple times in the last week And still when we came in  wegot tested again  and then wewere masked and had a shield and we were doing that up untilwe were in front of the camera and as soon as this is all done we re going to be masked and you know  sent out  right So this idea that you haveaccess to something just at thetime that you have access  andthere s context to whether youneed access or not  and whereyou re coming from and whatyou re doing and that should allbe added in to the decisionmaking of what you are  I thinkthat s what is zero trust And it s great that it waspioneered by Google  but todaythat is a model that is beingused everywhere I think where the question maybe coming from is about theBeyondCorp Enterprise and how isthat differentiated And I think the key thing hereis we ve seen the track recordof Google in terms of keepingour employees and our data safethrough BeyondCorp Enterprise and we re trying to get that outto our customers  and the onething that I would like to pointout is the Zero Trust Modelisn t just one tool It s a mindset It s a way in which we dothings And I m really excited aboutGCAT  the announcement that wejust made about thecybersecurity action team  wherewe re going to be having likethese security experts andCSO s  people like Phil Venableavailable to our customers tothink through this model andthink through end to end howthey re actually going to beimplementing it  and so that sthe exciting thing  I think overall We have the pioneering product We have the knowledge of zZeroTrust end to end We ve actually implemented itand we ve proven it    I love the analogy you ve puttogether about COVID andwhatnot  because it s like wewent from traditional networkcentric boundaries to identitycentric boundaries  and now it slike who are you and what s yourpurpose and what are all theother factors in to why youshould be performing thisaction And I think one of the mostamazing things about when Ifirst joined Google was nothaving to use a VPN  and justwork  and I thought that wasreally nice So we re going to go to aquestion from Amitab from Ford So the question is how do weimplement zero trust in our org    This is a great question And I m going to actually throwthat question to you You re in the solutions team So let s talk about this  right You know  how do we implementZero Trust in the org  and Iguess it starts with BeyondCorp but what are all the factorsthat you think about    I always like to say that alot of big workloadtransformations in security arenot just a matter of buying aproduct and paying for aservice It takes a lot of enablement andeducation from the top down andfrom the bottom up We have to structurally thinkabout how we re going totransform the way that we work and what implications those haveon the way that we work todayand what our pros and cons areand where our approach is goingto be and what our strategy isgoing to be to sort of getthere  and that strategy isgoing to be augmented by people by teams  by development teams by tools  by partners  so whenwe advise customers and we sitdown with clients and we talkabout solutions  right  andhelping transform elements oftheir business  we don t justtalk about how a product can beleveraged in your organization Because we ve got product teamsthat can talk about the product We talk about how we canactually transform the business how we can get the wholeorganization to culturallytransform the way that they workand how we can get on a tangiblepath to achieving a zero trustarchitecture in their workforce and that may be a multi yearstrategy  right And so there s many  many  manydifferent layers to this    I agree I think there s this whole thingabout you can think of aquestion only in    you know you can think of a product  anda particular problem that you retrying to solve  but if you renot thinking about itend to end  like security isjust as strong as the weakestlink  right And so if you have a contractorwho has direct access  then youhave a problem there    Yeah    So you re thinking about thething end to end    Awesome Awesome And by the way  big shout out tothe GCAP for bringing on thatsecurity advisory role So we ve got one last questionhere  and I m going to give youthis last question  which iswhat do you mean by shared fateand how is that different fromshared responsibility    That s a great question So shared responsibility hasreally unfortunately come tomean divvying upresponsibilities I m responsible for somethingand you re responsible forsomething else And so that is not thepartnership approach that Googlewants What we want is to be able tosay  well  we re sharing theresponsibility  and we resharing the fate from thatresponsibility So we re doing two things One is we re making sure thatwe re bringing in all the data like  to you in terms ofconfigurations that you mighthave in terms of secure landingzone or secure blueprints  andthen we also have risk managerto quantify and then be able toaddress your risk So all of that is what sharedfate is  where we re actuallybeing part of the process withthe customer    Awesome Thank you all for joining ustoday And big thanks to Sri  Tanesha and Marco for sharing theirinsights That s all we have for you onthe live show today Be sure to check out our greaton demand content Our live program kicks offtomorrow at 9A M  Pacific withour community spotlight Thanks for joining us  BRIAN SCHWARZ  Hi  I m Brian and welcome to our sessionat Google Cloud Next This session is going to beabout achieving high resiliencyand availability withGoogle Cloud infrastructure I m joined by my friend andcolleague  Chris Schilling We re both productmanagers at Googleand are excited to talkto you about this session The session today is goingto cover a number of topics First we re goingto talk about whyresiliency and high availabilitymatter in the Cloud a little bit about what GoogleCloud provides to customersas a foundation of theplatform  some of the thingsyou as a user ofGoogle Cloud needto take into account as youarchitect your solutionsand applicationson Google Cloud and then Chris is going to talka little bit about businesscontinuity planning  and it s agood idea for all applicationsand all systems  andthen we ll give yousome links about how tofind more information With this  I d like toturn it over to Christo get into the content CHRIS SCHILLING  Thanks  Brian Now let s talkabout why resiliencyand high availabilitymatter to technologistsbuilding in the Cloud Terms like resiliencycan be usedin a lot of differentcontexts in IT infrastructure We hear the term in thecontext of distributed systems context of databases  inthe context of networking When we think broadlyabout what definesresilient infrastructure there are two key components First off  thatinfrastructure needsto be able to handle failureswithout unexpected data loss If there is a networkpartition thatresults in the loss ofa node  for example a resilient distributeddatabase with many nodesshould be able torecover withouta split brain situation Similarly  if there is adisk failure in a resiliententerprise storagearray  the systemshould have copies of that dataavailable elsewhere to mitigateagainst data loss So that s the firstcomponent of resiliency Failure should notmean data loss The second component isthat resiliency needsmore than one layer of defense Just as infrastructurehas multiple components from software defined storageto the relational database to network and access security to business logic middle tier so too does resiliency Resilient infrastructurewill have defense bakedinto those layers and in certain layers multiple lines of defense If resiliency is afundamentally defensive conceptfocused on your data high availability on the other hand  is aboutyour end user s experience Highly availableapplications are stillavailable to the usereven when something goeswrong with the infrastructure That means that your usersshouldn t be adversely affectedwhen a failure occurs An application can be resilientwithout being highly available or it could be highlyavailable without resiliency We d all like to achieve both And that s why weat Google Cloudview these goals as linked If your goal ishigh availabilityfor a globalconsumer application you want to ensure thatan individual hardwarefailure in one ofyour cloud regionswould not impact yourcustomer s experience That almost always meansthinking about resiliency because data loss can be harmfulto your customers  as well Similarly  if yourgoal is resiliency you want to make sure thatall your defensive hardwork to protectagainst data lossand to create layersof defense will alsohave a direct businessimpact  ideallybecause yourcustomers never noticeany change to their experience even when things go wrong So those are linkedgoals  both resiliencyand high availability When we break down therequirements for this I think the firstone on our listis infrastructure durability It lines up more with thedefensive nature of resiliencybecause here we re talkingabout providing hardware SLAs we re talking about makingsure our data centers are safe and we re thinking about howwe respond to escalationswithin our network A good example ofinfrastructure durabilityis our public durability servicelevel objective for PersistentDisk  our block storage offeringFor regional balance PD for example  we share thatwe had better than 99 9999 durability  six nines Next up is applicationavailability That s something that veerstoward the high availabilitygoal  because we re thinkingabout how to keep a database upand running through yourtransactional applicationsor how to move traffic tothe right node or even regionwithin a distributedapplication A great product level exampleis our high availabilityconfiguration for Cloud SQL our managed database offering It lets you set up anarchitecture designedto limit user impact if theprimary database system fails Last but not least isbusiness continuity We view this as a corecomponent for both linked goalsbecause this is allabout planning  checking and double checking that youhave an infrastructure safetynet That means taking backupsto ensure resiliency and it also means testing youractive active disaster recoveryarchitecture to ensurehigh availabilityfor your application One product that wehave in this spaceis our multi region storage which stores your backup dataacross multiple Google Cloudregions in a given continent The result is that it s easyto restore in multiple GoogleCloud regions on that continent depending on what your businesscontinuity plan calls for What I would takeaway from all thisis that Google Cloud deliversinfrastructure durabilityto all our infrastructureas a service customers This is a default settingfor us  not an upchargeor an option If you want to build aresilient  highly availableapplication  we want totake care of the first step The next two steps  ensuringapplication availabilityand creating a businesscontinuity plan of attack those are equally important For them  we offer ourcustomers tools and servicesthat they can adopt tomeet specific goals We recognize that some tier onecustomer facing transactionalapplications willhave requirementsabove and beyond aninternal sandbox applicationwith a few users and nocustomer data  for example So let s talk aboutdurability first It s a great topicto kick us offbecause Google Cloudreally shines here All our customersget the benefitof infrastructuredurability thatunderpins some of the world sfavorite applications like YouTube And that starts with storagefor all types of applications Our block offering Persistent Disk our first  INAUDIBLE file offering  Filestore and our object storageoffering  Cloud Storage all count some of theworld s largest enterprisesas customers and users Regardless of whether you rea startup or a Fortune 100company  you can find the samepublic durability informationunder Persistent Diskofferings on our website All that informationis available publicly We recently published a blogpost to help customers furtherunderstand how weapproach this durabilityand what we do to keepyour infrastructure safe Similarly  becausewe re a service providerfor the infrastructure  we haveoperational safeguards in placeto make sure you have atrusted platform to build on That means having encryptionby default  for example so you don t have to worryabout accidental leakageof unencrypted data It also means having SiteReliability Engineering or SRE  coverage across agrowing global footprintand across all of our services Many of Google sSREs have contributedto the open literatureand public practicesthat help make all of us inproduct engineering betterat our operations You get the benefit of allthis knowledge and the benefitof having these SREs availableto address service level issueswhen you build on Google Cloud I ll hand the rest ofthe slide over to Brian BRIAN SCHWARZ  Thanks  Chris The other part ofinfrastructure durabilityreally comes from our globaldata centers and our network Our global data centersare significant We have 27 regionsand over 82 zonesbuilt out in manycontinents in the world and they offer a tremendousamount of on demand capacityand many layers of physicalsecurity in the datacenters themselves Of course  we ve layered ontop of this infrastructuresophisticated hardwaremonitoring  and all of theseare used to basically offerthe many managed services thatare part of theGoogle Cloud network And of course  the network  I always like to tell peoplethe Google Cloud network scalesand has high availabilitybecause Googleitself has scale andhigh availability Almost everybody in theworld has some appreciationfor the scale of Google sconsumer services You probably use some ofthese almost every day These same piecesof infrastructureand the same network is usedto provide the capabilitiesthat we offer in theGoogle Cloud platform And of course  we havesignificant investmentsin undersea cabling  edgePoPs  or Points Of Presence where we essentially arepeering into the internetand the network and of course  wehave thousands ofedge caches to makethe performance of Googleservices and  of course the performance of manyGoogle Cloud servicesincredibly performantand available The other thing I d liketo use is an exampleis our networking prowess andthe software defined networkingwe have in Google Cloudmakes it really easy for youto build on top of thisglobal infrastructure A specific example ofthat is the global VPCs or Virtual Private Clouds that you can build They are network constructsthat are very easy to set upand essentially spanGoogle Cloud regions So you can create one virtualnetwork that spans a region It makes it very easy for you tobuild global applications thattake advantage ofthis infrastructurethat we ve built Now let s talk aboutsome of the thingsthat it s importantfor you to thinkabout architecting asyou build applicationsand services on Google Cloud And it s important tothink about applicationsin terms of differenttypes of applications I often like to thinkabout  particularlyin a short session like this different types of applicationsthat would be like web serversand application servers often being stateless apps anda second set of applicationsbeing databases and verydurable apps  placeswhere all of your data lives And we re going to spend alittle time to talk about eachof those individually So if we look at the leftpart of the slide here the first thing I want totalk about is load balancing So the first two  load balancingand the GCE  or Google ComputeEngine  auto healingare really thingsto first think about when you rethinking about applicationservers and web servers  thatfirst category of applicationsI mentioned Load balancers essentiallyredirect traffic awayfrom unresponsive servers unresponsive web serversor application server They may be too busy It might be a software defect There could be alot of reasons whya node becomes unresponsive Load balancing essentiallywill redirect traffic awayfrom these unresponsivenodes so your customersand users of the applicationsget a great experience There are different options onhow you deploy a load balancer You can deploy it toessentially give capabilitiesto span zones and regions formore local load balancing There s also optionsto essentially doglobal load balancing So all of yourconsumers and userscan come into one singlepoint  and the applicationmay actually be spreadacross regions  so evenfull region outages will behidden from your customers They ll be redirected tosurviving  healthy nodes The second layer ofdefense   as Chrismentioned  it s good to havemultiple layers of defensefor applicationsand web servers   isto think about auto healing So in GoogleCompute Engine  thisis our main offering thatessentially offers VMs Many  many applicationsrun in these VMs and there s a constructthat we ve developedcalled managed instance groups It s essentially a set ofVMs that form an applicationtier in their environment They can essentially providea second layer of protectionand availability and  essentially they will automaticallyrestart unresponsive nodes So a node may be too busy It may get somehowlocked in some statethat it can t recover from The load balancer willshift traffic away from it and then the GCE autohealing will come into essentiallyrestart those nodes So again  furtherrehealing the system An important thingto think aboutas you start to buildthese layers of defenseis how they work together And an example isyou essentiallywant the load balancerto redirect traffic awayfrom an unresponsive nodebefore it gets restarted So as a perfect example you want the health checksthat are in yourload balancer to bemore frequent and aggressive and reshift trafficaway from unresponsivenodes  and thenthe managed instance group autohealing feature to kick in alsohas health checks to workslightly slower to actually goin and restart nodes That way the systemis  in some senses self healing or auto healing The second thing Iwant to talk aboutis more on thestateful applicationtier  which is why I ve talkedabout Cloud SQL and Spanner So Chris mentionedCloud SQL earlier You could of course runa database inside of oneof our VMs Many customerschoose to do that But it s also great tothink about managed databaseofferings to reducesome of the toilabout setting up andrunning a database server And that s really whereCloud SQL comes in It s a managed databaseservice we have It supports a lot of commondatabases  MySQL  PostgreSQL SQL Server  and it has a numberof interesting capabilitiesthat really add value inthe forms of durabilityand resilience andhigh availability One is usingsnapshot based backups You can automate it  soit takes regular backups Let s say every four hours youhave good protection points But you can also use itin on demand snapshotsif you re going todo a maintenancetask on your database add tables  changethe scheme  et cetera You can create a snapshot tocreate an easy recovery point The managed databaseservice alsohas a built incapability to offer youpoint in time recoveryfor your databases So you have snapshots  andthen you can essentiallymove the database to aparticular point in timebased on the logs thatare in the database So it s a combination ofthe managed database serviceworking in conjunctionwith snapshotsto offer you acapability to reallybe specific at exactlythe point wherethe database isconsistent and you wantto restart the application It s also great to thinkabout the high availabilitycapabilities in theCloud SQL offering because it offersfailover replicas These are essentially shadowdatabases that can be set up There are options inside theservice that can essentiallyrun in multiple zones toprotect you from a zone outage These zonal things can usethe regional PD offering the regional Persistent Diskoffering that Chris mentioned so we re synchronouslyreplicating the datafor the databaseunderneath the covers And then you essentiallyhave a Cloud SQL instancethat s ready to go in theevent of a zonal outage There s also acapability in Cloud SQLto essentially do crossregion protection This is  of course  anasynchronous capability but it can also protect youagainst full regional outages So  again  there s a lotof power and flexibilityin terms of theoffering that Cloud SQLhas in terms ofbuilding in resilienceand high availability The last thing I wantedto think about or talkto you about inthe database tieris our Spanneroffering  Cloud Spanner This is a reallypowerful capability It s highly differentiated It really comes fromtechnology that Googlebuilt to scale to thebillions of users we haveon our consumer applications It s a fully manageddatabase service so it s very easy to use But it s basically a singlecross regional database It s active active andread write in many regionsall at the same time  and that sreally what makes it unique So it preserves thevalue of standard SQL because that s such a commonapplication programminglanguage  but it provides youessentially  in some senses you can think about across regional databaseas a continent sized databasefor you to operate on So it really allows youto simplify how you thinkabout building these systems And it comes with afive nine availability SLA so it s really apowerful capabilityto have in the arsenal asyou think about how you buildapplications on Google Cloud Now I d like to turn it overto Chris for the next part CHRIS SCHILLING  Muchappreciate it  Brian So let s switch backto business continuity the insurance policy  the safetynet that we all know we need Business continuity isa pretty expansive term It can mean a lot of thingsfrom how a cloud architect plansfor disaster production  INAUDIBLE   to how a storageadministrator tests backups You ll noticed I mentionedplanning and testing rightoff the bat  and we have anumber of features and servicesbuilt for businesscontinuity purposes  featureslike being able to exporta copy of your databaseto our object storage forlong term    archival    for example But business continuityis about being absolutelycertain that the datawill be availablewhen the auditor calls That s why the human elementof planning and testingis so important here As the saying goes  anamateur takes a backup A professional teststhat it can be restored So when we dial intoa couple of services two of interesting onesthat I find differentiatedare our multi region storageand our orchestrated backup They play different roles inour business continuity package Multi regional storagecan mean keeping datain our object storage We ll keep twocopies of the datain two differentregions on a continent available in case of aregional failure or disaster This also underpins someof our other offerings It makes it possible  forexample  for our customersto restore snapshotsstored in NorthAmerica to differentNorth American regionswithout having to createa handful of copiesacross the continent and waitfor them to be replicated oneafter another And on the orchestratedbackup side we recently acquired Actifio a proven backup vendorthat sells Actifio GO  a backupas a service offering builton Google Cloud Actifio GO can protectGoogle Compute Engine VMsif you re building in the Cloud Or if you re looking atshifting relational databasesinto Google VMs and youneed application consistent agent based backups similarto what you have on prem Actifio GO can deliver thesame functionality to you allowing you to backup yournewly migrated databaseworkloads as frequentlyas every 15 minutes Best of all  that data can bestored in multiple Google Cloudobject storage buckets indifferent regions and classes and it can be restored directlyfrom object storage  whichmeans lower long termstorage costs for customerswith severe dataretention requirementsand easier discussions withyour internal compliancecounterparts when you testthat a backup is available for example  without the needto complicate restores or movedata around to differentblock storage options These are just twoof our featuresthat we ve created to makebusiness continuity easierfor you to plan and to testso that you and your teamcan achieve thosegoals around resiliencyand high adaptability If you d like tolearn more  we veincluded a couple of linksto some interesting contenton disaster recovery  highavailability  and resilience In the bottom right there s a linkto achieving resiliency andhigh availability in Cloud SQL It s a video fromlast year s Next and it covers alot of the topicswe ve discussed here as theyapply to that managed databaseservice Just above that  we have aDR planning guide for GCE Those of you who are runningstateful workloads and GoogleCloud VMs  we think you llfind that useful to help youplan for your DR requirements Over on the left  we havea few sessions of Nextthat we think you llfind relevant We ve included their titlesand the shorthand namewith numbers so you canfind those in the catalog I want to thank Brian forco presenting with me and I want to thank all ofyou for attending our session SANDER BOGDAN  Welcome toGoogle Cloud Next 2021 In today s talk Secure and ReliableContinuous Delivery toGoogle Kubernetes Engine In today s talk we re going to talkthrough some of the challengesof continuous delivery to GKE We ll discuss theprinciples thatcan be applied toaddress those challenges We ll be introducingGoogle Cloud Deploy And finally  we ll show a demo My name is Sander Bogdan and I m a product managerwith Google Cloud And I m joined today by BryanMorgan  who s an engineeringmanager with Google Cloud So let s start off bytalking about challenges Over the past 18 months we ve had the opportunityto speak with numerousGoogle Cloud customersabout the continuous deliverychallenges they face with GKE And from those conversations three themes emerge The first theme iscost of ownership Defining robustdelivery pipelines patching  pooling  and scalingan organizational continuousdelivery capability can beboth difficult and complex And even when an organizationalcontinuous delivery capabilityhas been established knowing what to measureand how to optimize thosemeasurements for successis often unclear But in all situations continuous delivery investmentintensity can detractfrom core business focus So the second theme we observedwas around security and audit Controlling releases asthey make their way outto a productionenvironment  approvingreleases for production  aswell as delivery pipeline accesscontrol and audit wererepeated as fundamentallynecessary to modernsoftware delivery And the final themewas integrations Continuous delivery capabilityexists within software deliveryecosystems And because of that systems integrationssuch as with continuousintegration  connectingcomplex workflows such asmulti step  multi partyapprovals  and advancednetworking requirementsdemand continuous deliverytooling flexibility as well asmodularity And these three themes are bestsummed up by recent customerquote  and that quotewent something like this  We cannot afford to beinnovating in continuousdelivery We want an opinionatedproduct that providesbest practices out of box  And while this quote isassociated with one customer the reality is itcould be assignedto any number of customerconversations that we ve had All right Now that we ve talkedabout the challenges I d like to talk throughsome of the principles thatcan be applied toaddressing these challenges So there are four The first principle is Easy  make it easy to definerobust delivery pipelines to onboard deliverypipelines  scale them and to apply best practicesas part of their setup And we believe the best way todo this is through a managedopinionated service and Bryan s goingto be talking moreabout that later The second principle is Control Continuous deliverytooling must facilitatemanual and automatedrelease controlas it makes its way out througha production environment So that includes activitieslike promotion  rollback as well as approvals The third principle is Security And what that means isplatform administratorsmust have the ability torestrict delivery pipelinedefinition and activities They need to be able tocontrol who is doing what where  and when And when an auditis needed  thatshould be handily available too And the final principle isabout providing insights So continuousdelivery capabilitymust provide insightsboth collectivelyas well as individually fordelivery pipeline progressions activities  and opportunitiesto further optimize deliverystability as wellas release velocity So again  the fourprinciples aremake it easy   to makeit easy to onboard make it easy toscale  and make iteasy to maintaincontinuous delivery toolingusing best practices The second is provide control Releases should be controllableas they make their way outthrough a productionenvironment The third principleis security   controlwho can do what  where and when with audit And finally  provideinsights to further optimizeyour continuousdelivery practices So now that I ve talkedthrough these principles I d like to handit over to Bryan who s going to talk tohow Google Cloud appliesthese principles Bryan BRYAN MORGAN  Thanks  Sander Continuous deliverycapabilities frequentlyexist within a softwaredevelopment lifecycleecosystem This implies flexibility andmodularity  as Sander noted as a deliverytooling requirement For GCP customers  connectingGoogle and third party toolingthat intersects withyour delivery processesis a necessity  particularlywhen you alreadyhave continuous integration approval workflows and associated test automation We have found that GCPKubernetes customers typicallymature in their usageof GKE over time Starting out  customers mayhave a single config thattargets a singlecluster  but this usuallygrows into multi config multi cluster usage From there  teams often findthat application configurationshould be standardizedand sourcecode controlled  allowing themto templatize configurationchanges Finally  advanced customers findtools such as KEP and Kustomizeto be extremelyuseful in allowingthem to make increasinglysophisticated and automatedconfiguration changesacross environments Most software deliverypipelines can be thoughtof as a three step process  define  manage  and deliver Because usage ofGKE is a journey we advocate using Skaffold forthe local development loop From there  Skaffoldprofiles  in combinationwith the managed continuousdelivery service can be used to define deliverypipelines and orchestratereleases Finally  that servicecan actuate the releaseto GKE target clusters Up until this point  we vediscussed the challenges principles  and applicationof those principlesfor continuousdelivery into GKE Now  I d like to stepthrough the key featuresof our recent public previewlaunch of Google CloudDeploy  GCP s continuousdelivery service for GoogleKubernetes Engine We have engineeredGoogle Cloud Deployto be a scalable managedservice that enablescontinuous delivery to GKE Cloud Deploy relies ona ConfigOS code modelwith declarative manifestsfor configuring the deliverypipelines In addition  it performsGKE manifest renderingand deployment on your behalf Cloud Deploy additionallysupports a numberof security features including a discrete IEMmodel  direct integrationto GCP Cloud audit logs manual approvals as part ofa deployment process flow and even execution onCloud Build private poolsfor customers that desiresecurity and controlover their environments Finally  Cloud Deployhas integration pointsto enable both Google andthird party delivery relatedtooling and processintegrations I d like to briefly walkyou through the experiencebefore I go into our demo In this first slide  you cansee that Google Cloud Deployorganizes CD workflowsaround pipelines which can bemonitored and measuredusing the GCP Cloudconsole as wellas our gcloud command lineinterface and API Next  in the DeliveryPipeline Details view a pipeline s rolloutscan be controlledthrough promotionsand approvals  whichare again availablevia the Cloud ConsoleUI and the gcloud CLI You ll also note adelivery pipelines releasehistory is accessiblefor similar reviewand taking action Users with appropriatepermissionscan then proceed witha release promotionto evaluate configurationchanges thatare included with therelease to be deployed The Cloud Console userinterface supportsdirect visual differentials toaid in the release progressiondecision making process I d like to now demonstratethe use of Google Cloud Deployto deploy an application to aseries of GKE target clusters In this demo  I willstep through the processof creating a GCP Cloud Deployrelease  deploying to staging then approving aproduction rollout Today  I m going to demothe new Cloud Deployservice from Google We ll start witha GitHub repo thatcontains two applications thatwill build two containers That application is inthis example directory and you ll seethere s a containercalled leeroy app and a secondcontainer called leeroy web Both containers include theirown Kubernetes deploymentmanifest as well asa Skaffold configthat defines howthose containers willbe built and deployed You can see herethat we re planningto use the standard kubectldplyr to deploy the two GKEclusters If I take a look at GKEand my Cloud console you can see that I ve alreadypre created three clusters  a test  staging  and prod So those are alreadyready and waiting for me So the first step I want todo is to go into Cloud Shell I ve already pulledthe repo down So I want to do a Skaffoldbuild and upload   buildingwill upload those containersto Artifact Registry All right We can see the containerbuild is finished They have been uploaded I m going to go check outmy Artifact Registry justto verify Yep We can see now justnow two containershave been uploaded leeroy app and leeroy web So for my next step  Iwant to actually now set upmy Cloud Deploy pipeline My containers arebuilt  They are ready So let s take a look atwhat that looks like Within my repo  you cansee I have a declarativemanifest for my pipeline And it looks just likethis  and it s very simple I give it a name  whichI ve called it  web app with a description And then it really justdefines three stages  test  staging  and prod The order in whichthese are defined inare basically thedirectional flowthat the pipelinewill promote through So let me go intomy Cloud Shell and I m going to apply thatmanifest to Cloud Deploy All right You can see it snow been applied I can go check out my CloudDeploy user interface I m going to do a refresh here All right Now  we have a web pipeline and it s   as we mentioned it s planning to promotefrom test to staging to prod Those are actually resources aswell that need to be created And you can see I have thosedefined declaratively as well I have a target test target staging and target prod I ve defined those inseparate manifests So I m going to runthose  and now thatwill create each of thesetargets in Cloud Deploy oneat a time All of these could have beenadded to a single manifest I just opted not to All right So I ve created the testtarget  the staging target and the production target At this point  I coulddo one of two things The first step I ll takeis just to manually createa release in Cloud Deploy So I m going to go ahead andcreate our first release And we re going tocall it  web app 001  And you can see now thatwe ve created the release and it started a rolloutto the first stagein our pipeline which we call test If I take a look atthat  you can seethe rollout has been queued It s starting And this is what the userinterface looks like We show that there s atest  a staging  and a prod Rollout is queued If you notice here between staging and prod it says  0 pending  And what that s referringto is zero approvals And it knows that because ifwe take a look at our prodmanifest  we configuredit to require approval So we ll talk aboutapprovals in a little bit But effectively Cloud Deploy is aware and it s expecting an approvalbefore it allows that promotionto that final target While this pipeline isqueued  we re going to go in and I ll show youas well that I vecreated a Cloud Build triggerthat will automaticallylook for changes And this is   for those thatare familiar with Cloud Build this is looking at my Git repo And any time a change happensto that repo  a commit is made It s going to builda new container upload it to Artifact Registry And then you can seehere  the second stepis it s going to thencreate a new release So we can test that out I will go into myVisual Studio code I have a sample app here Let s just call it Cloud Next 2021  We re going to update thecomment here  apply the change  Updated comment  We can see thattrigger is running now And what that triggeris going to do it s going to update thecontainers in the ArtifactRegistry And then whencomplete  it s goingto kick off a brand newrelease in Cloud Deploy OK Our trigger hassuccessfully run which should mean that a CloudDeploy release has been kickedoff  and we see that it has The test has beendeployed  and nowlet s say we wantto do a promotion So we re going to go aheadusing the user interfaceand force the promotion to  kick off a promotionto the staging target OK Our promotion tostaging was successful So now we reallyhave just one stepleft   that s topromote to production But if you recall production alsodoes require a manual approval So we re going to kickoff the promotion And you can see now wehave one pending review I ll click the Review option I m going to go in there and I can see that  oh  look I have a rollout thatdoes need an approval I do have theappropriate permissions so I can go in thereand review that At this point  I mready to approve it I know what changed I can see the differences And so I m going to approve I m going to goback to my pipeline Our promotion toprod was successful And you can see nowwe have successfullypromoted an automated releasethrough test  staging and production This release was builtoff of the GitHubtrigger using Cloud Build And this really showcasesthe end to end CI CD pipelineusing both Cloud Buildand Cloud Deploy To learn more aboutGoogle Cloud Deploy visit us atcloud google com deploy Now back to you  Sander SANDER BOGDAN  I d liketo thank you  Bryan for presenting thistalk with me today as well as all of you forattending Google Cloud Next2021 ANDY QIN  Hello  everyone Welcome to APP 101 breakout Today we d like tointroduce Google DistributedCloud  Hosting Mode My name is Andy Qin I m a product managerin Google Cloud I m here with my colleague Ray RAY COLLINE  Hi there I m Ray Colline And I m a software engineer who works on Google Cloud And it s great to be here ANDY QIN  In thissession  we ll firstlook at market pain points and the limitations of existingsolutions And then we willintroduce Google solutionin terms of how it sdefined  and what sthe key streams andthe characteristics And we will then divedeep into the productareas  like platform services  and support Today the public clouds provideamazing results and benefits So it seems everything ismoving to public Cloud But the reality isthat not all workloadscan move to public Cloudfor different reasons regulatory  IP protection sovereignty concerns Those workloads thatstayed private  it svery challengingfor them to leverageall the cool innovationfrom Cloud technologiesto empower their business and  oftentimes  thoseare very special and specificpart of their business As we in public cloudsthe infrastructureis quite transparent Users do not need to care aboutthe gory details of thingslike hardware operation However  that s not thecase for private Cloud In the legacy privateCloud customersstill have tospend a lot of timemanaging their infrastructure And most hobbyists pinpointedthat customers will end upwith a public Cloudand private Cloudwith different developmentmodels and toolings and the resulted inconsistencyimpacts business agility The market painpoints are very clear but if you look at theexisting solutions outin the marketplace they didn taddress all those pain points As we know  some products areviewed on proprietary stack There s no consistency betweenthat and the public Cloud Applications developed inprivate mode cannot be easilyported to the public Cloud It s very difficultto achieve write once and run anywhere  INAUDIBLE  Some private Cloud productsa very infrastructure serviceoriented No advanced Cloud innovationslike AI ML services and dataanalytics And the managementmethodologies are very legacy For example  the moredeclarative policy paradigmis quite common in publicCloud  but most privateCloud are still usingthe legacy imperativetools to match their systems Some private Cloud productsdo meet most requirements However  they requireconnections backto their Cloud region for thequintupling to even function This poses a huge challengefor sensitive workloads especially in thespecific market segments like defense andcritical infrastructure Google Cloud recognizesthose market pain pointsand the clear gapsin the marketplace We are going to offer asolution that can directlysolve those challenges The solution is called GoogleDistributed Cloud Hosted Mode There s three key things tohighlight on this solution First  it doesn trequire connections backto public Cloud All quintupling  data plane match plane can be local It helps you meetthe sovereigntyand the regulatory requirements Second  it comes withhighly differentiatedadvanced services  likeAI ML and the data analytics We believe those servicesare critical to empowerthe customer to staycompetitive in their own market And the third  it leveragesopen source and open API This provides an automatedportability and softwaresurvivability  which are superimportant to both businessagility and thesovereignty needs At this particularpoint in time the Google DistributedCloud Hosted Modeis still a pre GA product We are working hard to bringthis to the market soon Besides the threekey things  I thinkit s important to lookat the characteristicsand the key strengths of GoogleDistributed Cloud Hosted Mode The name Hosted Mode tells alot about location and control The solution can be deployedat a customer designated datacenter  even in the totallyair gapped environment This solution is not justthe infrastructure play it comes with Google advancedservices  like AI ML The power of those technologiesand Google s years of expertiseare no longer limitedto the public Cloud with Google DistributedCloud Hosted Mode customers can now enjoythose on their own premise Another strengthworth highlightingis that the entiresolution is flexible It comes as pre assembledand preconfigured racksand is delivered to yourdata center as a service It can scale from 1rack  to a couple racks and to hundreds of racks It has localquintupling built in so it covers all sovereigntydimensions  data sovereignty operational sovereignty and software sovereignty While it s builtby Google Cloud so it inherits the Cloud DNA Open and the Modern arethe design philosophieswe follow when webuild the solution We believe the solutionwill meet the needs of manyindustries  where they havea sensitive workload and theycannot move to the public Cloud but they still want to leveragethe modern innovationfrom Cloud Public sector is a primeexample of those industries With that  let meinvite my colleagueRay to talk about whatexactly is in the solutionand what are thekey capabilities RAY COLLINE  Thank you  Andy We re going to get into a littlebit of the technology thatmakes up Google distributedCloud Hosted Mode And we ll start with thelowest level  the hardware So at Google DistributedCloud Hosted Modeis full racks of hardwareoptimized to deliveran air gapped Cloud experience These racks includemodern hardwarefrom top OEM providers including HPE  NetApp and Cisco All of theinfrastructure managementis taken care of byGoogle Distributed Cloud These are things likeoperating system upgrades firmware upgrades  switch OSupgrades  NetApp upgrades et cetera All are taken care of as partof Google Distributed Cloud It also includes GPUsfor AI ML workloads And it includes storage block  or sorry  block  file and object storagein a redundant manner It also includes highperformance networking And in fact  each nodehas full bandwidthto all the other nodeswithin a GDC instance and these instances canbe up to 10 000 VCPUs In addition  it sKubernetes all the way down So Andy spoke earlierabout it being open  right We ve taken that and we veintroduced Kubernetes APIsall the way down into theinfrastructure management layerso that the notionof an OS upgradeis actually controlled throughthe reliability of Kubernetes Google DistributedCloud Hosted Modeis a full air gappedCloud solution stand alonefrom any other Cloud and so it comeswith sophisticatedmulti tendency As we know in publicclouds things like GCPhave the notion oforganizations and projects and one creates a projectbefore doing anything on GPC creating a VM  creating acluster  creating a database Within Amazon  one creates anaccount to do the same thing Well  GDC introduces the notionof fleets and fleet namespacesto provide that level ofresource management and tenancywithin a GPC instance Fleets are hardwareseparated from each other allowing a single GPC instanceto serve multiple applicationswith good securityguarantees between them And fleet namespaces like Amazon accountsand GCP projects provide isolationbetween teams andcomponents of applications Policy control is builtin from the beginning As we said earlier  we reKubernetes all the way down So all the APIs benefit fromhaving Anthos policy controllerbased on OPA gatekeeper and Kubernetes emissioncontrol  such that onecan govern all aspectsof the usage  and also audit This means you could for example  take a VMand say no VMs can everhave the name Ray in it and we could enforce such apolicy at that granularity It s also extensibleand customizable So Kubernetes APIs allowfor custom controllersto do any sorts of things We ve had customersbuild controllersthat disallowlaunching of new codeunless there s anassociated launchprocess that s been approved And Google DistributedCloud Hosted Modeis a rich and open ecosystem Because it s built onstandard Kubernetes one could use the entirecommunity of softwareavailable to operateKubernetes within GPC GDC comes with muchsecurity and confidentialityfeatures built in A local certificateauthority backed by an HSMis provided out of the box This allows GDC to meet FIPS140 2 level 3 compliance In addition  GDCcan be configuredto work with an existing HSMprovided by the customer Secrets management is providedusing the common secrets API but it s backed by theHSM to make it secure Also  GDC benefits fromGoogle s investmentin making GKE secure  andall of the enhancements putinto GKE so far  plusGoogle s contributionsinto the Kubernetes community Identity and accessmanagement issupported using standardprotocols like OADC and SAML2 0  meaning itcan be configuredto work with existingidentity providersthat our customers have And because it s Kubernetesall the way down every aspect of the systemis managed by Kubernetes rolebased access control If you know GCPE you will understandthat IM is it s part of everycontrol surface within GoogleCloud platform Well within GoogleDistributed Cloud Kubernetes RBAC performs andoperates at that same level providing fine grainedcontrol over all resources Additionally  supply chainsecurity  a top concern is something thatwe ve been working hardboth on the hardwareand software side We ve been working with Tier1 providers in each region and we require stronghardware supply chain securityas part of the GCD OEM process Additionally  the GKE andour Google security teamshave been ensuring softwaresupply chain securityin the Kubernetes OSSstacks for years and years And we ve mademany contributionsto making Kubernetes asafe  reliable systemto run workloads on And lastly  everything you door perform on Google DistributedCloud Hosted Mode is audited And these audit logs arecompliant with the NIST 853standards Google DistributedCloud Hosted Modecomes with avariety of services And because it sbuilt on Kubernetes anything out in theopen source communitythat runs onKubernetes will alsorun on Google Distributed Cloud We are also working withthird party OEM providersto provide enterprise versionsof popular open sourcepackages enabling firstclass support and enterprisefeatures Lastly  I m going to chatabout operations and support a key concern whenoperating a Cloud Google DistributedCloud Hosted Modehas a few differentoperational patternsto support customer needs which Andy will cover later But importantly  the bringup in deployment of GDCwill be facilitated byGoogle every step of the way As mentioned earlier  allthe operational aspectsare provided andoperated by Google meaning thatcustomers do not haveto worry about patching and OSupgrades  BIOS  firmware  CVEresponse  et cetera  et cetera Instead  Google makesthis largely push button or can even provide thesupport and operationsto do that for you Additionally  theability to support an airgapped deployments Google engineeringis ensuring thatevery area comeswith full detailed supportand training resources As an anecdote within theengineering organizationfor GDC  any pullrequest that s submittedmust come with metadata forsupport operations  whichmakes engineers thinkabout the full lifecycleup front  providinga higher reliabilityand operable system And regardingsupport  Google GDCcan be used and trusted inair gapped environments Google has developednew support techniquesand operational techniquesderived from Google SRE bestpractices  allowing GDC tomaintain a reliable SLO while also providingtrusted usersupport without requiring anydata to leave the GDC instance Importantly for alloperations  Googleis the only vendor acustomer has to call And lastly localized management So in addition to local UI local APIs  control plane everything being on aspart of the GDC instancein air gapped orother environments documentation andaccess to supportis also built in meaning that users do notneed any outside accessto operate and use GDC Now  in rare cases  ifthe customer desiresfaster support  wewill offer a wayto send sanitizedlogs  so they couldbe shared with the coreengineering teams  increasingsupport turnaround With that  I m going topass the baton back to Andy Thank you very much ANDY QIN  Thank you  Ray To sum it up  GoogleDistributed Cloud Hosted Modegives you the power of Cloudinnovation with control And it covers allaspects of sovereignty Data is on customer premiseor any data center designatedby the customer It can be operated by a customerdirectly or by an operatingpartner that has the rightcredentials and the trustfrom the end customer Google will trainand work closelywith those chosen partnersto ensure the serviceavailability In certain scenarios  Googlecan also operate the deploymentdirectly with cleared andcertified operational teams Its build withopen API  and meansa lot when customerswant to ensurebusiness continuity in anypotential black swan events And it definitely helpsto meet local government sregulatory requirements Google DistributedCloud Hosted Mode  RAY COLLINE    the benefitsof Cloud delivered locally ANDY QIN  Thisconcludes our session Thank you for listening Thank you  Ray  for beingpart of this conversation RAY COLLINE  Thank youfor having me  Andy And I hope all of you have agreat rest of the time at Next MAGDA JARY  Welcome to thecertification preparationProfessional Security Engineerand Professional NetworkEngineer certification My name is Magda  andI m the global leadfor the Google CloudCertification and DigitalBadges Go to Market team And joining me today is CoriPeele  customer engineerat Google Cloud Let s take a lookat our agenda today First  we ll review thecertification benefits Then we ll review the learningresources and the Google CloudCertification s portfolio We ll then talk with Coriabout her certificationand preparation journey And then we ll recap withthe additional resources So let s get started withthe certification benefits First of all  cloudskills are reallycritical to enablingcustomer successand fueling our ecosystem The job posting lookingfor cloud skillsgrew over 40  betweenthe first quarter of 2019and the first quarter of 2021 Secondly  over 90 of IT leaders expectto expand cloud services inthe next one to three years but over 80  of them identifieda lack of internal skillsand knowledge as the topbarriers to cloud success And then lastly  theGoogle Cloud skillsare particularly inhigh demand  commandingthe top two paying ITcertifications in 2021in the US We also love to hear fromour community about the CloudCertification Program impact 78  of our survey respondentsfeel more confidentin their professional future 82  can prove cloud skillscompetency to recruiters 83  feel that theirresume is more attractive And then finally  85  feelmore confident in cloud skills Here are some ofthe benefits youcan enjoy by joining thecertification communityand achieving yourcertification First  you distinguish yourselfwith a Google Cloud Certifiedbadge and a certificate You also receive exclusiveGoogle Cloud Certified swagfor our professional exams You can network and exchangeideas in the Google CloudCertified community And you also receive specialrecognition at select events Let s review some ofthe learning resourcesand our Google Cloudcertifications Here are some ofthe modalities thatreally help us build skills forthe future with Google CloudLearning  starting with theinstructor led format deliveredby our authorizedtraining partners Then we also have a modalityof the on demand trainingso you can learn any time by theuse of the on demand content The third piece isreally the hands on labsand is really all aboutlearning by doing So you can practicethere directlywith Google Cloud technologies And lastly  our GoogleCloud Certified programthat really provides a frameworkand benchmark for successin core job roles Our exams are proficiencyand role basedand provide you abenchmark for success And let s take a look at theoverall portfolio of our GoogleCloud certifications You ll notice we have the examsat the professional level  aswell as our associatecloud engineer and the very recentlylaunched cloud digital leader This final one is forthe business audience Otherwise  our associateand professional examsare all job role basedand really caterto the technical audience Today in our session  we focuson the professional cloudsecurity engineer and theprofessional cloud networkengineer So let s start with thereview of the readiness pathfor the networking andsecurity network engineer role You can notice the resourcesin the Learn section then some hands on practiceresources with skill badges and then some additionalpreparation resources You can find allof these resourceson our externaltraining website Specifically reviewing theprofessional cloud networkengineer role  let s reallyhave a better understandingwhat this role is really about A professional cloudnetwork engineerimplements and manages networkarchitectures in Google Cloud This individual may work onnetworking or cloud teamswith architects who designcloud infrastructure The cloud network engineer usesthe Google Cloud Console and orcommand line interfaceand leverages experiencewith network services application  and containernetworking  hybrid andmulticloud connectivity implementing VPCs and securityfor established networkarchitectures toensure successful cloudimplementations And you can review our exampage for further details On the exam page you ll also notice what are the core domainscovered by the exam And it s all available in thecertification exam guide that savailable on the website Likewise  let s reviewthe preparation pathfor the securityengineer certification You can note that thelearning resources  trainings then in the practice section the hands on labs and skillbadges  some of thepreparation resources like sample questions  and thenfinally  the certification So likewise  who is theprofessional security engineer This person enablesorganizationsto design and implement a secureinfrastructure on Google Cloud Through an understandingof security best practicesand industry securityrequirements this individualdesigns  develops and manages a secureinfrastructure leveragingGoogle security technologies The cloud security professionalshould be proficientin all aspects ofcloud security including managing identityand access management defining organizationalstructure and policies using Google technologiesto provide data protection configuring networksecurity defenses collecting and analyzingGoogle Cloud logs managing incident reports  anddemonstrating an understandingof regulatory concerns All of thisavailable on our examwebsite  including the detaileddomains that the securityengineer exam covers So very briefly  a little bitmore information about theseexams They are two hourslong  multiple choice and multiple select There are no prerequisites However  we strongly recommendyou have around three yearsof industry experience including a year  at least of hands on GCP knowledge So with this  I m reallydelighted to welcome Cori Hello  Cori CORI PEELE  Hi  Magda How are you today MAGDA JARY  Really good Great to see you So we would love to getyour insights  expertiseon the topic of the securityand network engineercertifications So if you couldstart with lettingus know about your rolein some of the projectsyou are working on CORI PEELE  Yes  thanks I m a customer engineerwith Google Cloud And in this role  I m ableto work with customersto understand theirbusiness requirements some of the things they maywant to do to maybe transformtheir industry  andI work with themto understand how theycan accomplish someof those goals usingGoogle Cloud and partsof Google Cloud s ecosystem And in order to dothis  I need to havea really good understandingof what we haveavailable in terms of features But not just that It s really focusing on  howdo you solve these businessproblems with the rightamount of emphasison performance for the rightprice at the right time And by doing this withsome of my customers  and some of you may be seeingsome of them speak duringGoogle Next  we are able to accomplishreally meaningful and impactfulaccomplishments  notjust for the customer but also for theusers they serve And in that way  theyall have a bit of a focuson improving our overallexperience in the world today So it s extremely rewarding MAGDA JARY  That sgreat to hear Thank you  Cori So please tell us about yourcertification preparationexperience CORI PEELE  Yes So in my case  I hadseveral certifications However  I was focusingon how to studywhere I m optimizing my time Because when you think aboutthe certification journeysand you look at theexam guides  youwill see that there arecertain common recommendationsacross networking and security So having a reallysolid understandingof the foundationalinfrastructurecomponents of Google Cloudwill be the first thing And so we have the ability touse instructor led trainingor on demand training throughPluralsight or Coursera So I mapped out thecourses  and I alsoleveraged thecertification pathsthat have been outlined byour certification leads And so by doing this  I wasable to go through the trainingin terms of on demand training I had also mappedout the questionsthat were maybegiving me some troublewhen I took the practice exam And this is extremely useful because the Google Cloudpractice exam gives you notonly an explanation regardingeach answer  but it givesyou links to resourcesfor further information So after doingthat practice examand understanding thatthere are other areasto focus on a bit more  Iwas able to really researchand make sure I had a solidunderstanding of those topics as well I also participated in communityactivities  where several of usare embarking on acertification journey together And this helps  because you reable to ask questions and alsobenefit fromcommunicating with others And so in this case once I felt comfortablethat I was able to answerthe questions across allof the sectionsof the exam guide then I registered for theexam and I was able to pass So I m very fortunate tohave several certifications including the securityand networking engineercertifications MAGDA JARY  Wonderful and congratulations  Cori CORI PEELE  Thanks MAGDA JARY  So howwould you encouragesomeone brand new to thesecurity or network engineeringarea to set a goal ofachieving a certification CORI PEELE  Yes  I would reallyencourage you to think abouthow this benefits yourday to day activities in addition to how it may impactyour company and your growthin your organization as a whole The things you are doingtoday in your environmentmight be focused on maybe acouple of different patternsand a few differentarchitectures based on what you have inyour environment today However  in order toreally be preparedfor different typesof challenges and to really have asolid understanding of howyou can apply thesetypes of technologiesto other business problems it s important to havea very detailed understandingand a broad understandingof a lot of the topics thatare covered in the exam And so I really wouldencourage you to thinkabout achieving certification Think about planning   andI would say think of itin terms of maybe in 12weeks  so within one quarter You may start your studying inthe beginning of the quarterand then plan to have anexam taken towards the end So that way  you reable to incorporatestudying for the exam withall the other things you reprobably doing at the same time And so with thattype of structure it s fairly reasonablefor you to beable to really havethat goal establishedand then also achieve the goal And don t worry if youdon t pass the first time You have the abilityto retake the exam So again  the focuson establishingthat type of target goal  andthen planning your scheduleaccordingly so you canmeet that timeline MAGDA JARY  Thank you  andthank you for all the tips Cori  could you review some ofthe synergies between securityand networking in Google Cloud CORI PEELE  Yes  and thereare a lot of synergies And actually  when you thinkabout what the cloud is we are thinking about theresources that are networked And so network securityis very important And you will find that thereare networking and securitytopics on both exams So one really importantperspective we have at Googleis to think abouta zero trust model We ve published a whitepaper called Beyond Corp And this is just adifferent way of thinkingabout managing security andnetwork security for those whoare used to the world whereyou have a demilitarized zone and you re allowingcommunications across the boardas soon as someonehas permissionto get into that perimeter Well  with zerotrust  this meansthat even though you maybe in the same network you do not have access So this is  again  nothing  no access unlessyou are granted thatby a policy explicitlyor something whereyou are inheriting it So being able to applythese types of principlesacross multiple products in thesecurity and networking spacewill be really important Also  it s important tothink about how we approachopen source software security So we have a lot ofdifferent mechanismsto plan how we aremanaging developmentand what happens from adevelopment standpoint But also  when you redoing additional builds there are many othertypes of approaches And so this is something thatyou want to really think about because the impact ofleveraging open source where potentially   thisdoesn t happen that often But it has happened wherethere may be a bad actor And so how do you prevent thatfrom impacting your business So making sure youhave enough awarenessof a lot of the latest practicesand demonstrating competencyto really plan to protect yourorganization is important Again  protectingagainst ransomware Ransomware is a problem ourworld is dealing with nowand it seems tobe getting worse However  we havetools and productsand processes to help protectagainst ransomware and othertypes of attacks So having this typeof knowledge letsyou understand what you may doon the networking side in termsof how you re protectingfrom that standpoint but then also from a security And then when people think aboutcybersecurity in general  howdo you really approach this And so we have  again  processesand tools and product offeringsto assist with that One major change inthe past several yearshas been the shiftfrom just workingwith data centers thatare owned by a companyto a cloud infrastructure And then now you actuallyhave several approacheswhere you re thinking aboutmulticloud and hybrid cloud So your approach to securityacross all of these boundaries and then also theway you approach it  let s say if you were maybemonetizing some offerings or doing joint ventures andyou have a different definitionof what these boundariesactually are  this becomes somethingthat can appearto be fairly complicated However  if you areable to leveragesome of the recommendationsand best practicesfrom Google Cloud  you ll havean understanding of the wayto approach this so that yourcompany and organization willbe able to be really flexible asdifferent demands and businessmodels emerge in the future And then really thinking aboutcybersecurity and resilience cyber resilience isextremely important And when you are thinking aboutthe broad spectrum of securityand networking and howit s addressed together it s really providingyou with the capabilitiesto address thesereal world problems So I really recommendanyone who  again  if you alreadyhave a certification or maybe you re someone who s amanager of people who are doingmore of the hands ontasks  it s stilluseful to think aboutgetting these certifications because thesecertifications focus on how do you approach the designto meet a certain business needor to address certain problems And it s not focusedon just tasks So by having a reallygreat understandingof how you approach the designof security and networkingwill really be very beneficial So yeah  and Magda  thanks forasking some of these questions This is a reallyimportant topic And I really hope that we havea lot more of our customersand other stakeholders considertaking the certification exam MAGDA JARY  Super helpful Thank you so much  Cori Great insights And thank you somuch for your time And with this  weare going to wrap upwith additional resources So if you do want to startor continue your readinessjourney in the networkingand security area here is your spot So we are very excited tointroduce the Google CloudSkills Boost  our newdestination for Google CloudLearning  with over 700 labs courses  and certificationpreparation resources And to help you buildthe technical skillsthat you and your teams need we are opening up accessto Google Cloud Learningfor a month at no cost So just visitg co cloud freetraining And thank you so muchfor your time today We hope you enjoyed the sessionand learned something new Bye bye KOBI MAGNEZI  Hello  and thankyou for joining us today My name is Kobi Magnezi I m product manager for GKE And today I would like toshare with you some of the bestpractices on how to leverageGKE to power a competitive ITe commerce infrastructure First  let s startwith looking at someof the drivers fore commerce organizationand the requirementsthey imply forthe supporting infrastructure In a recent validation studyof Google Cloud solutionfor e commerce  the ESG analysisfound that retail industryis focused on providingbetter and more differentiatedcustomer experiencethrough specificallydigital transformationinitiatives with about 50 of retail organizationstrategically prioritizingthis objective For retailers  creatingan e commerce presenceincreases not just thetouchpoints with the customer but also the salesopportunity  especially whenevents like the pandemicpushes more customersto online channels At the same time it also increasesthe competitive landscape To maintain the competitiveness an e commerce infrastructuremust provide  one  highperformance  two  scalability three  reliabilityand availability four  customizationand innovation and finally  flexibility Let s look deeperinto these areasto understand how Kubernetesand GKE address these areas We will also providesome recommendationfor configuration andoptions that you would wantto consider for your cluster Many customers use GCP tomigrate  modernize  and improvetheir deployments GCP infrastructure andnetworking are powering GKE So when you create aKubernetes cluster on GKE we provision the machine andthe resources needed for GKEto properly run your cluster scontrol plane and nodes on GCP The Kubernetescluster control planeis fully managed byus  backed by an SLA We will keep the controlplane up to date and we ll scale itautomatically up and downwhen cluster size changes For efficiency andhigh performance you can choose whattype of machine what machine configurationbest meets your needsfor your different node pools And as demandschange over time  sodoes the load on youre commerce workloads Scaling is necessaryto ensure that GKEgrows to match your need Think about the number ofpods  the number of nodes GKE includes severalautoscaling capabilitiesthat will automaticallyresize your cluster basedon your configuration andsignals that we monitor HPA  for example  automaticallyincreases or decreasesthe number of pods on yourcluster based on utilization With VPA and NPA  youcan enable vertical scalefor both your nodes and pods And if capacity planningis part of your practice especially for peak seasonlike Black Friday  Cyber Mondayor New Year  you may wantto reserve GCE capacity With GCE reservation when your clusterrequires to scale repair  or upgrade it will have a reservedpool of dedicated resourcesavailable for the performance You can also create a specificor non specific reservation and GKE will create newresources based on the poolthat you reserved Now let s talkabout availability And it s importantto distinguishbetween applicationavailability and infrastructureavailability The availability of thebusiness application which the customersinteract with and the availability of theinfrastructure  which hostsand support the application On the applicationside  the impactof insufficient availability canbe an unresponsive e commercewebsite or unreachable service On the infrastructureside  the impactcan happen at different levels At the clusterlevel  for example when nodes becomeunresponsive or when pods getrescheduled  at zone level or at a regional level where there is a connectivityissue  for example While both the applicationand the infrastructureimpacts the overallavailability it is actually thebusiness applicationthat drives the decisionaround redundancy  deployments and architecture With this definitionin mind  it isimportant to understand howGKE is structured as a sharedresponsibility product The data plane  the nodes wherethe e commerce workloads aredeployed  are the mostcritical piece for customers as any reliability issue candirectly impact the business The Kubernetes control plane isthe next most critical piece as it s responsiblefor scaling  repairing and managing the deployments And then most of the workloadsdeployed on the data planecan probably survive an outage a short outage of the controlplane without too much impact The GKE control planeon the leftmost sideis the least criticalpiece  as they are mainlyfocused on the operationsand the management of the GKEcontrol plane Let s look on the twomost important piecesfrom a reliability perspective Both control planesand nodes canbe provisioned in a singlezone  a zonal cluster as illustrated onthe right side or can be provisionedin three different zoneswithin a region  aregional cluster There are some cases wherezonal cluster topology betterserves your needsand constraints We do recommend  however to use regional clusterfor highly available Kubernetesinfrastructure  especiallyfor production workloads For control planes the redundancyprovide highavailability  and inturn provide higher SLAfor the Kubernetes API But besides of theAPI availability the API availabilityin the control panedoesn t really impactthe workloads themselvesnecessarily The control plane is alsoresponsible for maintainingthe cluster withscaling  repairing and other vital operations And you may want to takeinto consideration  basedon the nature of youre commerce workloads and needs the type of topology that isright for your architecture Since we provisioncompute resourceson GCP for theKubernetes clusters node availability are based onthe Compute Engine availabilityfor instances provisioned ina single or multiple zone Finally  the availabilityof your applicationis closely tied tothe node availability as you can imagine And luckily  Kubernetesoffers an easy wayto create multiple instancesof your applicationfor higher availability One great way to securehigher availabilityfor your application  and alsoto reduce potential disruption is the use of PDB  orPod Disruption Budget PDB allows you to stateyour disruption tolerance In other words  if yourapplication is replicated you can specifythe minimum numberof replicas that can bedisrupted when maintenance evenhappens In the example that I have onthis slide here on the rightside  we specify that we cannottolerate less than two replicasof our application This is just another way ofsaying max unavailable equalone  in case we have threereplicas of the application GKE respects PDBup to 60 minutes and a termination gracepolicy up to 60 minutes When a security patch orKubernetes update is rolled outon your cluster  GKEwill perform the upgradein a way that satisfiesthe PDB configuration The other thingthat you want GKEto know about your workloadsis what considerationit needs to applywhen it schedulesyour pods on differentnodes in the cluster By default  the GKE andthe Kubernetes schedulerautomatically spread yourpods across the nodes whether it s regionalor zonal cluster However some e commerceapplications  and specificallystateful workloads like Redis  for example require a quorum of serversto successfully workin a redundant deployment Without specificinstruction  Kubernetesmay decide toco locate all the podsfor your statefulapplication  stateful seton the same machine on the same node and basically create a singlepoint of failure and riskto the availability of thee commerce application Exactly for those reasons we can use pod anti affinityto ensure pods are not scheduledon the same specific nodeif we set the criteria So if I require aquorum of serversto properly operate a statefulapplication  for example I would wantKubernetes to refrainfrom scheduling all thereplicas on the same machine So if something happenedto that machine it won t take down allthe different replicasof my application On the flip side  thereare some applications workloads that may actuallybenefit from being co locatedon the same machine For example  if youhave a web serverand you want to co locateit with the cacheservice for betterperformance and lower latency In this case  you mightwant to use pod affinity Another important aspect ofthe infrastructure reliabilityis monitoring and specificallyproviding accurate signalsto Kubernetes about yourworkload state and health The first signal in thisslide is readiness probe which basically defines asignal for when a workload isready to serve traffic Say for example  if a workloadtakes some time to start the lack of such signal maylead to Kubernetes startingto send traffic to anot yet ready to respondworkload  which will leadto time out and impactits experience The second signal hereis liveness probe which provides a healthsignal that Kubernetesuses to determine whetherit should initiate a repairand replace of afaulty instance or not Defining an upgradestrategy for your clusteris another importantdimension of reliability A new Kubernetes minor versionis released roughly every fourmonths from open source And patch versions arereleased more frequently Because Kubernetes clusterscontain both control planeand nodes  there aretwo places to managewhen it comes tokeeping your clusters upto date with releases  securitypatches  and bug fixes Luckily  GKE offersdifferent waysto keep your cluster sinfrastructure up to date Being a sharedresponsibility product GKE keeps control planeautomatically updated And alternatively  you cankeep your node version upgradedautomatically  or initiatethe upgrade manuallyby running your ownscript  or using our API To ensure that yourinfrastructure is up to dateand also being kept as such ina safe and non destructive way we highly recommend the useof multiple environments So when there is a newversion or a patch it can be rolled out fortesting and qualificationon staging andtesting environmentsbefore it hits theproduction environment In general  we recommendto use the same versionin all environments so you getconsistency and correctnessof your qualification process However  you may want toconsider a second developmentenvironment where newerreleases can be tested This can be useful when a newGKE feature or Kubernetes APIare being introduced and you wantto test them to get a head startand be ready for the marketwhen you have anew feature that srelying on this capability The process of upgradingyour Kubernetes clusters keeping the control planeand nodes preferablyin sync when it comesto the same version and also in a supported version and also in a supported versionSKU that is mandatedby open source that process can be thornyand very cumbersome And one way to simplify itis by using release channel Release channelin GKE allows youto choose your upgrade path It also controlsthe type of upgradethat will be appliedon your cluster Some customers may want to usethe latest Kubernetes version because  as I mentionedbefore  dependencieson a new API orcapability  or maybe theyhave a specificbusiness objective In this case  you may want touse the rapid channel  whichis closer to open sourcein terms of releases Other customers may preferstability over functionality And in that case they will chooseeither the stable channelor the regular channel which are the ones we recommendfor production workloads When it comes to patchingor upgrading your cluster the nodes where the e commerceapplication are deployed we replace them Those nodes are being replacedwith new provisioned machinerunning the newerKubernetes GKE version whenan upgrade takes place When nodes are replaced as you can imagine pods running onthem are gracefullyshut down so the node canbe drained and replaced On the GKE side node upgrade usedto follow theworkflow of delete then create  meaningthat when we upgrade GKE used to drain a node delete it  and then replace itwith a new one However  to increasethe availabilityand also to reducedisruption  node upgradeis now happening throughsurge upgrade  wherewe change actually the order We first create a newnode for the upgrade and only when that nodeis ready and available we gracefullydrain the old node Not only that surge reducesdisruption to existing nodes it also allows you to controlthe number of nodes thatcan be upgraded concurrently The default settingis one node at a time So keep that in mind If you are using largeclusters with many nodes you probably want tochange that numberso more nodes can beconcurrently upgradedat the same time It is clear that sustaininga competitive e commerceinfrastructure requires acontinuous upgrade practice However as we all know sometimes business requirementsmay change  especially whendisruption simply cannot betolerated For example  thinkabout peak seasons like Black Friday  Cyber Monday New Year  or major companyevents  trade fairs or something like that We recommend you touse an exclusion windowfor this event With an exclusionwindow  you canspecify a range ofdates in which youwant to refrain completelyfrom any type of maintenance You can also set upmaintenance windowsto specify when you prefermaintenance to happen This can help you to improvepredictability of the upgrade and also allow to controlthe time of the upgradeson different clusters Exclusion windows andmaintenance windowscan greatly help you controlthe disruption and reduce risk But we ve seen withmany retail customersthat they areactually experiencingdifferent requirementsduring different timesof their fiscal year For example  ane commerce infrastructureis most likely to followsome regulation or compliancelike PCI  for example And this compliancehas specific guidelineswhen it comes to keepingyour software up to dateand how frequent andfast a new securitypatch should be consumed In this case  I might wantto allow all type of upgradesand patches on bothcontrol planes and nodesthroughout a low season month while during a high peakseason  like BlackFriday  Cyber Monday I want to refrain completelyfrom any type of maintenanceso my workloads willnot be affected And then also insome other cases I might be OK with justpatching my clusters  both nodesand control planes but would wantto refrain from upgradingto the next minor version We believe that thislevel of flexibilityis actually needed fore commerce infrastructureso they can controlmaintenance events and also the risklevel associatedwith the change in accordanceto their business needs This way you can delegate allpatching and upgrades to GKE while being able to instructit on what type of upgradesyou want to allow  the scope and when to initiate a process So now that we know how upgradesand patches get rolled out I think it s important toalso stay informed and knowabout changes comingto your clusters One place to learnabout GKE releasesis  of course  the GKERelease Notes page which includes linksto the release scheduleand the version support policy But we also recommend the useof GKE Notification  whichincludes notificationfor upgradesand upgrade available events GKE Notification  whichis based on Pub Sub you can listen to upgradeavailable events  whichwhen they fire indicate that thereis a newer version or patchavailable for your cluster In turn  you can use thatevent and trigger a workflowto apply thesepatches  for example on your testing cluster So the infrastructure versioncan be vetted and qualifiedbefore it gets released toyour production environment Finally  a robuste commerce infrastructuremust also providesome flexibilityso you can apply somechanges on the flyin a non disruptive way We ve identifiedseveral common placeswhere changesinvolve disruption because when youapply these changes it actually requires us torecreate the machine or nodepool For example  you may need tochange network tags in orderto change firewall rules or routes of your commerceapplication You can now do so  youcan change network tagsfor existing running VMs without recreating the VMs and basically applyingthis change on the fly The same thing goesfor node things  whichyou can use to steer awaypods from specific nodes or labels  which areused to schedule podson particular nodes All these three options are nowmodifiable without requiringrecreation of the machine So we ve learnedfrom the ESG reportthat there are specific areas ane commerce infrastructure mustcover to maintaincompetitiveness High availability where redundancyis important to avoidsingle point of failure That applies to bothcontrol plane and nodes Scalability  use multiplereplicas  the use of PDBto avoid disruption affinity and anti affinityto better instructthe Kubernetesscheduler on how to optimize thedeployment of your workloads Reliability andavailability  rememberthat control planes arefully managed on GKE but can go through maintenance Make sure that you don tcreate unnecessary dependencieson the control planeAPI in your applicationto avoid disruptionto your applicationwhen control planeis being upgraded And then  finally for flexibility  thereare some changes thatcan be applied on the flyand do not impose any disruptionfor existing workloads To close up  I want to sharea quote from Loblaw  a GKEe commerce customer  whoachieved great success usingboth GCP and GKE  despitegoing through the unexpectedchallenges of the pandemic Loblaw migrated andmodernized its e commercewith GCP and GKE  and alsoleveraged the Black Friday Cyber Monday white glove servicefor additional peace of mindduring this criticalbusiness period This resulted in betterperformance at scale faster applicationdevelopment and deployment high availability  andelimination of Black Friday Cyber Monday outages And while keeping theinfrastructure up to date Loblaw managed to leveragethe different infrastructurecapabilities to roll out theirCOVID 19 vaccines deliverysystem in nine weeksinstead of 18 months There s much more to learn You can downloadthe full ESG reportfrom the first link on thispage and find more documentationbest practices on the GKEwebsite on the second link Thank you so much forwatching this session I hope you enjoy therest of your Google Next MANOJ SHARMA  Hello  and welcometo the session that shows youhow to increase agilityand reduce costswith VMware as a service or as I d like to say what s this new hard VMwareon Google Cloud thingthat everyone s talking about Hi I m Manoj  and I lead the GoogleCloud VMware Engine productteam And I m co presentingwith Sai  whowill join us later after mysection of the talk is over So we learned today how GoogleCloud VMware Engine finallymakes it super easy for you togo from your VMware deploymentson prem to Google Cloud We ll also learn what theservice is all about  as wellas some key advantagesand use cases And then we show youhow to get startedwith the service with somecool demos along the way So what s the problem statement So let s face it  onprem infrastructurecan be a challenge It can strain  INAUDIBLE staff because theyhave to do laborious tasksrelated to infrastructureprocurement  and maintenance and be reactive to businessneeds with long lead times But it also means that they reusing legacy systems thatare not adding value to thebusiness  other than keepingthe lights on And therefore  theycan t scale with ease They re locked into whatthey have on premisesand are forced to payfor overprovisioningand forced to provisionfor peak usage So how do we solve this problem Google Cloud has created aservice called VMware Engine which solves the problem And it really gives youa fast forward buttonto your journey to the Cloud It s the fastest way tomigrate your apps from VMwareto the Cloud You get the platformyou know and lovewith all the familiar interfacesin the VMware platform such as vCenter And the service is sold on thebasis of hyper converged nodes which includes the service  thesoftware license  the storage compute networking and all infrastructureyou need to run VMwarenatively on Google Cloud The solution is sold by Googleand is supported directlyby Google  but it has been builtwith the cooperation of VMware And VMware fullycertifies this offering And you can consumeVMware basicallyin a fully automatedself service manner just like you consumeany other Cloud service And by getting all the serviceneeds from one company you simplify the way youconsume VMware in the Cloud I want to highlight that wehave a real strong partnershipwith VMware to make GoogleCloud VMware Engine happen And the partnership isnot just at the technologyand engineering level Our collaborationextends far beyond thatto ensure that our customershave a smooth experiencewith the service To find out more  Iencourage you to check outthe sessions and thecustomer conversationsat VMware s marquee event thatjust happened  VMworld 2021 You can also checkout the blog post The link is indicatedon the slide And a lot of the VMworldinfrastructure  I want to note is actually the user labs And a lot of thehands on experiencefor customers atVMworld 2021  in fact was hosted on GoogleCloud VMware Engine So we are very happyand proud about that So what we did to find outwhether the service adds value we surveyed a lot of IT andapplication leader owners And we found thatthe service delivers We found that theservice is really ableto meet the goals of theCloud transformation journeythat we talked about First  we findthat the customersare able to provision veryquickly and in minutesin Google Cloud and get to thetime to value to be really low You can run next to awide variety of data  AI and infrastructure services thatGoogle Cloud is known and lovedfor You get extremely reliable  highperformance  highly redundantinfrastructure toensure that youhave a no compromisesenterprise experience It s super important And the best partis that you ll beable to save money whileyou re doing all of thisby leveraging the Cloudeconomies of scale and the payas you go flexibility thatyou get in Google Cloud So when we weredesigning the servicewe had a certain missionand a vision in mind And that was really what wecall cloud native VMware And cloud native VMware meansthree very specific things First  it means that it s acloud optimized experience meaning that customers canconsume VMware as simplyas any other cloudservice  and take advantageof the elasticity ofscale that I talked aboutand the scale on demand model And you can do so muchfaster than any other wayin getting your VMwareinfrastructure to the Cloud We simplify the useof VMware servicesthrough unified identities access control  and support And a customer  INAUDIBLE has moved to a self serviceand scale on demand modelon their VMware consumptionwith the service  andhave done so much fasterthan was ever possiblebefore for them in the Cloud The second part of whatcloud native VMware meansis a cloud integratedVMware experience And what this means is you cantransform business applicationswith simplified integrationsof Google Cloud Services like Cloud Operations BigQuery  and Anthos And so you can respondto business needsmore quickly in a more agilefashion with these services A large telco provider hasstreamlined their operationsmanagement  monitoringand logging coming from bothVMware  as well asthe rest of cloud conversioninto a single pane of glasswith the CloudOperations integrationsyou have with VMware And the third part ofthe cloud native VMwareis the familiarthird party applicationsthat you consume on prem as well as in the Cloud What that meansis you can extendyour ecosystem ofthird party apps and servicesthat are criticalto your businessfor your various needs  suchas desktop and backup and DR And we ve partneredwith solution providersand third party applicationdevelopers  such as Veeam Rubrik  and  INAUDIBLE  And we ve also partneredto solve the storageinfrastructureproblem with NetAppto provide storagesolutions for VMware So that s how webring this missionto life with thesethree elementsof cloud optimized cloud integrated  and ecosystemenabled So one of the surprising aspectsof adopting VMware Engineis that customers  infact  save quite a bitcompared to bothrunning on premisesand choosing otheroptions in the Cloud And you must have seenthese kinds of studiesmany times that theyshow that they save overa three year window But then you find that the costof initial adoption is high And that s not thecase with our service Let me level with you If you re going tothe Cloud anyway and you re comingfrom VMware  there sno better place thanGoogle Cloud VMware Engine because the platform is thesame as what you can do on prem And with theoversubscription capabilitiesof the VMware platform you get very high densityand lower cost per VM And what better way tounderstand this valuethan to hear a testimonialdirectly from the customer So let s take a look ata video that gives youa good idea of howMitel is gettingthe best of both worlds  runningthe platform they know and loveon Google Cloud  VIDEO PLAYBACK SPEAKER  Mitel is a moderncommunications company leveraging state ofthe art technologywith officesthroughout the world Our challenge is reallynetted out to the fourS s   stability  SLA security  and scale Google Cloud VMwareEngine is reallysolving the stability and thereliability platform issue With that  we reable to move outof about 15 data centers  whichgets us closer to our datacenter decommission objective The benefit going forwardin the Google Cloudis now we can look at differentdeployment options as we grow  MUSIC PLAYING  END PLAYBACK MANOJ SHARMA  Whata great video And let me give you justa little more perspective I was  in fact  involvedin the Mitel experience And there moved thousandsof VMs  most of whichhappened on just acouple of weekends And they were runninga telephony applicationwith voice and video needs And they needed low latencyand really high throughput And with the VMware Engine sfully allocated networking fully dedicatednetworking  they wereable to getsustained performanceand not have to worryabout multitenancyor any other customerstepping on their toes And so that s how they trulygot to this best of both worldsexperience  and got fromthe VMware environmentthat they trulylove to Google Cloudthat they have come tonow know and love as well And with that  I ll handthe baton to my colleague Sai  who will walk us throughthe differentiation and the usecases and give you moreexamples of the goodness comingfrom the service and howyou can truly superchargeyour journey to the Cloud So over to you  Sai SAI GOPALAN  Thank you  Manoj I m Sai  and I m a productmanager with Google Cloud Let s get right into it With Google CloudVMware Engine  you reable to migrate to GoogleCloud without changesto your applications You use the sametools and processes and your staff can hitthe ground running From an experiencestandpoint  thereis a single vendor Google  whichmeans that for your purchasingsupport and service needs Google is there for you For the given that this isa Google managed service you do not have to managethe lifecycle of your VMwareinfrastructure Google does that for you soyou can focus on your apps With our differentiatednetworking you can truly leverage some ofthe native capabilities thatallow your apps andenvironments to be global But unified management unified access control and access to the samethird party poolingthat you have built and investedin   INAUDIBLE  becomes simple You can run bothVMs and containersin Google Cloud VMware Engine And this is extremely useful forhigh performance applicationswhere locality and runningon dedicated infrastructureis important Further  high bandwidth low latency  private accessto co located Googleservices reducesthe amount of work requiredto modernize applications Let us look at some of thekey use cases of Google CloudVMware Customers that want tomigrate their entire datacenters to the Cloud due toevents such as data centerexits  mergers acquisitions  or thosethat just want to extend theirdata center to add capacityin the Cloud can invest inGoogle Cloud VMware Engine It is completely consistent withthe on premises environment We simplify the networking andprovide the service globally This also appliesto customers whoare looking to supporthigh performanceapplications in isolatedcompute environments and also apps and  INAUDIBLE that are incompatiblewith otherinfrastructure platforms For those that are lookingto develop a cloud based DSstrategy due to events  suchas upcoming audits  protectingagainst ransomware protecting tier two tier three apps or geographic riskdiversification can also looktowards Google Cloud VMwareEngine  given ourlow cost DR solutionsand support for a wideecosystem of goods For customers looking tosupport distributed desktopinfrastructures inmultiple geographies or those that have upcomingsoftware licensing renewals or that are looking toaddress seasonality or supportwith more end usersat scale  savefor their work from anywhereinitiatives our service offers supportfor both industry leading VDIsolutions with VMwareHorizon  as well as Citrix Finally  for thosecustomers who are lookingto unlock the valueof Google servicesfor their applicationsand infrastructure Google Cloud VMwareEngine offers a pathtowards where you can moveyour applications rapidlyto the Cloud  and thenmodernize over time We are now availablein 12 regionsworldwide with the latestaddition of Mumbai Then we have four  INAUDIBLE of SLA in a single zone in eachof these locations In other Cloud context this might not be possible There are customers whowant even more availability And for them  we haveadditional zones coming up Please speak to your GoogleCloud sales representativeor your customerengineer if you haverequirements for aparticular region thatis not shown over here Google Cloud VMwareEngine is HIPAA compliant and other key industrycertificationsare being looked upon These include ISO PCI DSS  and SOC Now  Google CloudVMware Engine trulydelivers a globalhighly performant enterprise friendly  flexiblenetworking experience With simplified regionaland global routing modes you have one globalnetwork with no necessityto create regional networkdesigns and connect them This greatly simplifies andreduces the cost and complexityto have more than one pointof presence in the Cloud You can architect andconnect different partsof your environmenteasily across projectsand across regionswith capabilities such as multi VPC andmulti region networking With a fully redundant 100Gbps east west networkingwith no oversubscription  aswell as a superior low latentpacket rate performance our architecturereally helps customerswho have applicationsthat are latency sensitiveor have high performancenetworking requirements  suchas voice over IP applications and other end user workloads  asyou ve heard from our customertestimony Finally  with GoogleCloud VMware Engine your current networkingWindows solutionscan be brought to the Cloud We offer flexible publicingress and egress optionswith support for threedifferent ways for workloadsto get access to the internet We have the Google Cloud VMwareEngine edge  the Google Cloudedge  or your own corporate DNS We also have built inintegration with Cloud DNS and you can also bringyour own corporate DNS We are an enterpriseready platformthat scales and supportsfor your enterpriseworkloads with up to64 node private cloudsrunning on highlyperformant and scalableGoogle Cloud infrastructure dynamic resource managementwith autoscaling enabled We also provide afully dedicated single tenant high performancenode with all NVMe storageand with rich GCP networking and full functionality of NSX Twith access to NSX T manager With 99 9 SLA in a singlezone with 5 to 32 nodesin a cluster  you havethe high availabilitythat is required by some of yourmission critical applications In addition  we also supportrich media environmentswith large scale VMware Horizonarchitectures supported as well as the ability to extendand migrate VMware based Citrixmedia workloads It s fully validated by Citrix One of the questionswe often getis  can I bring my VMware just my VMware environment or everything I consume on it How can I make my VMwareenvironment and consumptionbetter in the Cloud The answer is we offer a fullycompatible VMware experienceso you can bring your existingtools and processes as is Whether it is your VMware Tools all your third party toolsthat support VMware  as well asour own marketplace services In addition  you can takeadvantage of Google CloudServices  whetherit is optimizingthe embedded protection usingCloud Storage  a zero footprintDR solution with INAUDIBLE    buildinghybrid  high performance and availabilityarchitectures for databases modernizing your applicationsusing containers and Kubernetes centralizing and modernizingyour operations management or enriching your app s dataanalytics  and more In the next segment we will lookat a couple of these examples Now  as you consumeVMware in the Cloud you want to ensure that youavoid switching between screensfor operations management For example  if you havea performance problem you want to be ableto troubleshootall aspects of the applicationdependencies in one placerather than go todifferent systemsto solve part of the problem While the VMware Engineservice fully supportsexisting VMware monitoring andlogging codes without changes as you start to develop morehybrid applications thatactually live outside ofthe VMware environment getting all operations data inone place will be important To solve this  you can useGoogle Cloud s OperationService  formerly Stackdriver You collect metrics logs  and placesacross VMware Engine andother Google Cloud Services You can use built in out of the box dashboardsand views to monitoryour platform as well as the applications andquery and analyze these signalsas well You can see the informationfrom the VMware infrastructure as well as the in guestand application As far as the VMwareinfrastructure goes you can see the vCenter the  INAUDIBLE  sidedo log analysis This does not requireany agent installation Metrics such as VM CPU disk  and network usageare all available For VM guests andapplications  youcan use open source agentsto collect additional logsand methods So if you want to troubleshootany application or gaininsight into theoptimization or usage you can do it all in one place As for the secondexample  we willlook at a demo of howwe integrate BigQuerywith workloads running inGoogle Cloud VMware Engineto unlock the value of dataanalytics for our customers  VIDEO PLAYBACK SPEAKER  The fraudulentcharges data setis in a MySQL databaserunning on premises in a VM We have taken that data andmigrated the VM to Google CloudVMware Engine using HCX Now we can access the datawith Google Cloud Services To move the datainto Google Cloud we replicate the datainto a Cloud SQL instanceusing the Google databasemigration service We can then writea simple SQL scriptto show the zip codes withthe most amount of credit cardfraud As you can see  GoogleBigQuery transforms that datato appear as a visual map Let s take a deeper look intothe data from a global viewfirst Now we would liketo see on a zip codebasis which part ofthe United Statesshows the highest incidentof credit card fraud Here  you can see that this zipcode has an unusually higherincident of credit card fraud  END PLAYBACK SAI GOPALAN  Now  howdo you get started Well  moving to theCloud can bring upconcerns about how torationalize existing licenseinvestments that you mayhave made on premises Now we are introducing thisprogram called the Catalystprogram  which providesGoogle Cloud VMware Engineusers financialflexibility to acceleratetheir journey to Google Cloud Eligible customers can now getone time Google Cloud creditsthat offset unused VMwarelicensing investments You can apply these creditsagainst the consumptionof Google Cloud VMware Engine as well as any other GoogleCloud service of yourchoice the following year This is aconsumption based program and this program is availabledirectly through Google Cloud or through existing GoogleCloud channel partnersthat you work with Terms and conditions apply So if you re interested  pleasedo contact a Google Cloud salesrepresentative or yourpartner service managerfor more details In addition  with theplanned capabilityof a single nodeprivate cloud  youcan experience theservice with a single nodefor non productionquick proof of concepts Note that the single nodeprivate cloud will remain livefor 30 days  after whichit will expire or reset During this period you couldupgrade the full productioncluster with allyour data as well So what are you waiting for As next steps  thereare a few resourcesthat you can take advantage of Google Cloud VMware Enginecomes in a hyperconverged formfactor with all compute andstorage included with the node However  there are cases whenyou need additional storagefor capacity hungryscenarios  such as large scaledatabases  media  electronicdesign automation  datawarehousing  or largescale analytics You also need to lowercost in DR scenariosso you can protect your missioncritical data without overhead To solve for suchneeds  we are nowannouncing todaysupport for NetApp CloudVolumes in preview If you think that this isgoing to be useful for you I encourage you to contactyour Google representativeto find out how you canparticipate in the preview Contact your Google team fora Google Search Cloud VMwareEngine demo We can also conduct a freeassessment with Google Cloud or with our partners  as wellas schedule a half day workshopwith your GoogleCloud team to go overthe results of that assessment This brings us to theend of this presentation Thank you so much for watching SAILESH KRISHNAMURTHY  Hello welcome to this sessionon innovating withtransformative databasesfor an always on digital world My name is SaileshKrishnamurthy and I lead engineering for cloudnative databases at Google Today I am thrilled tohave Hari Ramamurthy  whois a technology fellow atthe Home Depot join us Hari will share how the HomeDepot uses Google s cloudnative databases totransform their businessin the modern digital world We ll begin todayby talking abouttransformative capabilitiesand why they re so important Next we ll review ourtransformative databases Spanner  Bigtable and Firestore and the new innovations we arebringing to this portfolio And then we ll hear fromHari on how the HomeDepot uses these databases Today s innovatorsare disruptingtraditional industries byusing data and softwareto deliver personalized andseamless digital experiencesacross devices and channels Modern conveniences  likebuy online  pickup in store instant digital payments real time ride ridesharingrequests  and othermarketplace serviceshave raised the bar in consumerexperiences with the respectto the size and thecomplexity of the problem or even how regulatedthe industry is These disruptiveexperiences requiresophisticated andcomplex applicationsthat need to serve millionsof users in a reliableand continuouslyavailable fashion skillfully handle viral growthand unpredictable spikesin demand withoutmissing a beat and securely dealwith bad actors and comply with stringentregulatory regimes Speed is critical  andyou need the flexibilityto iterate quickly and deliverinnovative and differentiatedexperiences for your customers The key to scalably and securelyserving millions of usersis an operational databasethat ll power your applicationsand run your business You need databases withtransformative capabilities And the new table stakes are anavailability SLA of five nines with zero maintenance downtime and automatic failure recovery That s at most five minutesof downtime every year Online and unlimitedscaling and zerotouch global andmulti region deployments And most important the highest security complying with the industry smost demanding standards At Google Cloudwe offer not one but threetransformative databasesthat provide thecapabilities that youneed to deliver innovativecustomer experiences These are Spanner Bigtable  and Firestore And through thesedatabases  you getto access the very samegroundbreaking technologiesthat power the Google apps thatwe all love and use every day Search  YouTube  Gmail  Maps Each of these are used bybillions of users worldwide And while data is bornin operational databases these are only the firstpart of a bigger journey Our database services aredeeply integrated with a largerecosystem of market leadingdata and AI services thatare part of Google Cloud These include services likeBigQuery  GKE  DataFlow DataStream  Pub Sub  CloudFunctions  and so on And these serviceshelp you break downyour operational silos build data pipelines generate real time insights  andmake better business decisions In 2021 we delivered manynew features and enhancementsacross the entire portfolio Some of the morenotable ones includecustomer managed encryptionkeys  data access auditlogging  key visualizersfor observability and an availabilitySLA of up to five ninesfor Bigtable  matching thatof Spanner and Firestore And today we areexcited to sharehow we are raising thebar yet again with morenew innovations Let s take a closer look Spanner is a fully managedrelational databasewith global scale asset transactions and full SQL support strong consistencyacross regions andcontinents  high availability up to five nines withzero scheduled downtime To give you a sense of themagnitude of its scale Spanner processes over a billionrequests per second at peak There s never  ever beena relational databasewith the scale of Spanner And now we want todemocratize Spanner and makeit available to everyone Earlier this year we announcedgranular instance sizing  whichis available inpreview  and worksbased on a new concept ofProcessing Units  or PUs A node is made of 1 000PUs  and you can nowget started bycommissioning 100 PUs You can scale up in steps of100 PUs without any downtime going from 100  200  300 all the way to 1 000 PUs which is one node And you can then scaleout by adding capacityin increments of 1 000 PUs 1 000  2 000  3 000  or a nodeat a time With granular instanceconfigurations you will be ableto provision onetenth of a current nodefor one tenth the price and can now get started withSpanner for just  65 a month We think this is a gamechanger for customerswho need smaller instancesfor smaller and medium sizedworkloads  and alsounlocks creativityand innovation for smallerteams on tight budgets PostgreSQL has emerged today asthe de facto industry standardfor relationaldatabases as businessesaccelerate to the cloud And today I am excited toannounce Cloud Spanner sPostgres interface  whichfurther democratizes accessto Spanner for millionsof developers and ISPs This interface combinesthe power and reliabilityof Spanner with the familiarityand portability of Postgres With the Postgresinterface  enterpriseslooking to standardizeon Postgrescan now take advantage ofSpanner s global scale five nines availability and strong consistency usingthe skills and tools of thepopular and ubiquitous Postgresecosystem Spanner s Postgres interfaceis now available in preview and we d love your feedback If you are interested please sign up on the URLlisted on the slide Bigtable is a fully managed scalable  NoSQL databasefor large  operational and analytical workloads with an industryleading availabilitySLA of up to five nines It s ideal for apps that requireconsistent  single digit millisecond latencies and need to supporthigh throughput  millionsof requests per second Bigtable scales seamlesslyfrom a first gigabyteto petabyte scale And in fact  Bigtablehas more than 10 exabytesof data under management It also integrates withthe Apache ecosystemand supports the HBase API And today I m excited to sharethat autoscaling is coming soonto Bigtable With autoscaling  Bigtable willautomatically add or removecapacity in responseto the changing demandsof your workloads So you only pay for thecapacity that you need Autoscaling reduces managementoverhead  as your teams can nowspend more timeon your business and less time managingyour infrastructure And we ve also doubled thestorage units for Bigtableso you can now storemore data cheaply Bigtable nodes now supporttwice as much storage capacity From SSD you go from a limitof two and a half terabytesper node to a maximum offive terabytes per node and for HDD  from 8terabytes per nodeto a maximum of 16terabytes per node This is especially more costeffective for batch workloadsthat operate on largeamounts of data Firestore is our fully managed scalable  and truly serverlessdocument database that letsyou easily and rapidly developrich mobile andweb applications All this with an availabilitySLA of up to fivenines that s achieved throughstrongly consistent datareplication Developers love Firestorebecause of its abilityto serve both as adocument databaseand as a backendbusiness service It s therefore  nosurprise that Firestorehas built a thrivingdeveloper communitywith more than 250 000 monthlyactive developers and Firestoreapps powering more than 750million monthly active endusers using Firebase Auth You can develop anddeploy apps fast with direct connectivityto the database no middle tier needed These apps can have rich andcollaborative experienceswith built in real time livesynchronization across devices as well as seamlesssupport for disconnectedor an offline mode Firestore s developerexperience truly sets it apart And we continue to investheavily in this area With Key Visualizer  customerscan quickly and visuallyidentify performance issues With the latestversion of our web SDK we ve added tree shaking which reduces the SDK footprintby as much as 80  A smaller SDK means fasterload times for your app Both Key Visualizer and the newweb SDK are available today We also graduate the Unityand C   SDKs to generalavailability in Q4 Both of these SDKsmake it easierto develop mobilegames using Firestore We re excited aboutmobile gaming As game developers havetold us how Firestore letsthem quickly build anditerate on their games In addition todeveloper friendliness we ve been focusedon making Firestoremore enterprise friendly  witha strong commitment to securityand privacy Data access auditlogs in previewlets privilegedadministrators auditall operations  admin  anddata on a Firestore database Custom IAM in GA lets youalign Firestore permissionswith your organization sIAM roles In addition tothese enhancements I m excited toannounce new features With VPC servicecontrols customerswill be able to reduceexfiltration risksthrough network level securitycontrols that include Firestorein a security perimeter And with App Check customers willbe able to ensure only signedand pre authorized appsare allowed to makerequests to Firestone We ll roll out previewsfor VPC service controlsand App Check in Q4 Let s now take a lookat real world examplesand hear from Hari onhow the Home Depot isusing these transformativedatabases for a variety of usecases HARI RAMAMURTHY Thank you  Sailesh Home Depot is the largesthome improvement retailerin the US with close to2 000 stores in the US But specifically in the contextof this last year that sbeen so unprecedented the amount of stressthat we ve been placing onall our IT systems in generalhas really been significant And thanks to the Googleplatform overall and manyof these databases  I thinkour systems have actuallycome out doing quite well duringthese really challenging times Some of the keymetrics  if you reallylook at it in the course ofthis really extraordinary periodthat we are in  we re delightedthat we grew our businessin 2020 by  21 billion 86  of the digital sales growthand over  3 6 billion visitsto our website are someof the key metrics thatunderscore the criticalityof the technology platformsand solutions that we ve beenrunning through this period Let s look at Cloud Spanner andits usage in some more detail There are multiple scenariosin the organization especially in areaslike supply chain or order management  or loyaltywhere it is really criticalthat the system statemoves from one atomic ACID compliance state to anotherconsistent transactional state And we need to dothis in additionto those systems actually beingavailable in multiple datacenters and servingtraffic comingfrom various regionsto these APIsand the databasesthat power them And this is reallywhere Spanner hasbeen a key player inensuring that we remeeting the levelof service thatis expected of our customers To look at Spannerin some more detail I think the fact thatwe  in these areas are able to deploy our softwareon three or more data centersand have really largevolumes of data  I mean  the 10 TB is stillwhere we are currentlyand we see that growing rapidly But be able to serve thetraffic that s coming throughin multiple data centers and have the system movefrom that one consistenttransactional state to anotherhas been really key in makingsure that we are alwaysnot only serving trafficat extremely high volumes but we do not have to do it withany fear about the consistencywith which the systemis actually performing In addition  capabilitieslike secondary indiceshas been really helpfulin some of these scenariosin making sure thatwe can access the dataand reach the appropriatepattern of usage depending on the context The fact that weare fundamentallyable to deploy theseapplications thathave strong consistencyguarantees without needingto worry about uptime and be able to serveour customers in thesevarious geographical zoneshas really been agame changer for us And it s somethingthat we ve beenreally excited to useand employ as partof our overall strategy On the other sideof the spectrum there are also multiplescenarios where we may notneed the strongconsistency guarantees but we have extremely demandingrequirements with regardsto the response timeof the database as well as the scaleof the database And this is reallywhere Bigtable sbeen a game changer for us A specific case in pointis on the website wherewe employ personalizedexperiences for our customers we ve been able to tailorthese experiences at extremelyhigh volumes basedon the journeythat the customer has taken combine it with prior purchasehistory  as well as bothstatic and dynamic rulesthat we ve actually learnedbased on customer shoppingbehaviors And some of theserequirements demandthat we easily scaleup to thousandsof transactions per second And we ve been able to dothat quite effortlesslywith Bigtable In addition  there are scenarioslike inventory aggregationwhere we really have tobring together the inventorypicture across our network both the stores and the supplychain  and be able to do thisin a matter of millisecondsso that we are able toappropriately promiseto our customersthe inventory that savailable in various locations And we re really glad that wechose Bigtable as the platformto bring some ofthese experiences I think it s noteworthy thatin many of these scenarios we are dealing with certainlythousands of transactionsper second duringregular processes but in scenarios likethe recommendations there are scenarios where weneed to process certain batchprocesses that may push upthe database s transactionthroughput to up to 450 000transactions per second And we do this on a consistentbasis for short periods of timewithout needing toemploy a large clusterthroughout the period  butrather just bringing upthe increased number ofnodes when we need it and then going back to amore regular sized cluster As mentioned here  currentlythat s typically from 10to up to 30 nodes And we re really excited abouthaving this type of elasticitycome to play In addition  the fact that we reable to serve these extremelyhigh volumes oftraffic without havingto worry about maintainingthese databases worrying about updates and be able to do itin that multi datacenterset up that is almostnecessary for ourscales and volumeshas been truly agame changer and hasallowed us to dealwith the type of spikesthat we have seen throughthe course of this last year Firestore is anotherreally interesting databasethat we re doingsome prototypes with And the specificcapability of Firestore with its offlinecapabilities both from a readas well as from a writethrough cache standpointis something that sof great interest And there are certain scenariosacross our stores in homewhere we really believethat this capability couldcome in really handy And we re currentlyin the processof prototyping this database And we re really excitedby what it has to offer SAILESH KRISHNAMURTHY Thank you  Hari for sharing how the HomeDepot is innovating customerexperiences withGoogle Cloud Databasesand driving the growthof your businessin this very challenging andinteresting time for all of us Google Cloud Databases providea groundbreaking platformfor innovationbased on decades ofour own firsthand experienceswith shaping the digital world We offer the highest levelsof availability  reliability and global scale enabling you to deliverthe best possible experiencesfor your customers There is tons moreexciting content at Next And I encourageyou all to attendthese sessions to learn moreabout Spanner  Bigtable and Firestore To dive deeper take the Qwiklabs Thank you all so muchfor your time today PETER BLUM  Hi  everyone and welcome to our session Innovations in DDoS WAF  Firewalls and Network basedThreat Detection My name is Peter Blum And I run the network securityPM team here at Google Cloud And I m joined by Emil  who I mgoing to let introduce himself EMIL KINER  Thanks  Peter Hi My name is Emil Kiner And I run product managementfor our DDoS mitigation and webapplication firewall productline called Cloud Armor PETER BLUM  So today I m going to start offby sharing our visionfor network securityand provide a briefoverview of our solutions After that  I mgoing to hand overto Emil  who sgoing to get us upto date on all thelatest with Cloud Armor And then I ll pick back up andcover all the great new thingsthat we ve been doingin network protectionwith our Cloud firewall NAT  and IDS offerings So let me start with our broadervision of network securityat Google Cloud As you can see inour vision statement our goal with network securityis to make it invisible We want to weave itinto the cloud fabricwith automated  but verifiable protection  simplified control and clear prioritizationof critical threatswith relevant context And where we can  we alsowant to provide the solution Emil is actually goingto talk about someof the new capabilities inCloud Armor in just a bitthat I think are aperfect example of whatwe re working towardsacross the entire portfolio I wanted to provide an overviewof our network securityportfolio I m going to overlay this ona high level diagram that sshowing flows in  between and out of our cloud First  in terms ofhandling incoming trafficto internet facing services we provide an integrated edgedelivery stack that includesa global load balancer and CDNthat provide for availability scalability  and performance And then built intothis stack to provideour global scale DDoS protectionand a great web applicationfirewall is CloudArmor  which Emilis going to dig into shortly And then we providedefense in depthwith a network firewall and NATservice built into our cloudnetworking fabric These same twoservices also providefor protection and privacyfor traffic out of the networkas well And as you can expect  ournetwork security solutionsare integrated in ourCloud Security products bringing in context aboutthreats  integrationwith our bot  fraud  andapplication access solutionsas well And then we providea full complementof network leveltelemetry from metadataabout traffic flows insideand out of the cloud  dataabout traffic we block at thenetwork and application layer plus we provide fullpacket inspectionservices with our packetmirroring capability And with the nextfew months  you llsee we ve also releasednetwork threat detection It s a system calledCloud IDS thatwas a joint partnership withPalo Alto Networks  wherewe use their threat detectionsystems within our own cloud All the alerts and telemetryfrom these productsare also integrated intoour own threat hunting alert management  and unifiedsecurity management services And then we also integrate withthird party SIEM and sourcesystems as well So with that  letme hand over to Emilto take us through Cloud Armor EMIL KINER  Thanks  Peter Today  I m excitedto share with youall the great newdevelopments and capabilitieswe ve released with CloudArmor over the past year Cloud Armor is Google Cloud sDDoS protection serviceand web application firewall It provides global scale defenseagainst volumetric  protocol and application level attacks And it s built using the sametechnology and infrastructurethat we originallybuilt to protectour own billion user properties such as Search  Gmail and Maps Cloud Armor reflectsour vision of bringingto bear Google s global scaleto protect customer workloadswherever they are deployed Cloud Armor  inaddition to the restof our edge infrastructure including load balancingand CDN  can be deployedin front of your websitesor services  whetheryou are runningon Google Cloud  on prem  orwith another infrastructureprovider Over the past year  we vebeen privileged enoughto help protect a growingset of enterprise customerswhile moving quickly toexpand our capabilitiesand scope of protection all while investingheavily into building acutting edge machine learningsolution to meet the evolvingthreat landscape head on I m excited toshare with you someof our recent developments Earlier this year  we introducedCloud Armor Managed ProtectionPlus  a managed applicationprotection servicewhich bundles together allthe features and capabilitiesof Cloud Armor with additionalvalue added servicesinto an enterprise friendlyand predictablemonthly subscription Managed Protection Plusdelivers unrivaled applicationprotection while providingthe peace of mindthat Google has your back 24 7 In addition to includingall Cloud Armor WAFrules  policies  and requests Managed Protection Plusincludes premium capabilities like Adaptive Protection as well as 24 7 DDoS responsesupport  which gives youaccess to Google s DoS expertsto help mitigate attackstargeting your services Further  with DDoSBill Protection we re removing thefinancial risk of DDoSby insulating you from spikes inyour networking expenses causedby successful DDoS attacks Over the next severalquarters  you llsee us continue to addadditional premium capabilitiesand manage services into ManagedProtection Plus subscription Next  we want to talk aboutour new Adaptive Protectioncapability  whichhas already generateda lot of positive feedbackfrom a fast growing enterprisecustomer base Adaptive Protection is amachine learning based solutionto detect andefficiently respondto application level distributeddenial of service attacks We have developedAdaptive Protectionover the last several yearsin close collaborationwith external andinternal customers like Project Shield continuously trainingon attack data we observeagainst all of Google Adaptive Protection developsa per application baselineby learning what normaltraffic patterns looklike for eachprotected application In addition totraffic volumes  weassess dozens offeatures and attributesof the incoming request trafficto establish the baseline Next  Adaptive Protectionmonitors traffic flowsin real time andgenerates alertsfor anomalous orpotentially attack traffic A second set ofmachine learning modelthen distills theanomalous trafficinto a dynamicallygenerated signature The result of this analysisis presented immediatelyas part of the original alert We also take it a step furtherand provide a proposed CloudArmor rule that canmitigate the attacktraffic with minimalto no collateral damageto the good baseline traffic Adaptive Protection providesnear real time attack detectionwith immediatelyactionable alerts saving your incident responderscritical minutes or hoursof time consuming loganalysis  triage  and other IRactivities  allowing you tosafely and efficiently deliverreliable services This is the main AdaptiveProtection dashboard Here  you can seeincoming traffic patternsfor all of yourservices protectedby AdaptiveProtection as well asthe list of attack alertsdetected over the selected timehorizon When we click onone of the alerts we see a much moredetailed view of the attackand the results of our machinelearning powered analysis We present themachine determined confidencelevel in our findings  thesize and shape of the attack as well as the attack signature For this attackin particular  wehave identified andpresented a unique signatureacross the source region code remote host  and target hostattributes We also propose a rule that canbe deployed in near real timeto mitigate the attackas it s occurring Because of the sensitivityof our detection models alerts are frequently generatedwhile an attack is stillramping up  allowing you torespond before it has a chanceto impact your services When the operatorclicks Apply  theycan see the proposed rule  aswell as additional guidancefrom Adaptive Protection  to aidin understanding and assessingthe expected impact ofthe proposed mitigation Mitigations can also be deployedin a logging only or previewmode to validateeffectiveness and safety Specifically  you cansee the expected impactof the proposed rule onthe good and bad traffic In this case  theproposed rule isexpected to impact 0 of the good trafficfrom the baseline whileblocking about all or nearly allof the incoming attack So we are providingan end to end solutionto detect and mitigate attackswith near real time actionablealerts containing dynamicallygenerated mitigations  whichare narrowly targeted forjust the malicious traffic This insight not onlysaves incident responderscritical time in reacting andmounting an effective defense but also providesthe informationto aid triage efforts andhelp explain why we considerthe traffic suspicious Next  we re proud to announcethe preview release of CloudArmor Bot Managementwith a deep integrationwith reCAPTCHA Enterprise  whichis Google Cloud s enterpriseversion of the leading botand fraud protection product While customers can use theseproducts together already we wanted to simplifythe deployment processand provide a more integratedand scalable experience ReCAPTCHA Enterprise providesa world class bot detectionmechanism developedfrom and continuouslylearning from aglobally distributedmulti millionwebsite install base With this deep integration Cloud Armor customerscan now leverage reCAPTCHAto detect and blockbot traffic within theirCloud Armor security policies All of this occurs at theedge of Google s network far upstream of yourprotected workloads allowing us to leverage theglobal scale of our networkto mitigate bot threats withoutimpacting the performanceof your applications Critically  with theintegrated solution Cloud Armor andreCAPTCHA Enterprisecan protect customerapplicationswithout requiring server sidechanges on the applicationto decipher and enforcebot assessments You can configure CloudArmor security policiesin several ways toleverage this integration You can configureany rule actionto serve thereCAPTCHA challenge forcing suspicious endusers to solve the puzzlebefore being allowed through Additionally  you can instrumentyour client side applicationswith the reCAPTCHA libraryfor seamless and silent botassessments and then have CloudArmor decipher and enforcethe assessment at the edge In addition to thereCAPTCHA integration Cloud Armor Bot Managementintroduces an enhanced setof capabilities designed tohelp you deploy an effective botmanagement strategy These capabilities includeadditional rule actionsto redirect traffic as well as the abilityto insert a custom headerinto a suspicious request but otherwise allow it toreach your application Customers use this customheader to tag suspicious trafficfor alternative processingwithin their workloads like sending toa honeypot or notcompleting a potentiallyfraudulent transactionwithout notifying thebot that it was detected We re also proud to sharethat one of our most highlyanticipated features rate limiting is now available in preview With Cloud Armorper client rate limiting users can ensure thatno client exceedsa pre configured thresholdthrough two new actionsfor any existing ornew Cloud Armor rule With the throttleaction  requestsfrom individual clients will bedropped when the client beginsexceeding the prescribed rateto ensure that  on average the client srequest volume stayswithin the configured threshold Similarly  therate based ban actionaims to punish abuse of clientsby temporarily banning themfor a configured amount of timeif they exceed your threshold And clients can beidentified by their sourceIP  X Forwarded For header or any arbitrary header like a sessionidentifier or a user ID Cloud Armor rate limitingis an effective meansto detect and thwarthigh volume DDoS attempts mitigate credentialstuffing attacks all while ensuring theavailability and reliabilityof your workloads Finally  Cloud Armoredge security policiesare available in preview toprovide Cloud Armor protectionin front of Cloud CDN cache orGoogle Cloud Storage buckets The new edgesecurity policy typescan be applied to protectedservices in parallelwith the existing back endsecurity policies The new policy typesare stored  evaluated and enforced even furtherupstream in our edgeinfrastructure before a requestcan be served from the CDN scache or from a GCS bucket Requests that make it throughthe edge security policy thatcannot be served from cacheare still evaluated by anyconfigured back end securitypolicy for full Layer 7filtering and WAF before it isproxied back to your protectedapplication or origins Edge security policiessupport IP and geo filteringto filter out unwelcome endusers or geographies in orderto comply with securitycompliance  export controls or licensing agreements While delivering these muchanticipated capabilities we are continuingto invest heavilyinto expanding Cloud Armorscope and depth of protectionto help protect yourapplications and services So stay tuned Now I ll hand it backto Peter to tell youabout more great developmentsacross our network securityproducts PETER BLUM  Thanks  Emil It s great to hear aboutall the good stuff that shappening with Cloud Armor Next  I d like to get intowhat s new and innovativewith network protection I m going to cover innovationsin Cloud Firewall  CloudNAT  and Cloud IDS So let s start withCloud Firewall This is our integratedvirtual firewall servicefor ingress  egress  and insidethe cloud network protection It s fully integratedwith Google s SDN fabric And the firewallpolicy enforcementis distributedacross that fabric And to make firewallpolicies dynamic we allow for easiermicrosegmentationand provide a shift to azero trust workload protectionposture by using networktags and service accounts This removes the need forencoding static IP addressesinto firewall rules andmakes protection dynamic The example I like to give isyou can add new microserviceswith a tag And they ll automatically getthe established protectionrules And in terms ofrecent innovations we have two great additions First is FirewallInsights  whichprovides information that helpsoperators learn about whichfirewall rules are used never used  and overshadowedby other  more broad  rules This helps identifymisconfigurations attacks  and allows tighteningcontrols for better security And then we just released anew capability in preview herethat highlights rulesthat have perhapsover granted permissions which is a great opportunityto tighten up yourfirewall rules Next  we releasedhierarchical firewall policiesearlier this year  whichallows creating and enforcinga consistent set offirewall policiesacross an entire organization This improves protectionand simplifiescomplex  large deployments The great thing is that onceyou set up your organization sstandard set offirewall policies any new projects addedautomatically get that policy But it still allowsfor local overridesdeeper within theorganization when required We find it s reallyhelping customers simplifytheir firewallconfigurations by reducingthe number of firewall rulesneeded across an organization Next up is Cloud NAT This is our integrated networkaddress translation service Like our firewall  it sbuilt into the fabricand providessecurity and privacyfor outbound connectionswith the abilityto use static orauto allocated IPs And like our firewall it s built into the networkfabric versus a single gateway So you can scale it upto thousands of VMs And in terms ofrecent innovations we have two great additionsin preview now at the show The first is NATrules  which allowfor tighter control overwhich external IPs areused for differentinternal requesters For example  youcan ensure a servicemaking requests out over NATalways uses the same public IPaddress And then we introduced anew capability in previewcalled dynamic port allocation This allows for higher scaleby automatically optimizing NATport utilization in real time Previously  customershad to manually configureport allocations across VMs And they were generallyover allocated Now the system does it foryou on its own automatically And we just recently announcedour new Cloud IDS product which provides formanaged network threatdetection with highlevels of securityefficacy and low noise It s quick and easy to deploy It s managed  scaled and operated by Google with high performance andavailability all built in It s super easy to set up With a few clicks you just selectwhat you want to monitor which types of monitoringyou want to enable  and thenthe logging and telemetryis all integrated intoall our own systems like Cloud Logging  Chronicle And all the alerts flow to SCC We can also send the alerts andtelemetry to third party SIEMand SOAR products as well And then we re super excitedto share that we partneredwith our friends atPalo Alto Networksto embed theirindustry leading threatdetection technologiesin the product So this ensures we haveindustry leading detectioncapabilities  bothin terms of breadthof coverage and efficacy And it s all backedby their expert production proven technologiesand world class threatintelligence teams but all nicelyintegrated into Google Cloud sold and supported by us In fact  I wanted you to hearfrom our GM of IaS  SachinGupta  and Palo AltoNetworks s SVP  Anand Oswal more about the collaborationthat led to this product  MUSIC PLAYING ANAND OSWAL  The introductionof Google Cloud IDS  SACHIN GUPTA That s right  Anand It s been anincredible partnership And we re now taking itto a completely new level Over the last year our engineering teamshave worked together to builda new joint product thatcombines the industry leadingcapabilities of Palo AltoNetworks in advancednetwork threat detectionwith the simplicity andscale of Google Cloud We re calling this product CloudIDS  a cloud native servicedelivered to all our customers With just a fewclicks  customerscan inspect traffic  getapplication level visibility and detect advanced threats They don t need toworry about throughput They don t need toworry about redirectingnetwork traffic to the service It s all done ina cloud native wayas part of the normal workflow They don t need toworry about scale This is a service that s alwayson  that is highly performant and that scales on demand This is Google Cloudsimplicity at its finest And it s something thatcustomers should considerdeploying in every single VPC Anand  why don t you talkmore about the strengthof the security capabilitiesthat Palo Alto Networks bringsas part of Cloud IDS ANAND OSWAL  Absolutely  Sachin The core of what we doin the Cloud IDS productis a network threatdetection engine This is a machinelearning powered engine It looks at over 15 trilliontransactions every single day With all our network securityproducts and our endpoints we are constantly lookingat millions and millionsof samples of malware These are processed to ensurethat we protect our customersfrom day zero threats We are pushing over 4 3million unique securityupdates every single day And that s not enough In addition to ourautomated analysis we have 200 of ourfinest threat researchersof our Unit 42 organizationthat do in depth researchand have the best understandingof the entire threat landscape But Sachin  the pointyou talked about combining simplicityand showing that you rehaving security   we arebringing these two togetherwith this joint product SACHIN GUPTA  If you d liketo learn more about Cloud IDS go to cloud google com ids PETER BLUM  We lovethe collaborationwith Palo AltoNetworks  which hasled to a super easy to deploynetwork threat detectionsystem  but withproduction proven highefficacy  all poweredby Palo Alto Networks Well  thanks so muchfor joining us today And thank you  Emil  forco presenting with me We were glad youwere able to joinus to learn more aboutinnovations in DDoS  firewalls and network threat detection You can visit us on theweb at cloud google com products networking And then the Cloudnetworking teamhas a great set ofsessions during Next where you can learn moreabout how we re simplifyingnetworking  delivering newservices  and all about whatwe re doing on the edge Thanks again WILLIAM DENNIS  Hi  everyone I m William Dennis  a ProductManager on Google Cloud workingon GKE Autopilot I ll be joined laterby my colleagueGary  who s going to takeyou through a few demos I m excited to talk to youtoday about GKE s new Autopilotmode of operation to helpstreamline your applicationdelivery and management Before getting into thedetails of GKE Autopilot I d like to quickly goover some different optionsyou have today fordeploying your applications You can run them the traditionalway on VMs directly using GCE or build them forserverless with Cloud Run And sitting in the middleis Kubernetes with GKE As you move up thelayers of abstraction from raw VMs where you manageeverything  through Kubernetes which offers containerorchestration and management to serverless  where you justprovide your container to run your operations get easier asthere s less for you to manage But your deploymentchoices and flexibilityis a little bitreduced as you go upthe stack at the same time That s essentiallythe trade off You re trading the flexibilityto deploy things howeveryou like with the need to manageall that custom configuration So what makes Kubernetessuch a popular choice The main reasons Ihear are that youhave some requirements thatare not necessarily solvedby a higher order product While it s great if youcan adapt everythingto your serverless   andfrankly speaking  if you can I would recommend it  a lot of the time  thereality is there s juststuff you don t want to change Maybe you bring over a bunchof existing applications Maybe you need to run a fairlycomplex deployment with state  that needs an attachedpersistent disk And sure  in those cases people will tell you  hey you need to get rid ofthat persistent diskand go use a managed products And well  they might beright some of the time But practically speaking it s not always an option Maybe the state isfor a custom databasethat you re the oneresponsible for managing Maybe there is no managedsolution available or maybe you just don twant to change things Kubernetes is a very practicaldeployment environmentwhich understandsthat you have a lotof different applications some legacy  some new ones and they havevarious requirementsthat can be pretty complex So it is in turn a fairlypowerful system for handlingthese requirements What about compared to VMs Well  if you runstuff in VMs youhave to manage all these VMs which can be quite a burden You need to worry aboutwhere your application isgoing on which VM You have to remember thehealth of those VMs  and so on Kubernetes sits right inthe middle of this spectrum It s not too low level andit s not too high level and that makes it a veryattractive deploymentplatform for a lot of users Add to this a few otherproperties  like the factit s based onopen source technologyso you can move yourdeployments around and it becomes veryattractive indeed Kubernetes can runpretty much anywhere So you might be runningKubernetes on prem you might be runningin other clouds Using Kubernetes inall these environmentsmakes your deploymentsstandardized and portable So what are theactual componentsin Kubernetes that offerthis flexibility thatmakes it so attractive Kubernetes shipswith many componentsout of the box including Deployment for stateless webapplications  StatefulSet for custom databases and anything elsethat needs a persistent disk For batch jobs there s the Job object If you need to run anagent on every node such as to capture logs you have the DaemonSet Added to this are somescheduling constructs like zonal affinity and pod topologyspread patterns that allowyou to design your workloadaround failure domains  as wellas priority and preemption thatgive you fine grained accessto the scheduling priorityto enable rapid scaling ofhigh priority workloads At a technical level all these constructsare represented inKubernetes objectsthrough the Kubernetes API You can describe a whole bunchof things  even at this level at the Kubernetes API level what zones you want to run in or how you want to spreadyour pods for fault tolerance Say if you want to co locatetwo different apps togetheron a node so that each machineis running the frontendand backend application  whileat the same time being spreadout over a zone you can do that Now Kubernetes does have abit of a reputation for havinga steep learning curve After all  there s alot of different thingshere  many of whichI just talked about And that reputation maybe somewhat deserved But let me make acouple of claims Firstly  if you just needto deploy a simple webapplication  thenas my colleagueGary is going to demonstrate it s not that hard at all You only need to learnto API constructs  a deployment  whichhouses your code and a service  which is howyou expose your applicationto the internet   that s it So don t feel like you have tolearn everything all at once but know that it s there ifand likely when you need it And secondly  as aprofessional  you renot necessarily alwayslooking for the simplesttools out there Simple is good don t get me wrong But what you reallyneed are powerful tools You want to make sure that thedeployment tools you re usingcan actually deliver all thethings that you need to do And Kubernetes is reallygood at doing that so I d say it s worth learning Now one of the things thatmakes Kubernetes a little morechallenging  though  isthat you traditionallyneed to learn two systems You need to learn theKubernetes architecture   thatis  the open sourceAPI components above the line onthe screen here And you also need to learnthe platform API  like GKE s At the GKE layer  you createthe cluster and provisionit with the compute capacityyour workloads need  or set upautoscalers to do that for you And once created  you havea shared responsibilityfor those computeresources  and mayneed to debug nodeissues from time to time This is just ourKubernetes works and the overhead you incurfor getting the benefitsand power of Kubernetes At least  that show it used to be This is why we createdAutopilot for GKE Autopilot joins what we now callthe standard mode of operation giving you two ways to use GKE With Autopilot  mostof the platform detailsare handled foryou automatically It s still the same GKEthat you know and love We just now have a newway for you to operate it This is the Cluster Creationpage for GKE Autopilot As you can see it s pretty simple In fact  these areall the settingsyou need to getyourself a functioning production grade cluster You just need to giveyour cluster a name pick the region  andconfigure network So hopefully coming upwith a name isn t so hard As for the region  you mayhave a favorite already Otherwise  generallyyou would justpick the one closestto your users For networking  if youhave public networkingyou don t need todo anything more For private networks you just needto specify your subnetsand ranges like normal and that s it That s really it Sure  there s a couple ofadditional security featuresthat you can enable ordisable below the fold or you can just acceptour secure defaultsand change it laterif you need to Importantly  there is no nodeor autoscaling setup needed Just go ahead and clickthe Create button get yourproduction grade cluster Going back to the API view ofthe world  what does this mean Well  basically we shrank thewhole GKE API service downto a single core the create core As for the KubernetesAPI  which ishow you representyour workloads well  that s still there And this is really the point Our goal with Autopilot is tosupport most of those workloadsthat GKE standard does today In particular  we still supportall those stateful workloadsthat need a persistent disk Our goal here is not to changethe Kubernetes API  justthe platform it runs on So how does this work How can we still offer most ofthe Kubernetes features of GKEstandard  while reducingthe platform API to just thecreate function Well  with Autopilot  youdescribe your workloadsin Kubernetes terms as you always have like your container imageand the resources it needs Once scheduled on a GKE clusterrunning in Autopilot mode we ll automatically figureout the node resourcesneeded to run thoseworkloads and provisionenough capacity for you For example  if your containerrequests two CPU coresand 6 gigabytes ofmemory  we ll provisiona node big enough to handle it We apply the bestpractices that we velearned from sixyears of running GKE giving you a production gradecluster without the hassle So that s handy But what about afterthe cluster is created This is actually where the realbenefit of Autopilot comes in With a traditionalKubernetes platform including GKE s standardmode of operation there is a sharedresponsibility over the nodes While GKE standard handlesthe lifecycle of those nodes for example  the creation updating  deleting and offers severalfeatures to help automate like auto repair   atthe end of the day those nodes are still VMsthat you need to manage You have admin access tothem and can potentiallymodify them to behave in waysthat GKE might not expect So for Autopilot  welooked at this and asked do you come to GKEfor the abilityto be the root useron those nodes Or are you primarily lookingto run your own applicationson this platform We believe for most  the goalis to run your own businessapplications So that s what we offer here In this mode you re responsiblefor describing your workloadsand running them If your workload crashesdue to a bug in your code I hate to say it  it sstill your problem But most other componentsrelated to the nodesare now Google s responsibility along with the cluster itself This is a big deal And so with this  weintroduced a pod level SLAfor the first time on GKE It s three 9s This means you now have acluster that s easier to set upand easier to manage So as a developer you re more productive and in production you spend less timeon the minutiae ofmanaging the thing and more time runningyour application  whichis presumably what you reultimately here to do The third major differencewith this mode of operationis how resources are built Since you describe everythingat a pod level  we billyou at a pod level as well It s kind of intuitiveand it bringswith it some nice benefits One benefit is you don t haveto be a so called Kubernetesbinpacking expert  because youno longer care if your node has32 pods or 0  or whetheryour node s CPU hasbeen fully allocated or not Because all of thatis now our problem Our system is responsiblefor packing pods onto nodes And if there s any wastedspare capacity  that s on us You don t pay for it So it s a really nicebilling model as well And with this comesanother nice attribute If you re runninga multi team setup you can now figure out howmuch each namespace costs justby adding up all the podrequests in that namespace And we even have toolsto help you with that What s more is that Autopilotcomes with a strong securityposture right out of the box We implement the GKEhardening guidelines and since the nodes aremore locked down in orderfor us to providemanagement and that SLA you have the benefit ofthe reduced node surfacearea as well You won t need to worryabout unpatched nodes either since we will keep everythingup to date automatically And if automaticupdates worry you fear not  as wehave several toolsto manage that as well including maintenance windowsand pod disruption budgets So coming back to the Kubernetesworkload constructs I presentedearlier  and oneof the main reasonswhy people use Kubernetes in thefirst place  let s have a look Do they work on Autopilot The answer is  for the mostpart  yes  and in the same way Deployments  statefulset  jobs  theyall work exactly the same way Yes  even that complexstateful custom databasewill just work on Autopilot withno additional platform specificconfiguration needed You can move it right on over Let me repeat this because it s so important We understand thata lot of usersrun stateful workloadsin their clusters So when we created Autopilot  wedidn t take any shortcuts here Everything just worksfor stateful setas you would expect from GKE Now since podsrunning on Autopilotdon t have admin privileges it does alter some use cases especially for DaemonSets But this is mostly in the domainof administrative functions The exact type of functions  wehope  won t be needed anymore And we ve engagedseveral key partnersto deliver theirsolutions on Autopilotfor logging  monitoring and security in the same waythey work on GKEstandard  coveringmany of the popularDaemonSets that we see There s a coupleof other features like GPUs  which GKEstandard offers that scurrently lacking in Autopilot For the full list  see ourcomprehensive Autopilotoverview documentation I ll put a link inthe description below So we launched Autopilotabout half a year ago and companies likeUbie are already seeingthe benefit of this approach Ubie  if you haven theard of them yet it s an innovativemedical technology startupbased in Japan They found thatwith Autopilot theywere able to focusmore on doing the thingthat they createdtheir company to do which is to build betterhealth care solutions  and lesson managing the clusterand all the infrastructure You can read allabout this case studyon our website  whichI highly recommend Again  I ll put a linkin the description below And with that  I want tohand it over to my colleague Gary  who s going to runthrough a practical demo usingAutopilot Over to you  Gary GARI SINGH  Thanks  William In this demo  we ll tryto deploy a standard webapplication with a Redisbackend on Autopilot The application hasthree services  a Redis leader with a singlereplica  a Redis follower which actually has threereplicas  and a Redisand a frontend webapplication  which has threereplicas as well Now let s deploythis application All right Now let s check thestatus of our pods Now notice that we have onein the Ready state and severalin the Pending state That s because we actually don thave enough compute resourceprovisioned in the cluster But not to worry  Autopilotwill handle this for us Behind the scenes Autopilot is actuallyprovisioning additionalcompute for us Let s take a look You can see that herewe ve added one new node and we have two new nodes in theprovisioning status right now If we take anotherlook at our pods we ll see that we actuallyhave a couple morein the ContainerCreating stage We ll wait for a littlebit  but soon we llsee all of our application So all of our pods willactually be deployed All right Let s check our podstatus one more time All right All of our pods are Running We ll also notice we now havefive nodes up and runningas well to meet the demands Now let s actually testout this application We need to get the addressof the frontend application All right We can see that the externalIP address here is that We ll copy that  pasteit into our browser All right We have the Guestbookapplication We ll type in afew messages here All right we ll close this out But if all is working  we shouldsee these messages back againcoming from the cache All right Our application is upand running  all as well Now in this demo  we showedthe ability of Autopilotto automatically anddynamically provision computeas needed by our workload We didn t get into autoscaling but William s goingto tell us a bit about that And then I ll be backwith another demoto show you that as well WILLIAM DENNIS  Thanks  Gary So as Gary showed  Autopilotwill automatically provisionresources to scheduleyour pods under the hood based on the resourcesthose pods request But what about scalingthe workloads themselves For this  GKE offers topod level scaling options The vertical pod autoscalercan increase and decreasethe resources used by the pod It does this bymonitoring the resourceutilization of the pod And it can adjustthe pod requests either increasing itto improve performancewhen it notices moreCPU might be needed or decreasing it  whenit sees that the CPU isbeing underutilized And when you Autopilot that s all you need to do Autopilot willensure that you havethe right node resourcesavailable to runthose resized pods And of course  you only pay forthe precise resource requestsof your pod To enable in GKEAutopilot  createa Vertical Pod Autoscalingobject that referencesyour deployment You can optionallyexclude certain containersin the pod from the VPA I ll put a link tothe documentationin the description below The other workloadscaling optionis the Horizontal PodAutoscaler  or HPA The HorizontalPod Autoscaler canchange the number of podreplicas in your deploymentbased on demand With the HorizontalPod Autoscaler you set a metric toobserve  and a target and the HPA will attemptto create more pod replicasto keep that targeting range For example  if you wantto have 1 pod for every 100requests per second thathit your load balancer it could do that With GKE Autopilot  you canjust set and forget this HPAand let it handle theusage spikes for you as Autopilot will provisionand de provision automaticallythe compute resourcesneeded to handle those pods And of course  as always  youonly pay for running pods To enable  create aHorizontal Pod Autoscalingobject that references thedeployment to be scaled specifying the minimum andmaximum replicate counts the metric to be observed and your desired target Horizontal Pod Autoscalingcan be configuredto use CPU  memory or custom metrics like requests per second or pub sub queue depth Now I d like to handit back to Gary  who sgoing to show you a reallycool demo of using Autopilotwith a Horizontal Pod Autoscalerto handle a pub sub queue Back over to you  Gary GARI SINGH  Thanks  William In this demo  we re goingto show how to auto scalea pub sub workloadbased on metricsprovided by GoogleCloud Monitoring As you can see  we ve alreadycreated a topic named echo and a correspondingsubscription named echo read The goal for this demo is toscale the number of subscribersfor this topicbased on the numberof unacknowledged messagesremaining in the subscription As you can see on thescreen  pub sub convenientlyprovides this metricfor us  and later we llactually show how toconfigure the Horizontal PodAutoscaler to use it Now this is all made possibleby using the custom metricsadapter  which will providecloud monitoring metricsfor our cluster Now while William wastalking I actuallytook the liberty ofdeploying this adapter so let s take a quick lookjust to make sure everything srunning All right So now we see we have our custommetrics adapter up and running Now we can actuallycheck to make surethat the metric that we wantis available by queryingthe Kubernetesexternal metrics API And here in this box you ll see that wehave our subscription numberundelivered messages metric And we actually see that we haveit for our actual subscription So we ll be set up to usethis in the HPA later on But before we candeploy our application we ll need toconfigure an identitythe app can use to authenticatewith Cloud pub sub Autopilot workloadsuse workload identity which is the recommendedway to access Google CloudServices from applications It provides improvedsecurity and manageabilityover using things like secretsor mounting token files  whichare insecure First  let s create a dedicatednamespace for our application Great We ll use the namespace pubsub Next  we ll needto create a serviceaccount that we ll actuallyuse in our deployment metadata We ll call itpubsub service account We also need to create aGoogle Cloud service account And we ll need togive it permissionto actually access pubsub Here you see we re givingit the pubsub editor role Next  we ll actually bind theKubernetes service accountwith the Google service account And finally  we ll annotatethe Kubernetes service accountso that it knowsabout this binding Now let s take a look at ouractual application itself The actual applicationitself is a simple appwhich uses a Python SDK toactually just pull messagesfrom the subscription Now our deployment for thisapplication is pretty simple We have a standard container We re going to name it pubsub And as you can seehere  we ve actuallychosen the serviceAccountunder whichthis particular deploymentwill actually run Now along with that  wealso have a Horizontal PodAutoscaler resource And as you can see here  weshowed this metric earlier We re using the external metric We re using theundelivered messages We want this to be for ourecho read subscription And our goal here will bethat we never want to  no matter how manysubscribers we have we want the average value ofthe backlog to actually be 5 All right So let s actuallydeploy our application First  we ll just setthe default namespacefor our context Then we ll actuallydeploy our pod We ll check to makesure our pod is running All right We ve successfullydeployed that Next  we ll actuallydeploy the HPA resource Let s take a quick look Here  we canactually see that wehave it set up as atarget of 5   that swhat we configured in there The MINPODS is 1 andthe MAXPODS are 5 And right now we don tactually have any messages so the backlog is 0  and ourDeployment is actually fine Now  let s actuallygenerate some load We ll do that by publishingabout 100 messagesto the topic itself All right Now that our messages are there let s take a look at the HPAitself As you can see here  it sactually updated now We actually see that the valuethat it s seeing here is 22 and that it s lookingfor a value of 5 So if we actuallydescribe the HPA we ll see that it actually wantsto trigger up and autoscaleour stuff up to themax size of actual 5 Let s take a quicklook at our pods You ll see that we ve nowactually scaled up to 4 pods and we have a fifth pod pending So as you can see here  we wereable to build an application have it subscribe set the metric basedon the unacknowledged backlogfrom Google Cloud metrics and autoscale our application And once again  thegreat thing herewas we didn t haveto worry about anyof the underlying computeresources required for thisto actually work All right You ll see that our containeris actually creating And yeah  that s aboutit for this demo So thanks  William Back over to you WILLIAM DENNIS  Thank you  Gary That was a reallypowerful demonstrationof what Autopilot can do All you need to do is addyour Kubernetes workloadand we ll take careof everything else Well  that s GKE Autopilot To learn more  you can visitthe link on the screen or those in thedescription below Thank you LEE YANCO  Hey  everybody Welcome to App 203 introducing our new managedservice for Prometheus My name is Lee Yanco I m a Product Managerwithin Google Cloud I m joined today by BartoszJakubski  Senior DevOpsEngineer at OpenX So today we re going totalk about some challengeswith Prometheus  we re going tointroduce Google Cloud ManagedService for Prometheus do a bit of a deep divein the components of the system talk about how it comparesto cloud monitoring and then OpenXis going to talkabout their journeywith managed servicefor Prometheus So  challenges with Prometheus Prometheus is great It works really well There is a very good reasonwhy it is the de facto standardfor Kubernetes monitoring Managing Prometheus for smalldeployments is pretty easy Managing Prometheus atscale can be very hard So first of all  Prometheus canbe hard to scale horizontally When you have a smallcluster  you justput a couple of gigs of RAM inthe server and it just works Your cluster getsa little larger you have to scale up bythrowing more RAM at the server Eventually yourcluster gets so largethat you have to shardby frontend applicationand backend applicationor by A through M and Nthrough Z orsomething like that Somewhat interestingly fora cloud native service Prometheus doesn t have aparticularly good horizontalscaling story Prometheus can behard to make global So when you re small and youonly have a few clusters it s no big deal You have a couple of differentdata sources within Grafana you swap between them  it works Eventually you might want to geta global view  so you federate Eventually your Prometheusdeployment grows even more So you have multiplefederation servers You might simplify itby switching to Thanos So you have Thanos queryingyour Prometheus servers And then you gohybrid or multicloud and you have Thanos queryingThanos querying your Prometheusservers And then somebodyon your team addsuser ID or IP address as alabel  cardinatily explodes and the whole thing fallsover  which leads meto my third point  managing Prometheuscan be an ongoing operationalburden and a thankless task Your end customersdo not care how goodyou are at managering monitoringinfrastructure They care how good you are atkeeping your app up  of course But the monitoringinfrastructure partis really a thankless task None of this is hard it s just toil  it s work It s 20 to 30 hours ofsomebody s time  every week in perpetuity Here s the thing Google has been throughthis exact experience 10 years ago  wealso had a systemfor monitoring where wehad local servers thatfederated intoregional servers thatfederated into global servers We built some newtechnology for ourselvesto solve thisproblem for Google And we re excited toshare this next generationPrometheus compatiblesolution with you So we re introducing GoogleCloud Managed Servicefor Prometheus  a drop inreplacement for Prometheusand Thanos This is the managed servicefor Prometheus ecosystem I m going to use thisto frame a conversationabout the differentparts of the systemand how they benefityou as a customer So in the center sits Monarch Monarch is Google s planet scaletime series metric storageand retrieval database Monarch is the sameexact system that weuse to monitor internal Google So ads  search networking  YouTube Google Play  youname it  all useMonarch to make surethat their apps are upand running and stable We have a separate instance ofmonarch called Cloud Monarch Cloud Monarch is thesame instance that sused by Cloud Monitoring And Managed Servicefor Prometheusshares some of thesecloud monitoring APIs And I ll get backto why that s superinteresting in the second On the left side  wehave the collectors So our collector is compatibleanywhere that Prometheus runs Because our collectoris Prometheus It s Prometheus plus acouple hundred lines of codethat wrap things up in aproto and send it to Google So we have two differenttypes of collectors So the self managed collectorsuse your existing Prometheusoperator configs toconfigure the scraping So you just copy pastethose  switch out the binary and they just work If you re in GKE  we alsohave managed collectors in which case Google will handlethe scaling  the sharding the deployment ofthese collectors And you configure thescraping using CRDs On the query side  becausewe have implemented PromQL your existing Grafanadashboards written in PromQLjust work  your Prometheusalerts and alert managerconfigs just work  andyour PromQL based tools any PromQL basedtool that you mightfind on the internet  anycommunity curated dashboard any random alert template youmight want to use  just works And because we re using thesame APIs and backend systemsas Cloud Monitoring your Cloud Monitoringmetrics   your freemetrics  your free BigQuery your free Anthos  your freeIstio  your free Dataflow your free Pub Sub whatever you use those metrics will beaccessible within Grafanausing PromQL  right alongsideyour Prometheus metrics giving you a singlepane of glass And likewise  because we reusing the same backend system your Prometheusmetrics will alsobe available withinCloud Monitoring So you can use advanced featuressuch as SLO alerting  cloudalerting  our Dashboard Builder et cetera  with your Prometheusmetrics in Cloud Monitoring We built this to be as drop inof a replacement as possibleso that you can keepdoing what you re doingand just outsource the hardpart  the metric storageand retrieval part  to us Now let s do a bit of a deepdive of some of the componentshere So let s talk about Monarch the planet scale in memorytime series database It s the same tool we useto monitor internal Google Monarch has over a trilliontime series in RAM 14 quadrillion points ondisk  writes 2 and 1 2terabytes a second  processes16 million queries per second Whatever you can throw atus  we can handle  promise Monarch is configured fortwo year retention  includedfor all metrics bydefault  And Monarchhas regional storage withad hoc global aggregationsat query time So if you say you want yourdata to live in cloud zone x your data will livein cloud zone x On the collection side we re compatible anywherePrometheus is deployed There s two flavors managed for GKE environmentsand self hosted for otherKubernetes environments This means that managedservice for Prometheusis hybrid  andmulticloud compatiblefrom the very beginning Our collectors areopen source  so youcan see what s running withinyour clusters of course And there s no need tostore any data locally So you can use way lessRAM to run these collectorsthan normal Prometheus On the query side  we refully PromQL compatible So your existing Grafanadashboards just work You can query yourfree GCP metricsalongside yourPrometheus metrics getting you a singlepane of glass And read permissionsare enforced on groupsof projects at query time So this is reallynice for customersthat have tenants of their own So you can do somethinglike send tenant 1 s metricsto project A  send tenant2 s metrics to project B And then you have aseparate permissionsthat has both project A andB included for your SRE teamsto get a global viewacross all your tenants On the alerting andrecording rule side we work with PrometheusAlert Manager out of the box We work with PromQL basedalerts and recording rules anything you mighthave developed already out of the box And we are compatiblewith Google Cloud salerting as well So as you can see here  webuilt an interactive query UIwithin Cloud Console You can see this isworking software This is me queryingPrometheus metrics And you can see themreturning to the screen We also have fullGrafana compatibility So you can see this isme just changing the datasource from local to foreign And it just works Nothing changes So this is great for you asa customer  terrible for meas a presenter Because reallynothing should change When you swap fromyour local Prometheusto Managed Service forPrometheus as your data source it should be exactlythe same data And that s what you reseeing right here So let s do a brief comparisonwith Cloud Monitoring because we do now have twoproducts within the same area So there are two productscoming out around the same time There s Google Cloud ManagedService for Prometheus  whichis Google s recommended wayto run Prometheus at scale and there s Cloud Monitoringand the upcoming GKE WorkloadMetrics  which isGoogle s recommended wayto monitor Kubernetesapplications So maybe another way tothink of this might be if you  as a customer  ifyou re saying to yourself you know what  I don t wantto deal with any monitoringinfrastructurewhatsoever  I wantGoogle to manageeverything for meand I want to get the bestpractices of Google monitoringand Google SRE s philosophybuilt into my monitoringinfrastructure  then CloudMonitoring and GKE WorkloadMetrics is the product for you If you re sayingto yourself  youknow what  I like Prometheus  Ilike the Prometheus ecosystem I want to keep whatI have already I just want to outsource thehard part of metric storageand retrieval scaling  thenManaged Service for Prometheusis the product for you Another way to think of thismight be  how much do youwant to be managed So with ManagedService for Prometheus you get scalablecollections  global queries and long term retention includedas a managed service  of coursewith open source interfaces Whereas with Cloud Monitoringand GKE Workload Metrics you also get alertmanagement  dashboarding and unified metrics  logs and traces as managed servicesas well Remember  because we reusing the same backendand share a lot of the sameAPIs  swapping between theseis pretty easy So you re never lockedinto your decision You can choose the best solutionfor you at any given time We are in preview right now We expect to be ingeneral availabilitysometime early next year But we would love tohave you in preview You can sign up atg co cloud managedprometheus Once again  that sg co cloud managedprometheus So I m going to turnit over to Bartosznow to talk aboutOpenX s journeywith Managed Service forPrometheus in preview BARTOSZ JAKUBSKI  Hi  myname is Bartosz Jakubski and I m a senior DevOpsEngineer at OpenX OpenX is a globaladvertising exchange We handle over 200 billiondynamic requests per day In 2019  we completed ourmove from on premises serversto GCP Before moving to cloud we ve had a custom monitoringsolution which isbased on Graphite and use Grafana nowfor visualization We ve used migration toGCP as an opportunityto update our monitoring stackto Prometheus based solution We re a global company So our monitoringspans many GKE clustersin multiple GCP regionson three continents And right now we ingest over350 000 samples per second We do use CloudMonitoring too  mainlyfor metrics fromservices managed by GCP When it comes toPrometheus  we runwhat I d call a pretty standardglobal Prometheus setup We use Thanos forlong term storageof metrics and global view We do use Grafana with about100 dashboards right now And majority of ourother team works on datafrom more than one GKE cluster So it s usually quite global And we store about twoyears of metrics right now We really do like our stack Grafana  Prometheus  andThanos work well togetherin the cloud Most importantly  it just works But there are some issues Running full kind of stack canbe quite an involved process You can see here a listof Thanos componentsthat are running in ourglobal observability cluster You have to deploy  manage and monitor all of them In some cases  likewith Thanos start you have to manuallyshard instances by timeto ensure they won trun out of memoryand performancewill be adequate Scaling  managingresource requirements modifying shardingconfiguration is something we haveto do regularly Just keeping the lightson does require work For example Kubernetes upgrades mayrequire modifications andmanifests for our components There s alwayssome effort needed We do have someperformance issues Some of them could be solvedon modifying our setup But again  it needs timeand resources to do so Due to a large amountof components involved our global alertingand monitoringis a bit more fragilethan we d like Also  for global setup intercluster connectivityis required It is another piece ofcloud infrastructurethat you have to setup and secure yourself Also  more componentsand more involved setupmeans that moreknowledge or more timeis required to makechanges or fix issues Why we decided to use ManagedService for Prometheusin preview First  as a company we are alwaysinterested in reducingcomplexity and maintenanceburden One way to achieve that isby using managed servicesinstead of rolling out andmaintaining our own ones Another reason is Prometheusquery language compatibility This is a bigselling point for us Complete migration toany other query languagewould require a major effort And ability to rununmodified queriesmakes the move possible Self managed collectoris just Prometheus You just deploy it like youdeploy your regular Prometheus It can run alongsideyour Prometheus And to do so  it onlyrequires small changesin existing configuration So we installed collectorsin our production clusters gathered some data  addeda data source to Grafana and we got the samegraph basically So it turns out that Grafanadashboards indeed do work We do use more advancedPromQL queries in some places And their compatibilityis pretty good too So what opportunities for OpenXManaged Service for Prometheusprovides This is a simplifieddiagram of componentsthat we are running in ourglobal monitoring stack If we were to move to ManagedService for Prometheus in transitioning period  whenwe will still need accessto our older data incloud storage bucket some components canalready be eliminated Once we are fully migrated toManaged Service for Prometheus most of the components willdisappear with only Prometheuscollectors remaining This reduction in complexitywill get us some time back We will no longer need to adjuststorage volumes and resourcerequests There will be no need tomodify sharding in Thanos startinstances Fewer componentsmeans fewer incidentscaused by problems in alertingand monitoring system Last but not least better integrationwith Cloud Monitoringwould allow us not onlyutilize Cloud Monitoringtools like SLO alerting but would also simplifyand streamline accessto metric data No more combining two querylanguages in single graphin Grafana to combine ourapplication metrics with for example  loadbalancer metrics Thank you for letting meshare my OpenX Prometheusstory with you Now back to Lee LEE YANCO  Thank you  Bartosz Once again  we would loveto have you for the preview You can sign up atg co cloud managedprometheus Thank you for watching  MUSIC PLAYING RICHARD SEROTER  Hey Welcome  everyone I m Richard VIDYA NAGARAJANRAMAN  Hi  I m Vidya RICHARD SEROTER  We re reallyglad you joined us here today We re going to have alot of fun talking tech And I wish you could behere with us personally but we re live at least And that means we cankind of say anything Like  I could say  Googleshould bring back Google Reader and I m still working here So that s awesome If you have otherhot takes  you wantto share some feedback put it in the chat We can see that stuff liveright now  so keep it coming VIDYA NAGARAJANRAMAN  If you can tsee the liveinteractivity  please clickon the blue Join LiveInteractive Experiencebutton on the nextwebsite and we llsee you on the other side For those of you just joiningus from the Spotlight session you heard about some of ournew product announcements including Anthos for VMs  AnthosMulti Cloud API  and the GoogleDistributed Cloud RICHARD SEROTER  Awesome In the next 15minutes  we re actuallygoing to show you all of thesein action  which is great So  here s what we regoing to do together First we re going to migratesomething and demonstratehow to get some newvalue from an old systemby moving VM basedapplications to a newercontainerized platform And we ll start by showingyou how to run a legacyapp in a serverless platform So buckle up  Vidya VIDYA NAGARAJAN RAMAN Next  we ll create We ll build a newexperience for customersthrough our AI poweredsoftware  and then we lldeploy with somebrand new deploymenttools  unique to Google Cloud RICHARD SEROTER  Love it Finally  we re going to beexpanding our deploymenttargets We re not just shippingto Cloud anymore but extending thatawesome path to productionto other public clouds andeven closer to your customersat the edge All right  so let s switchsome things up  shall we I now work at Cymbal Shopsprobably because of that GoogleReader crack So let s pretend thatI am the director of ITat this company  Cymbal Shops And like many of you our global businesshas been shifted because ofincreasing demands online And at the same time we have some problems So old software was nevermeant for all the customerload we re putting on it now It s holding us back We re trying to offer mobileexperiences  real time stuff We have to do itas quick as we can And look  I can trewrite everything So how do we reduce some costand add some capabilitiesto that existingsoftware a little easier without much too much effort So I want to start with asmart migration approach I want a repeatable efficient wayto move old systems tonewer stuff  maybe evena serverless one but Cymbal Shopneeds help from our trustedpartners at Google Cloud Help  Vidya You re my only hope VIDYA NAGARAJAN RAMAN I got you  Richard So our fit assessment tool which you can see here is initiated from theGoogle Cloud console It helps us understand whatcan be automatically migratedand which workloadsare going to be mostsuccessful in their new home It s unique to GoogleCloud and saves you time By pointing our migrateservice at a setof over 2 000 virtual machinesin our on prem vSphereenvironment  we justgenerated a fit assessmentfor the workloads runningin each of those VMs And remember  these VMs couldbe running on Prem  in GoogleCloud  in AWS  or in Azure So now  let s take a look atthe results of the assessment See here  we have a graphicalview of our most likelycandidates for migration This tool is particularly goodat detecting compatibilityfor older Java applications like Java EE applicationservers Now that we haveassessed  let s migrate Using our migrate service I m taking a Java apprunning on WebSpherein an on prem VMand generating acontainer image thatcan run on any of our containerservices  all via automation And you can see here that theclassic Java app is now servedup by our onlypay for what you use serverlesscontainer product  Cloud Run RICHARD SEROTER Wow  I love that So this is great Let me make sure we noticewhat we just did there I mean  I don t have to manageall these partially utilizedvirtual machines anymorebecause you containerized it And I ve offloaded some of theoperating system management I don t have topatch OSes anymore That s awesome And you ve actuallygiven me accessto brand new functionalityand saved me moneyby running on a modernplatform  like Cloud Run So you just tooka classic Java appand ran that on aserverless platform That s pretty wild stuff If you like that tell us in the chatthat you have a need for puttingsome of your older systemsin these newer platforms All right  so next up let s create some new valueand get that to Google Cloud So Cymbal Shops has someproblems we want to solve We have a app that trackssome of the store performance but it runs in VMs And it needs someserious modernizationto support all those sortof new customer experiencesand can keep us competitive So this means upgradingfunctionality and even changinghow we could deliverit to production So curbside pickup super hot right now How do I add functionality thento count how many customersdid curbside pickup each day That seems really important Vidya  I don t know Can you help me with this one VIDYA NAGARAJAN RAMAN Yes  of course I can So  Richard  what does yourcurrent app consist of RICHARD SEROTER  Well thank you for asking Our current app has a MongoDBdatabase with multiple servicesthat track orders and pickups We actually want to addnew services and datato capture and analyzethat curbside pickup info And to make thathappen  we couldlook at historicalfootage from the past day and count the pickups  anddo some analysis later VIDYA NAGARAJAN RAMAN  Thislooks like a great opportunityto do two things First  we re goingto add Google CloudAI to process video footageand capture metrics Second  we ll containerizethese workloadsand put them on acontinuous delivery pipelineso that it s easy tokeep making changes RICHARD SEROTER  I like that So if you do this then we actuallyknow how many customersare doing curbside pickup maybe even how longthey were waiting And I think you regoing to fix themon my path to prod andmake it easy to packageand keep changing thesoftware over and over again I love that VIDYA NAGARAJAN RAMAN Yes  absolutely Check it out So Google Cloud VisionAPI can detect vehiclesin this footage to determinehow well curbside pickups havebeen going at each location So now  let s write some codeand deploy the updated appto Google Cloud This is exciting And I m hoping all ouraudience is excited too I m using VisualStudio Code here but I could easily useIntelliJ or any JetBrains IDE And using the no costCloud Code plug in I can easily browse Google CloudServices  add them to my app and then code and testthis container based app RICHARD SEROTER  You are thefastest coder I ve ever seen That was remarkable So that was pretty simple too I like that So now that everythingis working locally how would they thendeploy this to the Cloud VIDYA NAGARAJAN RAMAN Let me show you First  they need to packageup this app into a container You probably want yourdevelopers spending timewriting code  not Docker files And we can help with that I ll show you how we liketo package containers And while I m doingthat  audience tell us how do youpackage your containers So here is our poll  and wehave four options in our poll So I m hoping you wouldactually pick it up RICHARD SEROTER  Awesome VIDYA NAGARAJANRAMAN  So Cloud Buildis a serverless built toolthat many customers usefor continuous integration We also now supportindustry standard Cloudbuildpacks to package up aapp into a container imageautomatically and pushit to artifact registry Artifact registry storesyour container imagesin regional repositories And here you seethat we automaticallyscan for vulnerabilities RICHARD SEROTER  That s neat What else do we useartifact registry for I haven t seen asmuch about that Can you tell me alittle bit about that VIDYA NAGARAJAN RAMAN  Yeah So while we useartifact registryfor storing dockercontainer images  Richard we can also useit for storing allyour language specificartifacts in one place For instance  we just went GAwith Java  Node js  and Pythonpackages using the MavenNPM and PyPI repositoryformats respectively RICHARD SEROTER That s pretty cool So while we re waitingfor the poll results looking at some of the chat I saw some questions about how do we do day two managementof migrated workloads I think the migrate toolinggives you a container imagethat now I coulddownload to my desktop I can run in different places I have differentways to run that Some other questionsabout  again  thinkingabout how do we migrateto GKE and Cloud Run I think these tools are prettycool that they can actuallytake an app and run it in eitherone  which is pretty great And I think we re seeingthe poll results coming in VIDYA NAGARAJAN RAMAN  Yeah  itlooks like the results are in And it s amazingthat a lot of folksare already automatingtoday  and they reautomating via Docker buildcommands in CI pipeline That s excellent So now that we have acontainer in the registry I think it s time to deploy it So to deploy our app  let suse the new Google Cloud DeployService  a continuousdelivery service for deployingcontainerized workloads While the service iscontainerized and couldrun in GKE or Cloud Run the app is fairly coupledto our MongoDB infrastructurerunning in Kubernetesand is part of ourmulti cloud strategy So let s target one of ourGKE Autopilot clusters GKE Autopilot is the fullymanaged Kubernetes servicewhere Google Cloudprovisions  scales upgrades  and troubleshootsthe cluster for me Here we see adeployment pipelinethat helps us manage releasecandidates and environments Cloud Deploy helps usmanage promotion and rolloutacross these environments And once the app isdeployed to GKE Autopilot you can start using it fromeach retail store that you have You kick this all offwith a push to git and also you manually approvefinal promotion to production In fact  I m goingto have someone dropthe URL in the chatwindow now so that youcan see it for yourself It all works just like magic RICHARD SEROTER  That s cool It must be true You put the link there That s great So it s awesome So from development topackaging to deployment  I mean I think  personally this is the bestset of integrated toolsfor services and buildingcontainers that I ve seen So I think that s awesome So we re also seeingsome other folksin the chat talking about someof these components as welland what they reseeing for migratingthe apps and  hopefully clicking linksand trying to break theapp we just deployed All right So now that we ve justtaken that existing app we added AI functionality we dramaticallychanged the knowledgethat each of our storeshas about thecustomer experience That s pretty cool VIDYA NAGARAJAN RAMAN  Indeed Isn t it  Richard What else do youwant to throw my way RICHARD SEROTER  Ithink we got some time So we ve acquired afew other retail chainswho use different clouds I mean  nobody s perfect  right So at the same time  this appneeds to run in more placesso that each of thosestores can analyze curbsidepick up behavior  regardlessof what cloud they re using So I guess  Vidya my question for youis  how do I runthis app everywhere VIDYA NAGARAJANRAMAN  Yeah  that sa great question  Richard And we do have asolution for consistencyacross any environment Check out how we doit through Anthos With the newmulti cloud API  we canprovision these clusters rightfrom the Google Cloud CLIor console in otherclouds  includingAWS and Azure  whereyou already hadsome applications installed See here that I m using a singleGCloud command to create a GKEcluster on Azure Here  we havedeployed GKE clustersto Google Cloud andMicrosoft Azure I m managing it allfrom Google Cloud and I m even able to centrallydeploy and view workloadsto any of these GKE clusters RICHARD SEROTER  That s wild Thanks  Vidya So you re actuallyputting GKE   and Ithink it s the best Kubernetesin the public Cloud  anywhere I want it I mean  that s reallypowerful stuff I love that new Azure support And again  each store manager whatever Cloud they re using can actually see a reportat the end of the dayto know how they performedon curbside pickup That was the goal All right  butnow  you re makingme think that we couldprobably even do more  right We could expandsome of our thinkinghere and maybe respond to thecustomer demands in real time Could we actually evolve fromthat after the fact analysisof parking lot footageto maybe improvethe experience in real time By that  I mthinking of  could Irun this AI model againstlive camera footage insteadof recorded stuff  and maybebe able to do something with itas it happens But of course  asalways  new challengesemerge if we think ofsomething like this So off the top of my head we d have to somehow customizethis AI model because now I haveto identify the numberof cars in motion how many are waiting  isthe curbside lot picking up And I probably want tooperationalize that modeland move it closer tothe store  because I mprocessing data in real time So latency matters And then I want tointegrate all that insightI m getting with theexisting in store systemsfor the managersto use so they canmove their employees around So that s a lot of stuff I don t know Can you help me with that VIDYA NAGARAJAN RAMAN  We can So  I m calling in mycolleague  Gabriele He has been working with oneof your on prem locations Richard  in Austin And he can explain howwe improve your softwarefor edge scenarios Hi  Gabriele GABRIELE DI PIAZZA  Hi  Vidya And hello  everyone Glad to join you  Richard By the way  congratson your role in Cymbal What you re askingfor can be solvedwith our newly announced GoogleDistributed Cloud on the Edge As you mentioned  it bringsthe best of Google Cloudto the Edge It consists of afully managed CPU GPU optimized platform with a common set of Googleand third party applications You build once anddeploy anywhere As an example  here atsome of your stores So let s get going We re going to add real timeintelligence with AI modelsand process the data locallyto reduce video latency Let s get these runningon the Edge in each store So we start out by firstusing Google Vertex AI So  we train the engines andthe visual objects recognitionand classification modelsin the Central Cloud Here we are monitoringthe progress and accuracyof the model And as you can see the train modelscan now be packagedinto container images Those container images can beready to be deployed anywhere including in the Edge In this case  we ll be targetingfive different Cymbal stores From the Google console  we canchoose the Kubernetes clustersrunning at specificEdge locationsacross all the Cymbal stores Once we pick those clusters  wecan deploy the vision AI modelthat we created toeach Edge in the storeand manage thoseEdge clusters justlike any other GKE clusters The Cymbal operations team hasthe same familiar experience only now Cymbal can alsoleverage the GPU optimizeGoogle Distributed Cloud toachieve better performanceat lower latency And as a fully managedservice  Distributed Cloudcomes with the sameintegrated Google Cloudoperation andmanagement capabilitiesthat you re used to Here  Cymbal can monitor thehealth and state of their storedeployments  managecapacity scale all of these using afamiliar Google consoleand backed by GoogleSRE practices So as you can seein the video here a real time analysis ofcurbside service levelfrom the live footageled to real time insightsfor Cymbal shops to build abetter customer experienceat each store Specifically  thisnew model at the Edgeis recognizing carsin motion  telling youwhen the curbside pick uplots are filling up all executed in real time So what do you think  Richard Back to you RICHARD SEROTER  This is great You re amazing  Gabriele I can t believe you builtall that all by yourself So this actually givesall our store managersreally new insight into thereal time customer experience letting those stores allocatestaff and people basedon real time customer demand That s awesome So for the audience out there I m actually interested What sort of apps do you thinkabout running at the Edge Is that a real use caseyou re considering What sort of thingsmight run there All right So I do love whatyou built for me I don t want to be greedy but I want a little more This app isn t an island  right You re doing some cool AIcontainerized based stuff but there s a lot of thingsalready at that store location right Our notification services other back office It s all in VMs So now that we have to integrateour new customer serviceapp  these VM basedsystems  am Isigning up for completelydifferent managementexperiences across containersand virtual machines or can you do something for me GABRIELE DI PIAZZA Oh  absolutely not We re going to provideyou the same experiencewith the new Anthos for VMs We can actually help youbring those virtual machinesinto the Anthos platformand manage them the same waywe manage containers So see here that I ve moveda set of virtual machinesunder Anthos management which now gives mea straightforward wayto move and modernizeexisting apps at the Edge RICHARD SEROTER Wow  that s awesome There s nothing youcan t do  Gabriele That s great So what s powerful here is thatall the Google powered fleetmanagement in Google Cloud other clouds  at the Edge is all based on Anthos  with oneopen control plane for whereverthose workloads run This simplifies ouroperations a lot Or that open foundation makeshiring developers and operatorsa heck of a lot easier All right So last challenge for you Cymbal has this growingEuropean presence but some of thoseregional storesoperate with some restrictionson data sovereignty I don t want to sacrificeall the amazing developmentand management capabilities thatyou and Vidya have showed metoday So is this a lost cause  ordo you have something for me GABRIELE DI PIAZZA  Yes we have something for you As we announced it yesterday with the new Google DistributedCloud posted  we can use asingle hardware and softwarestack from Google Cloud but now with local computeand a local control plane It runs not only ourcontainer and VM workloads but also the dataand applicationservices we care about You could run this as an example with a regional hoster fully air gapped and have local storesconnect directly to that One open modern platform fromwherever you want to operate RICHARD SEROTER  I m sold You ve done it So that s pretty cool So same Anthos controlplane tech  but alsolocal services in thisfully air gapped setting That s great stuff Thanks  Gabriele Appreciate you joining us here And  Vidya  did you like that VIDYA NAGARAJAN RAMAN I did  actually And  Gabriele  that was amazing And I m sure our audiencethinks so as well Over the last 15minutes  we ve shown youhow Google Cloud offersa world class experiencefor building new software ormodernizing what you have Our solutions forcontainer based appsare second to none And we re now makingit possible to extendthose terrific services towherever you need us to be RICHARD SEROTER  All right Thank you all for joiningus  Vidya  Gabriele as well This was terrific stuff And thank you out there forthe chat and the engagement It was awesome to see that So now  stay tuned for thelive Q A coming up next We re going to talkabout everythingfrom the spotlight all theway through to this demo Get your questions in there and we ll be answering it live Thanks so much Bye  bye VIDYA NAGARAJAN RAMAN  Bye  MUSIC PLAYING AJA HAMMERLY  Hi  everyone Thanks for joining ustoday for the developerlive Q A  I m Aja And a reminder thatthis Q A is live If you want to jointhe conversation reach out on Twitter withthe hashtag googlecloudnext We ve reached out asking foryour questions on social media and y all have been responding We ll be answering yourquestions during the next 20minutes And first of all let s welcome Priyanka who s here to answer yourquestions with me today Hi  Priyanka PRIYANKA VERGADIA  Thanks  Aja That keynote wasabsolutely amazing Yeah  so let s getinto the goodnessthat Urs and Aparna shared A highlight for me was reallythat upcoming documentaryon Kubernetes So cool I also enjoyed the GoogleCloud innovator communityannouncement  securingsoftware supply chain and building sustainably What caught your eye AJA HAMMERLY  Oh  a lot ofthe stuff you just mentioned Definitely our innovationsand sustainability The ability to see yourcarbon footprint of your cloudworkloads isreally  really cool When I heard about itin yesterday s keynote I immediately went and lookedat all my personal projectsto see the carbon costof what I d been doing And I love havingthat kind of datato use in decision making like  help me make theright decisions But I m mostly really excitedabout the Cloud innovatorsprogram I ve really missed interactingwith our Google Cloud devcommunity I want to hear and seeall the amazing thingsthe community has been up to And the innovators programshould let us do that And we re working on some superspecial innovators only eventsin 2022 that I can t waitto tell the community about PRIYANKA VERGADIA  Wow Yeah  that all soundsreally amazing I m also excited aboutthat secure supply chainannouncement  SLSA   and how the combinationof Cloud Build and BinaryAuthorization actuallyhelps kickstart your journeyto secure your softwareartifacts by fullyautomating that build process AJA HAMMERLY  Yeah  that sjust so interesting That s so cool So let s get toour first question We ve got questions coming injust for the folks at home The backstage crew is lookingfor your tweets as well so keep them comingin during the thing We ll be bringing them in live So our first questionis from Asher and it is who wouldbe good candidatesfor the participation inthe private preview of EarthEngine PRIYANKA VERGADIA Yeah  so anybodywho is like doing workon sustainable things So with thesustainability lens  if you re thinking aboutfinancial services the customer packagedgoods  and their impacton the environment and how all of thatcan be combined together So yeah  anything that you redoing with the sustainabilitylens Obviously  it s in privatepreview  so you have to qualifyand stuff But if you have thatangle of sustainability you re probably aright fit for it AJA HAMMERLY  Awesome Thank you for that Ah  second question camein from Priyanka s Twitter I saw this one this morning asI was coming in to do this Q A and this one isany examples   dowe have any examples ofend to end Spark basedpipelines for ML They d love to see them Specifically  they reinterested in learninghow serverless Spark can beleveraged in big data and MLops framework on Google Cloud PRIYANKA VERGADIA  Yeah  sothank you for this question I think it camefrom  INAUDIBLE   I saw it on myTwitter this morning So really what it s allabout is the serverlessSpark ML frameworkis about not havingto create your backendinfrastructure to run Spark So that s all takencare of for you So you can justrun your workloads which is  in thiscase  like you retrying to run a machinelearning Spark job  you can just getstarted by not worryingabout the infrastructure So that s one part The other piece of itis the Vertex Workbench which is the Jupyternotebook  but hosted But it also givesyou the opportunityto connect withDataproc  or the Spark MLjobs that you might havebuilt  and run all of that So as a data scientist  Ican just use my workbench and that becomes my hometo kind of get the data massage it  connect withDataproc  Spark ML jobs And then get allthose predictionsright in that one spot AJA HAMMERLY  Awesome Thanks  Priyanka That was a reallycomprehensive answer Ah  so now we have aquestion from Christopher and it s what are the benefitsof the innovators program You mind if I take this one PRIYANKA VERGADIA  Yeah  please AJA HAMMERLY  So the big benefitof the innovators programis going to be access toinnovators only events Like we re going to bedoing some AMAs we replanning right now  roadmapmeetings potentially And there s also aninnovators virtual backgroundthat you can download and usein your Google Meet meetings Ooh  another questioncame in from Twitter Awesome So let me read this one My POV  massive focus onthe security   massive focuson security focus inthe Developer Cloud Isn t this a topicthat matters morefor the managers of developersthan for developers And I ll take this one too if that s cool with you PRIYANKA VERGADIA  Yeah AJA HAMMERLY  So I ma manager  and yeah that s a really good question Security is vital to allaspects of software development and security needsto be everyone s job Yeah  managers need tocare a lot about it but we need to make thetools so that developerscan do the rightthing automatically and that everyoneis participatingin making our cloud more secure And we talked aboutsome of those things We talked about our toolingthat can help you make surethat you put your secretsin secure locationsas opposed putting them in code We talked about the SLSA We talked aboutlots of other parts And all of this is part of asecure software supply chain but security does needto be everyone s job Managers can help byteaching their developersand enforcing it but everyone needsto take the steps tomake things more secure So yes  good point  buteveryone s involved This is a great one fromPriyanka s LinkedIn Is it possible to connect tothe public IP of a Cloud SQLinstance from a privateCloud Run service Ooh  I ve been asked this onebefore when visiting customers Also is thiscross project set up Cloud Run service andCloud SQL instanceare on separate GCPprojects  so can youdo a cross projectsetup with your databaseand you re running aserver somewhere else Priyanka PRIYANKA VERGADIA You actually can So you have   soin this scenario you have a Cloud SQLinstance in one project and you have yourcompute or whateveris calling that Cloud SQLinstance in another project And you could totallymake them workby using what is calledas private service access What it does is it connectsthe two together evenwith a private IP So you don t even have to exposea public IP for your Cloud SQLinstance  which again comesback to the security pointyou were making earlier  everybody has tothink about security So you re not exposingwith public IP You re just using the privateIP of your Cloud SQL instance and connecting it tothe virtual machineor wherever you rerunning your computeto call your servers from So it s possible Private serviceaccess is the serviceyou re looking for to kindof connect the two together It s a VPC offering AJA HAMMERLY  Awesome Thank you so much  Priyanka Cool A question from Andy This is one of my favorites So there s a lot of ways torun an application with GoogleCloud It s a huge platform How do I know whatI should choose Where do I run my stuff Should I run it on GKE Compute Engine  Cloud Run So many good choices PRIYANKA VERGADIA Yeah  and thisis a question we get a lot And it really justdepends on your situation And the situations can bea lot   the type of team the size of team  and thenumber of developers you have and the languages thatyou might be using So there are lots ofdifferent scenariosin which you can decide So I ll give anexample of a few So for example  Compute Engine If you re migrating and youjust want to get from on premiseinfrastructure into cloud and you just want the speed  you just want to get there  I would choose Compute Engineto just migrate as is  and thenmodernize later if needed Sometimes  you don tneed to modernizeif you have licensingrequirementsand stuff like that So that s Compute Engine If you want to work withcontainers   need a little bitmore abstraction  less abstraction you can work withcontainers with GKE And that gives youa lot more controlover the numberof nodes you have and the processingthat you re using But if you might be justwanting to run containers but don t want to manage theunderlying infrastructure like nodes and stuff likethat  and the regions just use Cloud Run because it s serverless but it allows you to useyour container imagesand just deploy them Cloud Functions is kindof like everywhere So you re trying to do  handle one function  or a pieceor a feature ofcode that you justdeploy in thatfunction as a Functionas a Code sort of service But it kind ofapplies everywhere So I wouldn t say  so Cloud Functionis not like an OR  it s more of an AND  like it works with any of those It s just more of anextension and enhancementof your serviceswith serverless So I hope that helpedclarify a little bit of that but there s a lot thatgoes in that decision AJA HAMMERLY  Yeah  I know And I really likedhow you called outthe Cloud Functions isnot an OR  it s an AND It s   Cloud Functions isjust fantastic at tying piecestogether It s one of the thingsI love about it And I m just going topoint out for folksthat we have sessions on allof these in the breakouts So if you want to golearn more about these go look in ourbreakouts  and youcan find out more aboutthe different offeringsthat Google Cloud has So now we have aquestion from Esther What languages do Cloud Runand Cloud Functions support Oh  let s see if I cando this from memory I ve been practicing So GCF   these arethe standard languagesyou see on Google Cloud We ve got Node js  Python Go  Java   NET  Ruby  and PHP I got them all Awesome Cloud Run supports anylanguage  or any library or any binaries that youcan put in a container But if you want to use thesource code deploys featurethat Abby showed earlier  thatis supported on Node  Python Go  Java  and  NET And specific versions ofthose languages are supported so please do go to the website and make sure the versionthat you need is theone that we support PRIYANKA VERGADIA  Greatmemory  by the way AJA HAMMERLY  Oh yeah  I ve beengetting really good atnaming all those languages So from Selena   ooh yes  great question When should I use GKEAutopilot versus Cloud Run PRIYANKA VERGADIA  I lllet you take that one AJA HAMMERLY  You re goingto let me take this one OK So this prettymuch boils down to do you want Kubernetes or not If you want Kubernetes  if youwant the enhanced flexibilitythat Kubernetes has  or youwant to have all those knobsand dials that you can turnto really fine tune everythingfor your networking needs oryour particular load profile use GKE Autopilot If you want to   ifyou have a container and you want to run it onGCP  and that s your goal Cloud Run is great Cloud Run is fantastic at that And as we pointedout  you don t evenneed a container if youuse Cloud Run sourcedeploys for thoselanguages I just mentioned More questions Ooh  this one sfor you  Priyanka This is from Caleb What file formats aresupported with the Doc AI stuffthat Anya talked about PRIYANKA VERGADIA  Oh  yeah So you can do images and PDFs So it s really aboutthe unstructured imagedata  so PDFs and images AJA HAMMERLY  Awesome Let s see what else we got Ooh  this one s from Mark Another for you  Priyanka Ooh  it s anothersecurity question So can you tell us moreabout Binary Authorization We covered it verybriefly in the keynote but it s something I vebeen hearing a lot about and I d love to know alittle more about it PRIYANKA VERGADIA  Yeah So again  it kind of boils downto the whole security narrativethat you mentioned  that everybodyis kind of responsiblefor the securityof the entire platform So in this case  the binary authis really deploy time security So you re deploying  andmaking sure that your images  or container images if you reusing GKE or Cloud Run  works with binary auth So when you re at yourdeployment stage  you have  you can provide signatureauthorizations on your images So if   and the verificationand the authorities for those So if they are authorized only   binary authwill apply the authorizations And once the imageis authorized only then you can deploy it AJA HAMMERLY  Awesome So I just got the signalthat we re running outof time  so this isgoing to  unfortunately be the last question And this questioncomes from Wesley Can I use the buildintegrity featureswith my on prem software PRIYANKA VERGADIA  Hmm Yeah  so you kind of can So with Cloud Build  it sreally any container image whichis built on Cloud Build you can use both on  you can use it in bothon prem or on Google Cloud You just have to use the binaryauth attestation Cloud Build And it s on the GitHub page so you can check that out But if you re buildingit in Cloud Build you can deploy it on premor in Google Cloud AJA HAMMERLY  Thanks  Priyanka Well  that was a lot of fun It was great to hear all thequestions from the audience Y all had some great ones And I want to sayjust a huge thank youto Priyanka for joiningus  and answering so manyof those questions Be sure to join us backover on g co cloudnext as the open infrastructurespotlight will be kicking offshortly  and they havesome amazing things thatthey re going to show off Thanks for joining us  everyone AMEER ABBAS  Hello  and welcome My name is Ameer Abbas I m a product manager at Google I focus on application andplatform modernization Last year I delivereda presentationwith an eerily similartopic to this one It was called  BuildingGlobally ScalableApplications with Istioand Anthos Service Mesh  So you re probablywondering why am I doingthe same presentation again Well  not quite I thought this year I wouldtake some of the foundationsand principles wetalked about last yearand put them into practice and I can show yousome of the practicalapplications of deliveringmodern applications So let s get started Let s talk about a service Everything revolvesaround a service We build our teams  ourorganizations  our engineering our product teams to deliverbeautiful  scalable  resilientservices to our customers Every serviceneeds a few things For instance  a serviceneeds to be resilient It needs to be scalable It needs to be secure And most importantly  itneeds to be manageable And what we mean bythat is by resiliencyI mean that we needto be able to weatherthe storms against anyinfrastructure  human or machine error Scalability is prettyself explanatory We need to be able to run ourservices on many computers maybe in manyKubernetes clustersin a lot of differentzones in many regions For security  we want to encryptall traffic  all communicationbetween services And we want to be able tohave the tools to implementpolicy and enforce policy toand from every single service And for manageability there s a lotof things that fallunder that umbrella For instance  what dowe want to observe How do we want tomeasure the signals How do we deliver traffic tothe right instance of a service And how do we reliably rollout new versions of a serviceand keep innovating andmanage the configuration thatrevolves around a service All of that fallsunder manageability So let s recap Last year we talkedabout running servicesin a single cluster Here we have a servicecalled front endrunning in the front endnamespace with a pod And it s running ina single cluster And everything is fine and dandyuntil that cluster goes down And as soon as thatcluster goes down that service andany other servicethat s running in that clusteralso goes down with it So we ve kind ofbroken that first tenetof resiliency  whichis that we can t evenweather the storms againstinfrastructure failures So what do we do Well  we run our servicesin more than one cluster So we run something calleddistributed services A distributed service isjust a Kubernetes servicethat runs on multiple clusters From an outsider s perspective it s the same logical service I don t need to know that it srunning on multiple clusters So here I have thesame service frontend running on four clusters On the left hand side I have two clustersin region 1 in twodifferent zones And on the right handside  I havetwo clusters in region 2in two different zones So you can see that not onlydo I have cluster resiliency I have zonal redundancyand regional redundancy And what that means isif my cluster goes down I still haven t lost my service I have three other clustersthat can serve trafficfor that service I can lose an entire region andI have not lost that service I can lose multiple clustersin multiple regions As long as that lastcluster has enough capacityto serve traffic forthat particular service I m good to go So you can alreadysee the effectsof scalability andresiliency whenyou run distributed services But with distributed servicescomes a few considerationsthat you may not considerin a single cluster life For example  servicediscovery and traffic routing In a single cluster  servicediscovery and traffic routingare kind of automatic When you drop aservice into a cluster it gets automaticallydiscovered Other services can get to it Routing just works So how do we do that ina distributed serviceenvironment Here we have twodistributed services What if the frontend service wantsto talk to the cart service How does the frontend service knowwhich pods are healthyfor the cart service Which pods to get to andhow to route to those pods So this is kind of theeast west traffic problem Next is ingress Again in a singlecluster environment we have an ingress object We deploy the ingressobject  and wecan get traffic from outsideof the cluster to our serviceno problem Well  how do we do that ina multicluster environment How do we do amulticluster ingress So this is themulticluster ingress or the north south problem And then when we bringlocation into the mix things get a littlebit more complicated Here we have service runningon two different regions Let s say region 1 is inEurope and region 2 is in Asia So the users that are inEurope or closer to region 1 we want to deliver theirtraffic to an instanceof a service that s closerto them  so in Europe And likewise  for theusers that are in Asia we want to deliver their trafficto an instance of a servicethat s running in Asia Only when somethinggoes wrong  for example let s say everything inthe Europe region is down now we want to send thetraffic from European usersover to the Asia user So how do we manage all of that And then  lastly  telemetry Again  in a singlecluster environment we have implementationsand toolswhere we can measurelogging  and tracing and metrics forevery single service How do we do that ina distributed service Which signals to measure How do we know if aservice is healthy What to alert on So all of those thingswe re going to talk about and we re going to useAnthos Service Mesh to solvesome of these things So let s go to the demo Let s get orientedto the environment Here I have a Google Cloudproject with four GKE clustersrunning GKE 1 and 2 are runningin region US West 2 GKE 3 and 4 are runningin region US Central 1 So it s the same four clustermodel that we just saw Let s look at theWorkloads page I ve deployed a fewsample applications And one of them is thefamous online boutiqueretail application thatconsists of 13 services I ve deployed each servicein their own namespace and you can see by the workloadsthat each service is deployedas a distributed service So ad is running onall four clusters Cart is running onall four clusters Shopfrontend  which is howyou access the application is also running onall four clusters Let s take a closer look atone of these deployments So we re going to look atshopfrontend running in GKE 1 And I m going tolook at just the podsfor that particular one You can see that this podis running two containers One is theshopfrontend container That s your actualapplication container And the other one isthe asm sidecar proxycalled the istio proxy Now  this proxy governsthe flow of trafficto and from thisparticular container And if we look at theconfiguration of this proxy we can see what versionof asm we re running So in this case we re running 1 10 4 which also correlatesto the Istio version We can also see that we reusing the Google managed meshcaas our certificate authority We can also see that ourtrust domain is the workloadidentity for the project And I m using the same trustdomain for all four clusters And that s how servicescan talk and trusteach other across clusters What we want to do is we want tolook at from this istio proxy sperspective  how doesit view other services Let s quickly jumpinto the terminal And I m going to go aheadinto my GKE 1 cluster Let s look at the pods So kubectl get pods This is the same podthat we were justlooking at in the console We re going to use theistioctl CLI command And I just want to lookat the proxy config And I just want to look atthe endpoints configurationin the proxy config forthis particular pod And instead of dumpingout the entire config I m just going to grep it forone of the services  whichis the cart service And here you can seethat from the perspectiveof this shopfrontendpod  it seesthat there are fourendpoints out therethat it can access when itaccesses the cart service These IP addressespoint to the podIP addresses ofthe four endpoints Let s look at the pods So the first thingI m going to dois look at the podsin the GKE 1 clusterthat s running inshop cart namespace So shop cart namespace is wherethe cart service is running And you can see thatin GKE 1 clusterI only have one pod running withthe IP address of 10 0 2 11 which is the same IP addressthat you see up here which means that the otherthree IP addresses correspondto pods running in theother three clusters And we can quicklyverify that by justpicking one other cluster for example  GKE 4 And again  in GKE 4 we re also just runningone cart pod with an IPaddress of 10 8 2 143 which is the IP address here So the asm control plane goesout and discovers the endpointsfor every singleservice and configuresevery single istio proxywith those endpoints And that s how every singleservice and every single podknows how to get toevery other service So that s how the servicediscovery piece is working But we also want to makesure that the traffic routingeast west is working So for example  thefront end servicecan talk to the cart service So in order to demothat  it s best donevisually through a dashboard So here I have created a customdashboard for the cart service And this dashboard hereon the left hand side I m looking atthe request rates And I ve groupedthem by location So I want to see whois accessing the cartservice from which location And from the legendhere  you cansee that I m receivingrequests to all four pods Further  I can group this by let s say  a source service source canonical service And here you can see thatthere are only two servicesaccessing shop cart One is shopfrontend The other one is checkout And both services areaccessing all four locations And you can see that I havea load generator that srunning that s showing me thatthe traffic routing is working So in GKE  every pod cantalk to every other podif you have deployedGKE in VPC native modeas long as you have theappropriate firewall rules So now east westtraffic is working Next  let s take a lookat north south trafficand how ingress works So for that  let s goback to our Clusters page And you ll notice that I havethis fifth cluster calledthe ingress config cluster This is the clusteron how I configuredmulticluster ingress Let s take a closerlook at this cluster So I m going to goback into my terminal I m going to change my contextto the ingress config clusterand change my namespaceto istio system Inside of theistio system namespace I have two resources I have a multiclusteringress resource and I also have amulticluster service resource And I have multipleapplications And that s why yousee a couple of these So the multicluster ingressresource correspondsto an ingress resource and amulticluster service resourcecorresponds to aservice resource just like you wouldnormally configurean ingress and a service So let s take a closerlook at each of these So I m going to look at themulticluster ingress resourcefirst for the online boutique And you can see thata multicluster ingressresource looks a lotlike an ingress resource So I have a rule with a host So here s the hostof how I m goingto access this application And its points backto an app  a service But in this case instead of pointing backto a Kubernetes service  it spointing back to a multiclusterservice on port 80 And if we look at themulticluster serviceresource thatcorresponds to this again  it looks a lot like aKubernetes service resource So if we look atespecially the specof that particularresource  you cansee that it has aservice name  a port and then most importantly it has this selector And what it s selecting isthe istio ingressgatewayas the backend service The only difference hereis these cluster selectors And in thisparticular case  I veselected all four clusters So essentially what I ve createdis the multicluster ingress You can think of it as thefront end that points backto four differentbackend services thatmatches this selector  whichis the istio ingressgatewayselector And what that creates  ifwe go back to the console and I m going to goto Network Services Load Balancing is a frontend with four back ends So here if we justlook at these top two as a matter of fact  ifwe look at the second loadbalancer  which is the onlineboutique  istio mci  that gotcreated when we createdthat MCI resource And it gave me a front end We ll talk about thecertificate here shortly But more importantly  wehave created four back ends And these four backends are pointingto the istio ingressgatewayservice that srunning on all four clusters And we can verify thatby looking at the zone And you can see that allfour zones are covered And if we click on anyof these endpoints you can see the actualpod IP addresses and then they are healthy If we go back tothis  the certificate I ve created a Google managedcertificate for a DNS name So remember  in themulticluster ingress we had a host field thatpointed to this DNS name And so in order toaccess this service I can just simply copy this open a new tab  paste it and you can see that we canaccess our online boutiqueapplication And we can navigatearound this application So here I m looking atproduct description adding it to the cart Here I m looking atthe cart service I can place the order  whichinvokes the checkout service And you can seeall the functionsare working for thatparticular application The way the actualtraffic flow is workingis when I access thisURL  if I scroll back I m going to thisfrontend IP address From this frontend  trafficis being routed to one or moreof these istio ingressgatewaybackends And then theistio ingressgateway backendroutes the trafficto the shopfrontend So the next thing is theimplication of locations So remember I haveistio ingressgatewayrunning in two different regionsin four different clusters I also have theshopfrontend runningin both of these regionsin all four clusters So what I want is if I mcloser to US West 2 region I want to go to the US West2 istio ingressgateway and then eventually also tothe US West 2 shopfrontends So let s take a look athow that is configured I m going to go back intoKubernetes workloads And I have a loadgenerator runningfor the online boutique And you notice that myload generator is onlyrunning in the West 2 region And this is so I can showyou locality load balancing I also have locality loadbalancing configured  but onlyfor the shopfrontendapplication and the reason is justfor this demo purposes So let s go to the console Let s go to one of theclusters  GKE 1  for example And I m going to show youthe destination rule that srunning for the shopfrontend You can configure localityload balancing mesh wide but I prefer to configureit on a per service level So here I ve configured itjust for the shopfrontend And the destinationrule determineshow the traffic is routed oncethe routing decision is made And this is where you configurelocality load balancing So here in the loadbalancer sectionyou can see that I havelocality load balancing enabled I also have a failover settingwhich says go from US West2 region to US Central 1 since I m looking at the GKE 1configuration And then I also havethis outlier detection which determines when afailover or a failure occurs This is a very aggressive rulejust for the sake of the demo Essentially  I msaying in every second even if I have a single error consider that an outage and eject that particulardeployment for a minute And so you can see thatjust with the additionof these 5 10 lines  I vecreated locality loadbalancing And I have the sameexact destinationrules for all four clustersfor the frontend service So in order to prove thatthat is actually working remember that my load generatoris only running in US West 2 So what I should expectis only the West frontendto be receiving traffic  becausenobody else is really accessingthis particular application So the best way to do thatis let s look at the charts So here I have acustom dashboardthat I ve created forshopfrontend very similarto the one that we werelooking at for cart earlier Again  we re lookingat the request rates And I ve groupedthose by locations So I have the four locations So US West 2 a  b and Central 1 a  b But you can see that I monly receiving trafficto the shopfrontend onlyin the West regions and not in the Central regions even though I have podsrunning in the Central region That means locality loadbalancing is working And you can configure thesame locality load balancingfor every single service So in a sunny dayscenario  that works great Everything will go toits closest endpoint But what happens whenthere is a failure So my load generator isrunning in the West region What happens if theWest region is down I should expectthat load generatorto start sending trafficto the Central region because I have configuredthe failover rule as such So let s do that In order to simulatethat  all I m going to dois scale the replicas for theshopfrontend in the West regiondown to 0 So let s go to the console And I m going to runthe scale command And I m just going tofix this spelling error So all I m doing is scalingthe shopfrontend deploymentreplicas down to 0 in GKE1  as well as in GKE 2 These are my twoUS West locations What that s going to do isrender a failover from US West2 region to US Central 1 So we ll wait a few minutesand see how the charts do A few minutes havepassed  and now youcan see that allof the traffic isrouted to the central servers So I have not changed theload generator s location It s still running in US West But because of thefailover from US West we re now going to Central 1 The chart to the right isalso showing us error ratesby location You can see thatfor the same timethat the failover occurred  wedid not experience any errors So it was a seamless failover And we expect thisto kind of go backwhen we change thereplicas back to one So let s quickly do that So we go back to the console All I m going to dois rerun that command But this time change the replicato 1 for GKE 2 and for GKE 1 we ll wait a few minutes And now we see that the traffichas gone back to the US West 2clusters  and no more trafficgoing to the central clusters So we re back to our sunnyday state of locality loadbalancing And again  we did notexperience any errorsthroughout this changeover So this technique isalso useful if youwant to perform some scheduledmaintenance on a cluster You can now manuallyand explicitlydrain clusters by scalingjust the replicas downfor any cluster down tozero  and shifting trafficover to different clusters aslong as there s capacity there And then you canperform an upgradeor any kind of maintenanceon those clusters The last thing wewant to look atis observability andmanageability of services So for that  let s goback to the Cloud Console And here we re going togo to Anthos Service Mesh When you have AnthosService Mesh deployedon one or more clusters  youwill get this dashboard view And this dashboard viewgives you a table viewand a topology view for allthe Anthos enabled services So here I haveall of my servicesthat are running inmultiple clusters for example  shopfrontendis running on four clusters I can look at which fourclusters it s running on This comes completelyout of the boxwithout you having to instrumentanything in the application If I click on shopfrontend I can get further informationabout this service So I can see inboundand outbound servicesthat it s talking to I can get the golden signals I can look at the metricsfor this particular service These are canneddashboards  again that come with everysingle service So you get thesedashboards out of the box So you get requests persecond  error rates  latencyfor the threedifferent percentiles the 50th  95th  and 99th  aswell as request size  responsesize  CPU  memory  and diskfor every single servicewithout having toinstrument anything From here  you can alsogo to Application Logs So I can just clickon Application Logs and it will create theright filters for me So I can just look atthe application logsfor the frontendso I can see that Ihave requests going in and out I can also look attraces from here So I can just click on traces So if I want to lookat request trace this is something you do have toconfigure in your application I can look at the histogramof my requests coming in And then I can click onan individual request And I can obviously look at thatparticular trace information Again  all of this is partof the info service mesh Furthermore  I can go tohealth  and I can create SLOs So here I ve createda latency based SLOfor the frontend service And if I open this up and if I edit this SLO I can just kind of showyou how I created it So the first thing is Ipicked latency as my metric I can either chooseavailability or latency I can hit Continue Here s the actual signal orthe metric that I m using which is the response latencies I can see the historicalresponse latency And based on that  Ijust picked a value So here I could see that thereare some spikes above 1 500milliseconds So I picked one for1 500 milliseconds You give it thecompliance period So I m picking calendar day And I m keeping my goal at 95  So 95  of the timeI want my latencyto be 1 500milliseconds or belowfor that particular service If I hit Continue  it alsogives me the JSON viewof that particular SLO So I can use the SLO APIto create this if I d like And then I just create my SLO So for every singleservice you can do that And I recommend you do thatfor every single service And you can do that as partof your CI CD pipeline This is where you ll get yourSLIs and your error budget as well as if you veconfigured any alerts You can create alerts byjust clicking on this And you can measure howexactly your service is beingexperienced across the board So we typicallyrecommend at leastcreating two SLOs forevery single service one for availabilityand one for latency So you can see whetheryour users are actuallyable to access yourservice  and thenwhat is theirexperience when theyaccess that particular service So that s the end of the demo Let s go back tothe presentation A few calls to action First  hopefullyit s very obvious run distributed services Never bet your serviceson your infrastructure By running your service on morethan one Kubernetes cluster you ve immediatelygained the advantageof scale and resiliency Second  monitor yourdistributed services Golden signals is agreat place to start Somebody gave me this analogythat when you go to the doctor they don t try to diagnoseyou for every single disease They look at your symptoms and then they figure outexactly what s wrong with you So alert on symptoms  not onevery single potential cause In the distributed world  thereare thousands and thousandsof signals out there And if you want to alerton every single timea CPU or memory spikes  you regoing to get overwhelmed Lastly  use locationas a benefit I see two mainbenefits for location especially if you re runningmultiregional services First is you can deliver yourservice closer to your user And secondly  thinkingabout failure domains So I think of failure domainsin terms of concentric circles For instance  a Kubernetescluster is a failure domain When a clustergoes down  it onlytakes down the servicesthat are runningon that cluster A zoneis a failure domain If all the clusters in a zoneor the zone itself goes down then that zonecannot serve traffic And a region isa failure domain Thinking in thoseterms helps youkind of do figure out howto do maintenance windowsand scheduled operationson your infrastructure If you want to builddistributed services you can go to this link bit ly run distributed services or you can pause the video andscan this QR code that takesyou to the same link I thank you for your time And enjoy the restof the conference VICTOR SZALVAY Hello  and welcometo our talk onGoogle Cloud GoldenPath to Application Delivery My name is Victor Szalvay I m a product manager  andI work in the DevOps spaceat Google Cloud I m joined by mycolleague  Chris Willis CHRIS WILLIS  Hi  myname is Chris Willis and I m a customerengineer at Google Cloud I m really focused onhelping our customers succeedwith Kubernetes  and helpingpeople build and run cloudnative applications at scale VICTOR SZALVAY So Chris and I aregoing to talk to you alltoday about applicationdevelopment with thespecific focus on Kubernetes So we re going to look atdeveloper tools and CI CD which are often a painpoint for organizations but really are also anessential part of the journey I think many of you knowthe benefits of CI CD But just to kind ofrecap quickly  itincreases developerproductivity And that s directly linkedto customer value generation And getting thatvalue out to customersfaster means you remore competitive You re scoring highercustomer satisfactionwith your customers And with betterautomation and pipelineparity comes higherquality and security But we understand that youare where you are  right And CI CD andDevOps improvementsneed to happen incrementallyand with your existing toolsand processes So they say that proofis in the pudding  right And our research highlights thatelite performers  the ones whoship code multipletimes per day are 1 8 times more likelyto achieve or exceedtheir commercialgoals  includingthings like profitability productivity  and customersatisfaction But our research also shows thatorganizations are increasinglysuccessful at adopting CI CD So more and more ofyou out there aregoing the path of CI CD  and areseeing those improved businessoutcomes as a result You may be here on yourCI CD journey  right Developers areiterating and committingcode  which then feedsinto a CI CD pipeline And ultimately  endproducts are delivered outto your customers But you re also facedwith some challenges And for some of you  thatincludes self managingyour CI CD infrastructure Also bespoke and complicatedconfiguration pipelines There s often weak separationof duties and concernswhen it comes to CI CD systems And often those CI CDsystems are overused They re doing build andtest  but they re alsodoing the kitchen sink And lastly  there arevariances between developerand deploymentenvironments  whichsurfaces unwelcomesurprises in production So this ultimatelyleads to FUD  right And from the operationsside of the house we re talking aboutfear of deployment Are developers reallyiterating in productionlike Kubernetes environments Are they evenadopting Kubernetes And when they are is the innovationthey re doing around renderingtools affecting thingslike pipeline reliability Chris CHRIS WILLIS  Yeah this is reallywhy I jumped at theopportunity to getinvolved with the golden path So some of the customersI ve worked with struggleto get their developers to workdirectly with Kubernetes  justbecause they have theirdevelopment workflow They have their local Dockercontainer deployments And they re used to testingagainst that  and just kindof working like that And it s really somethingthat they re notinterested in changing And it makes sense  becauseoperators were reallysold on Kubernetes whenit exploded in popularity But in some cases  developershaven t really followed suit They really weren t in the roomwhen that jump to Kuberneteswas made And they re kindof now just beingtold to deal withit  essentially VICTOR SZALVAY Yeah  and I thinkthere s also impact for thesecurity side of the house right A lot of uncertainty there Is there enoughautomation to go aroundfor things like securityanalysis and vettingbefore things go out the door Or are we relying on manualprocesses for that vetting And also  are we findingthese security issues too latein the development cycle causing a lot of churn So there s a lot of FUD And what we wanted todo here was presenta pathway that s beenbuilt out and derivedfrom a lot of the learningsthat we ve taken at Googleinternally So we identified thefollowing three concepts And as I mentioned  thesehave helped our developersstay productive internally whileenabling automation at scale So the first one is todecouple and insulatefrom configuration change So Kubernetesmanifest rendering it s often tightly coupledwith CI CD pipelines And that really creates sortof almost like a gridlocksituation  where bothsides  developers workingin their renderingworkflows and operationswith their CI CD pipelines they re hesitant to make moves They don t want to breakeach other  so to speak And so instead  we want toseparate out configuration And we want to separate outthe responsibility model Next is unificationof the dev experience And what I mean here is acrossthe developer inner loop the CI CD loop  butalso all the wayout to productionKubernetes clusters The biggest pain developersface in Kubernetes developmentis knowing thequirks and variancesacross the various loops So in the inner loop youmight be using somethinglike minikube In CI CD there may be someslightly different environment And ultimately  your production enterprise Kubernetesenvironments  those mightbe different altogether And I m talking aboutthings like the useof differentenvironmental variables perhaps different secrets replicas  and so on So what I really want is away to bridge these variancesand provide a moreseamless experienceto all my developers And the last pointhere is shifting left It wastes a lot of developerenergy and nerves  frankly when policy orsecurity constraintsare discovered downstream So what we want to do isactually inject those policychecks right into the IDE tocatch those issues before devsspend too much timedown a certain pathway So we re excited toshare with you todaythe notion of the goldenpath for applicationdelivery on Google Cloud Now the key to the golden pathis a product called Scaffold Scaffold is an open sourcecommand line utilitythat unifies thedeveloper experiencewith build  deploy test  security and provides a uniformexperience across allof those loops You can use Scaffoldwith your existing tools But it s also  of course bundled with the Google Cloudtools you see here And while you may be familiarwith Cloud Code and CloudBuild  we re reallyexcited to introduceCloud Deploy  our fullymanaged  opinionated  continuousdelivery service purposebuilt with Google KubernetesEngine in mind Chris  you have anythoughts to add on this CHRIS WILLIS Yeah  just I wantedto highlight a few things here So for those of you that arenot familiar with Cloud Code it s an extension of VSCode in    Inteligase    or it is an extension thatis produced for these IDEs So there s no realswitching between IDEs You don t have to changeyour IDE just because you reusing Cloud Code It s baked right into your IDE The Cloud Code runon Kubernetes featurereally clearly delineates wherethe developer s responsibilityends and ops begins This means developers can reallyfocus on writing code  and nottrading YAML files  andtroubleshooting surpriseswhen deploying to staging So this is really thatshift left mentalitythat Victor was talking about And then integrationwith Scaffold it being an opensource project meansops can really start developingan opinionated approachto CI CD I know with theintroduction of Kubernetes which is really a platformto build platforms  it s not a platform itself  there s a lack of opinionon Build and Deploy So Scaffold can really helpyou build those opinions and carry them toother cloud vendors So you re not lockedinto Google Cloud I mean  obviously GoogleCloud integrates heavilywith Scaffold And we re going to make thata first class experience But you can take theScaffold container imageand run that same Scaffoldconfiguration elsewhere So you re not reallylocked into our services VICTOR SZALVAY  Yeah  andChris  at the beginningwe talked a lot aboutincremental adoption and sortof the reality of makingchanges to your CI CD pipelines We want to encourage allof you to really startwith your most urgent need And for many of ourcustomers  they vegot a development loop going They ve got CI going But CD is the next frontier And so perhaps youcan get the benefitsof Scaffold and the goldenpath by adopting somethinglike Cloud Deploy as afantastic starting point Or perhaps you canstart with Cloud Code As Chris mentioned this is an IDE plug inwhich bundles Scaffold And it provides agraphical user experienceon top of the Scaffold CLI Chris  anything else youwant to add about Cloud Code CHRIS WILLIS  Yes  I mean  thecoolest thing about this oneis that it s free So developers and operatorscan  right after this session go grab Cloud Code And there s no dependencyon Google Cloud We have sample apps that lljust work out of the box You ll click the Runon Kubernetes feature and bam  it sdeployed to minikube So it s a really cool  powerfultool that hopefully y all willcheck out after this VICTOR SZALVAY  Yeah and the last one here is perhaps Cloud Build If you re lookingfor a fully managedbuild and continuousintegration platform Cloud Build offersa Scaffold builder which easily integratesScaffold into your continuousintegration workflow So Chris  I think the best wayto experience the golden pathis to see it in action And what do you think Should we get into the demo CHRIS WILLIS  Yeah  let s do it VICTOR SZALVAY  Allright  here we go All right  so I just have abasic Go application here And we re going touse Scaffold in itto generate our deploymentmanifests  our Kubernetesdeployment manifests and our Scaffold YAML And these are both going tobe applied against minikube We ll add a testhere real quick But I want to breakdown this Scaffold YAML So we have a buildstep which is actuallygoing to build our image And we re usingBuildpacks because we regoing from source You ll notice there sno Docker file here And a test step which is just goingto run a simple Go testinside the container image And a deployment manifest where our Kubernetes deploymentis defined And this is all on minikube So it s just going to pullwhatever Docker image resolvesthere for Go mod image All right  so nowwe re going to usethat Run on Kubernetesbutton that wehad talked about previously And this is going to executethe Scaffold YAML steps and use the deploymentmanifest that we generatedto deploy to minikube And we ll go and checkout our application Just a very simpleGo application Refresh it in thecounter increments So now we want to make achange to our applicationto have that quickfeedback as a developer So we want to make a localchange and see if it works So we re just going to changethis color from blue to red I know  nothing huge And we see file syncedas soon as we saved it Scaffold picks up onthat change locally And here we go We have a red insteadof a blue application Very important All right  let s stop it here And obviously debugging is important So this is my favorite feature So let s go down andinsert a breakpoint Looks like we had one left over But insert abreakpoint at line 81 right where thecounter increments And click Debug on Kubernetesthis time instead of Runon Kubernetes So this will actuallyinject a debuggerinto the container andforward those events This is all part of Scaffold Forward those events to the IDE And so when werefresh this page it s going to drop us rightinto the debug output And we can search throughlocal variables here And we can go into thecall stack and lookat all sorts of things troubleshoot certain thingsabout our application thatmight have gone wrong We can resume We can step over  allthe normal debug stuff So very importantfor developers to beable to debugtheir applications So right now we havethis dev inner loop The dev is workingthrough Cloud Code deploying locallyusing minikube They have theirapplication deployed and they re working away at it So now the ops folks aregoing to go and createa Cloud Build trigger Which any time there isa PR pulled into master or some change tomaster  this isgoing to fire off andexecute those scaffold stepsthat we specified And so now we have any timethe developer pushes to getor PR is approved  CloudBuild will trigger All right  so you ll seewe ve added some thingsto our Scaffold manifest We have a profilenamed staging  whichwill deploy the deploymentstaging manifest which is we just copiedthe deployment manifestthat we generated earlierwith Scaffold in it  and addedwhatever is specific to staging And we did the exactsame for Canary and Prod And this is veryimportant  because theseare going to stretch acrossinto what Cloud Deploy is latergoing to use to deploy to thevarious staging Canary and Prodenvironments They re kind oflike scopes  right You re trying to get acrosshow this tool should behave Or how this deploymentshould behavein a differentenvironment  whichis where we talked about kindof the applying of opinions and making an opinionatedCI CD process thatsticks with Scaffold and notthe Cloud providers  specifictools We have our pipeline YAML  wherewe re specifically saying whatthe stages of the pipeline are So we have stagingCanary and Prod And for those stages inthe pipeline   so each onein the Cloud Deployinterface  you llsee it ll create kind of a boxas it visualizes the pipeline For each one you resaying  I wantyou to use this Scaffoldprofile called staging So this is whereScaffold and Cloud Deploymeet is that you re specifyinga specific step in the pipelineand what Scaffoldprofile should run Then we have the actualconfiguration for the stagingstep in the pipeline So remember  in thelast file we specifiedthat there was a step inthe pipeline called staging This is where we re goingto say  OK  staging You re going to use this GoogleCloud project  this clustername  called staging and then obviouslythe region that it s in Now we re going to createthe pipeline in Cloud Deploy We are going toconfigure the stagingstep in Cloud Deploy usingthose files we just discussed And then we re going totrigger the pipeline All right  so we go tothe Cloud Build interface and we will check thebuild triggering here Going to go and doall of those Scaffoldsteps that we ve defined And we ll go into Cloud Deploy Check out the pipeline This will trigger anddo the actual deploy as we ve specified And we ll go and checkon our application And here it is  running So the developer is nowworking in their inner loop They submit a PR An operator approves it Cloud Build triggersbecause master was changed The artifact is built andstored in artifact registry And then the associated stagingprocess in the Cloud Deploypipeline is triggered And a deployment tostaging is initiated And we re going to promote thatstaging release we deployed We re going to promoteit to a Canary deploy And that will deployto Prod  right So normal Canary deployment it ll deploy to Prod And then if there s anythingwrong  you just roll it back And hopefullynothing bad happened So let s make sure that thatis deployed to the Canary or the Canary isdeployed correctly And let s check onthe application Looks good We got our littlechickadee thereto signify that itis Canary deployment And now we will promote therelease from a Canary deployto Prod And there s going tobe an extra step here So see that it s pending So we know that we needan approval  someonewith the authority to approvethis release to production Not just anyone can pushto production  hopefully And so this person isgoing to come in and say OK  yeah  looks good We re going to approvethat rollout to production And so we ll go backand check on it And the deploymentto Prod is underway It finished Now  let s check on ourproduction application Let s make sureeverything s in order Looks good All right So our final state isthe developer  again working in Cloud Code their local loop which can be minikube Local can kind of be anywhere That s what I foundout with customers It can be I m on a laptop and I m on an airplane local Or it can be I m justin my normal GKE clusterthat someone made for me And that s where Ido my development So they re doingthat dev interloop getting that fast feedback They submit a PRto the Git repo And an operator approves the PR And there is a triggerfrom Cloud Buildbecause master has changed The artifact is generated stored in artifact registry And the associated stagingpipeline step is run We obviously had an extraCanary deployment step in here And once Canarytries to go to Prod an operator actually has tocome in and approve that And then you haveyour change to Prod So that is the fullend to end picture And that is the end of the demo All right  so let s summarizewhat we ve seen today The golden path for applicationdelivery on Google Cloudcenters aroundunifying pipelinesacross dev  CI  and CD We accomplishedthis with Scaffold Scaffold is open source It s a CLI  but it s wrappedin a UI provided by Cloud Code It helps developersget productivewith local Kubernetesdevelopment and debuggingright within the IDE But Scaffold alsocreates pipeline parityby commands that fit rightinto your CI and CD processes You can adopt thisapproach incrementallystarting with only thosecomponents that you need most So here are someresources that we vetalked about in today s talk If you d like toget started  we dinvite you to check these out Chris CHRIS WILLIS  Yeah  sojust one thing to note That GitHub link at the bottomis just on my own GitHub But that read me andthat starter applicationis exactly what I usedin the demo here today So if you want to tryit out on your own it ll all work justfollowing the readmeand generating all ofthe YAML files as you go So feel free to check that out VICTOR SZALVAY  Well that s it for a session I want to thank my colleagueChris for a fantastic demo And I want to thank you allfor spending some time with us and learning about the goldenpath for application deliveryon Google Cloud Thanks LAKSHMI SHARMA  Hello  everyone Welcome to thesession on monitorand troubleshoot yournetwork infrastructurewith data driven insights This is Lakshmi Sharma Director of Product Managementfor Networking with my colleagueIrene Abezgauz  ProductManager  working onone of these productsthat you will hear about today In this 20 minutetalk  we are goingto discuss thecommon user journeysduring migration troubleshooting and optimizationof your network We will also demonstrate howNetwork Intelligence Center the product that providesyou tools and capabilitiesto support these user journeys is helping you by data insightsand offering aplatform that wouldbe used in most effectiveways throughout your userjourneys of infrastructure specificallynetworking  troubleshooting migration  and optimization Network IntelligenceCenter is a wayof offering you AI ML poweredNetOps acceleration platform This is Cloud AI Ops in real working for your workloadsonto GCP Our goal is to become thesimplest Cloud to operate onthrough an IntelligentSelf Driving Network thatextends beyond GCP to hybrid to multi cloud  to branches to places and capabilities thatyou have built for operatinginto virtual and Cloudjourneys  and also integratingall the way to legacyinfrastructure and servicesthat are built on premor any locationhere henceforth Imentioned  leveraging allthe AI and the data insightsthat we continue to buildfor building GCP and makingGCP the platform drivenby intelligent insightand self driving Cloud So Network IntelligenceCenter is Google Cloud s wayof delivering onto that visionof Self Driving Network It contains multiplemodules to ensureyou can deploy whatyou have intended to Basically  it gives youtools for the deploymentto be matched to your intent It offers you troubleshootingin any kind of performanceand connectivity issuesas you re migratingor as you re operatingon the GCP  whicheverlocation  whichever place and whichever applicationyou have been runningcurrently or youhave been building towards It helps you optimize yournetwork  including firewallsand other network servicesand infrastructure servicesthat you will be using todeploy your workloads onto GCP Networking is the foundationfor all Cloud deploymentsand business processes We expect it to be upand performing well Our customers expect it to be They want networkingto be seamless regardless of the usecase and the journeythat you are working towards User journeysthroughout Cloud  basedon our experiences ofcustomers migrating and workingon GCP with us  specificallynetworking scenarios there are threemain categories  planning and migration  day 2ops at scale  and optimization So these are the three sets ofcategories and user journeysthat we will betaking it through and how different modules andNetwork Intelligence Centerhelp you build anddrive towards that Here s what you need ineach of those stages Early stages deployment is live So when you re migrating yournetwork or your workloads while your deployment is livewhat you want to make sureis that what you intended todo  what kind of architectureyou deploy for  is what you regetting when you have migratedor when you re migrating And then you also  once youhave measured configurationand your intended INAUDIBLE    then you reallywant to make surethat your traffic isflowing as you intended to So that s kind of number onein the early stages of our userjourney duringplanning and migration The next step would bethat yes  I have migrated Yes  I have it as intended  configuration and trafficas intended  as Iexpected it to be Now the second phase isthat  is it production ready Is it production live So that s the time whenyou want to make surethat your networkcontinues to stay healthy you are able to identifyand troubleshootissues in advance  not at the time the issuehappens  but prior towhen the issue has happened Or in cases issuehas happened  youbeing able to troubleshootit and identify the rootcause for each of thoseissues in operationor during productionoperations very fastand be able to remedyit at an instant We also want to be able tooptimize your deployment You want to be able to optimizeyour deployments that you rerunning by having the controlon performance and cost First you want to be makingsure that your applications arerunning with theperformance and reliability no matter what kind ofoptimization that you make  configurationoptimization   INAUDIBLE optimization  and servicesoptimization  maybearchitecting it again So we give you a tool sothat you can optimize it backand then you can go backand check your intent again your traffic patternsagain  and you re backto a very healthyoperation cycle So for all ofthese user journeysthat we talked through  you needa lot of data to be available You want your data to becontinuously monitored for you You want the control on wheremy data is  where my logs are where my alerts are  you want all of that informationacross all your deploymentsacross thesestages  and you wantthe data monitoring andtelemetry  everythingto be available to you so thatyou start to make decisionsin a self driving andself sustainable mannerwithout really anyinteractions   to be honest like  literally zerointeraction with peopleon the other side of the Cloud So for all of these data  howdo we bring this data to you Network Intelligence Centerprovides that visibilityinto working on this network how the deployment actuallyworks  how and where thetraffic flows  and are thereor if there are any problemsand issues with configurations or the traffic  or therouting   any kindsof  INAUDIBLE  or any anomalies or any suboptimalities that yousee So once we have all thisinformation for you the next step is for usto provide users the toolsto make that dataeasily availableand have the systemintelligently and thenautomatically addressany existing issues as well as service the user themost important things  the mostrelevant insights We do want to avoidthe noise fatigue And after all  the networkis a business enabler We want your focus to be onthe most important and relevantinformation It allows everythingelse to communicate Network allows you tocommunicate with your users Network allowsyou to communicatewith your business processesand your business people You want the network toauto resolve problems wherethey re relevant  and toservice users any issues thatrequires intervention That is why we are workingon enhancing  INAUDIBLE capability across differentservices and capabilitiesthat we have innetworking to deliverthe most importantinformation to our usersacross all thesejourneys that you putin a timely and concise manner So we want to begoing from a placewhere you replanning  migrating writing your architecture you re making surethat your intent isright  then you reautomating and optimizingtoward for your performanceand reliability and cost Once you havedelivered this cycle we want the  INAUDIBLE to be working with you sothat you are able to deliver And you want thesecapabilities to bedelivered in a self driving self contained manner to you So Day 2 Ops Day 2 Ops capability  whichyou see in the middle are what we are focusing in now And you will see somecapabilities and planningand migration statein Day 2 Ops caseso that you are able to leverageour products and capabilitiestoday while we bringyou onto the journeyof eventual optimization andself driving capabilities IRENE ABEZGAUZ Thank you  Lakshmi So basically  when welisten to the users they emphasize thefollowing thing We want the networkto be up and running It needs to be available  andit needs to be performing and we have a business to run We don t need to dealwith anything else And essentially  it needsto be up and running If it breaks  we want toknow as soon as possible Now  this needs to happenregardless of what shappening with our traffic It can be spiky In a nice way to say it  itcould be dynamically changing And in those conditions we need to beable to monitor the network And essentially that s what the usershave been saying thatNetwork IntelligenceCenter has been giving them And I think the best example or the best quote that Igot from this  from oneof our users  was  listen we re used to aggressivelymonitoring everything on prem Now  you take our mostprecious workloadsand you put it on thismagical mystery black box and we don t wantto lose visibility We cannot affordto lose visibility And that reallyresonated with me  maybewith the tiny exceptionof not wantingto bring SNMP to the Cloudunless I really have to But with the exceptionof that  I reallybelieve in that   as muchdata as possible  and as muchvisibility as possible Now  let s jump rightin and take a lookat the modules of theNetwork Intelligence Centerand how they re usedin the user journeysthat Lakshmi has described The first place whereusers typically startis Network Topology Basically  they come here whenyou ve just migrated something or you ve createdsomething from scratch Now  there is anew thing  and youwant to make sure that whateverit is that you ve built that it matches your intent Are those the serversthat I wanted to have Are they communicating inthe way that I expected How is traffic following How is it flowing Are the relevantregions being servedfrom relevant continents Do I have any inter regionalcommunicationsthat I m not supposed to have Am I bypassing load balancerswhen I shouldn t be All those thingsare basically thingsthat you see inNetwork Topology with the addition of beingable to see performancemetrics on the edges Now  from here  if you retrying to get more information there is actually a directpivot from Network Topologyinto VPC Flow Logs So if you are lookingat a specific serverat a specific edge  and youwant to get more Swiss knifekind of information you can justgo directly fromhere into VPC FlowLogs in the relevant contextand see what s happening Now  in NetworkTopology  you cansee compute instances load balancers  connectionsto on prem  Cloud APIs We are going to add GKEinfrastructure support And that s essentially Topology typically used in migrationor where something changes Now  the next thing isthe Performance Dashboard This is where you endup when things are notgoing quite as expected Usually  when you seethere is a problemor you suspect theproblem  you ll go here And here  you can actually seepacket loss and latency metricsfor traffic in your projects There is a combination here ofsynthetic traffic and samplingof actual user trafficof your machines You will see here only zones regions where you are deployed And you can essentially seeyour own specific performance Now  one thing that wehave recently launchedis the GlobalPerformance Dashboard In many cases  whenthere is a problemand you get to thePerformance Dashboard the question you regoing to ask yourselfis  OK  something isnot going as expected I m seeing high latency  orI m seeing high packet loss OK For that matter  is the problemanywhere in my deployment or is it a GoogleCloud wide kind of problem And having those two PerformanceDashboards side by sidebasically lets you compareyour project performanceto the performanceof all of Google And then you can know whetherthere is a network outage or maybe there is just a fewservers choking somewhere  100  CPU   and that is thereason why they re not properlyresponding to requests Any Google wideoutages are goingto be very clearly visibleon both dashboards And obviously  when you pivotfrom your project to this one you ll be able to see whereis the source of that problem Now  as I said  we are goingfrom relatively high level  OK  I just upgraded Please tell me in high levelhow everything looks because I m trying to understandthe connection between things And from there  youcan actually drill downto very specific metrics As I said  it really actuallydepends on the type of user Right Some users  they regoing to say listen  just throw allthe information you can I m going to derive allthe insights myself Just give me allthe data possible I m just going to plug itinto a third party system or I m going to process it And I want to see raw data And other typesof users are justgoing to say  pleasecrunch everything for me and please only call me ifit s super critical  superimportant  and you reabsolutely sure that thisrequires my attention Now  for the kindof users who wantto see everything and wantto get this visibility there in Cloud monitoring There are very specificproduct metrics You can see  for example  forevery VPN in every project You can see the stateof the VPN tunnel Are there any issues You can see whether thetunnel is up or not have there been anyHandshake problems what s the throughput For Interconnect aswell  you can seethe status of the attachment You can see the throughput You can see the throughputover a period of time Essentially  you can go fromvery kind of high level this is your Topology  tovery  very specific details And I think this is the goalof really providing somethingthat we can proudlycall data driven  right OK  here s all the data And you can crunch it yourself or we can crunch it for youand get to a point Now  another   thisis actually oneof the favoriteproducts by our users saving a huge amountof support calls And it goes like this Give me endpoint A Give me endpoint B And this product checks whetherthe configuration allowsconnectivity betweenthese two endpoints This means essentially  ifyou re looking at VM to VM nodes to master  VM to CloudSQL  on prem to Cloud SQL is your configuration actuallyallowing this connectivity And then if it s notallowing this connectivity basically  this moduleis going to tell youthere is a firewallrule blocking it or you have a routing issue Now  these lookat configuration And what we have justrecently launchedis dynamic connectivitytests  whichfor quite a fewof the scenarios they also look atwhat s actuallyhappening on the data plane OK  so your configurationallows connectivity Now let s take a deeper diveand look at the metrics What is the latencybetween these two What is the packet loss ratio if any  between these two You have the connectivitytest to basically tell youwhether any twoendpoints are able toor should be able tofor configuration and are actually able toconnect and talk to each other and how thatconnection looks like Now  Network IntelligenceCenter has additional modules like FirewallInsights  for example That s going to tell you whetheryou have unused firewall rules or any additional information So there is quite alot there that youcan go and actually check out And a quick recap In this not very longpresentation we ve talked about what we haveseen to be the typical userjourneys when usersare migrating when they re actuallyoperating at very large scales and when they re tryingto do optimization And now  since thisis a recorded session I want to share with yousome of the questionsthat we re usually beingasked  and the answer to these Probably the mostcommon questionI get is  what about GKE And the answer is that yes essentially  the PerformanceDashboards show theperformance for GKE cheekyworkloads as well We have GKE connectivityin Connectivity Test As I mentioned  inTopology  we areworking on showing theGKE infrastructure That is a big partof the workloads thatare running on GoogleCloud  and that s definitelysomething to be covered We re also looking atserverless   for example not just GKE Again  the same support You have Topology You have Connectivity Test So there s quite a lotthat you can learn This is definitelynot just for Compute Another super common question especially by the largerorganizations  is how does thisfit into my existing processes because I am hybrid  becauseI m the Cloud  because I alreadyset up a thing  andI want to continueusing that thing  whichmakes a lot of sense So basically  we re lookingat it from two aspects One aspect is taking all theinformation in the NetworkIntelligence Centerand making itavailable to be consumedprogrammatically So you can grab it via API You can plug it intoyour own systems So you can trigger any sortof response based on alerts for example  which you veset  and so on and so on The other one is that thecombination of NetworkIntelligence Centerand Cloud Monitoringbasically allows you tosee all your monitoringin a single place Last question  as I seewe are approaching time is can this be usedfor compliance And that s actually one ofthe major goals of the NetworkIntelligence Center You can see wheretraffic is flowing or for  INAUDIBLE purposes  you can seewhere traffic is not flowing You can get VPC FlowLogs on the trail You can get screenshotsthat you can thenuse in compliance reports So there is actually quite a lotin mind in terms of compliance and quite a lot thatthe Network IntelligenceCenter can deliver This team is always availablefor any additional questionsthat you may have There is also quitea lot happeningin the Next  21networking sessions and I do encourage you togo ahead and take a look Thank you ADAM MICHELSON  Hi  andwelcome to our Cloud Nextsession on load balancers And today  we regoing to be talkingabout the next generationof our Cloud load balancers We ve introduced manynew features recently And we re excited todayto share with you someof what those features entail But first  let s talk  let me present toyou our presenters So Adam Michelson  I m a productmanager at GoogleCloud  and I helprun the Cloud load balancers Babi  do you want tointroduce yourself BABI SEAL  Hi  everyone I m Babi I m also a product manageron the load balancing team Thank you  Adam ADAM MICHELSON  Awesome So with no further ado let s dive right in So today  we re going to talkto you about three items One is next generationload balancing We ve been working tore platform many of our loadbalancers to include advancedtraffic control capabilities Our load balancers havebeen re platformed to runon top of the Envoyopen source system And if you re familiarwith the Envoy system then many of these featuresmay be familiar to you And if you re not that s totally fine You don t need toknow that platformto use our Cloud load balances We ll also be talking abouthow using this common platformwill enable you to have commoncapabilities across our loadbalancers as well as commoncontrol planes across our loadbalancers Next  we ll also be talkingabout load balancingeverywhere This is a feature where  ifyou have services that run offof Google Cloud  such ason prem or in a multi cloudenvironment  how you canconnect to those servicesthrough our load balancers  so if you run services thatare not running onGoogle  you can stillaccess those servicesvia our load balancers And finally  we re goingto talk to you about someof our enterprise readyfeatures such as Private Service Connect which enables you to connectto third party   or servicesthat are not necessarilyunder yourorganization s control as well as how you can connectto third party applications So first  we re goingto talk about someof our next generation loadbalancing capabilities So for the folks who may notbe familiar with our loadbalancers  this is a quickoverview of our load balancers We have a few load balancersin our load balancer fleet We have both a set of Level 7HTPS load balancers as well asLevel 4 network load balancers Most of the next generationcapabilitiesyou re going to see in ourLevel 7 load balancers So as we were saying many of our load balancersare getting an upgrade  wherewe have advanced traffic controlcapabilities brought by Envoy Now  Envoy  as I said earlier is an open source platform If you re familiar withthe Envoy platform  then again  these servicesmay be familiar to you One of the commitments Googlehas is to the open sourcecommunity And this is anexample of that  wherewe work with the Envoy communityto contribute features backto that community And then we consumethose featuresinto these load balances So as part of thatcapability  you llsee our load balancer haveconsistent set of features also consistent with theopen source capabilities And through thatconsistency  you llsee common control planes common observability and common feature setthroughout our load balances During the initial releaseof our load balancersthat support ourEnvoy  you ll have  you ll see many new features Here on the right  you ll seeexamples of what those are I m not going to gothrough all of them but I ll call out a couple So one example is weightedtraffic splitting Weighted traffic splittingis used when you roll out for example  a newfeature or a new serviceand you want totest that service such as running a canarytest across those services Weighted trafficsplitting allowsyou to take a certainpercentage of your traffic  1   2   5   50   whathave you   and push itto the new serviceyou re rolling out Another example isfault injection In this example you can emulate whatwould happen if you hada fault in your systembefore it happens in production That way  you canharden your deploymentto make sure that when errors doinevitably occur in production you ve had an opportunityto test them usingfault injection  as an example Now  this advancedtraffic control capabilityof our load balancer actuallyextends beyond just loadbalancer capabilities For those who  again  may befamiliar with using somethinglike an Envoy sidecar  thisexample may be familiar to you For those who arenot  Envoy  again is an open source componentand platform whichhandles traffic management We have some of thosecapabilities built nativelyinto our load balancers And again  you don t haveto understand Envoy at allto use our load balancers They re just native featuresof our load balances But for deployments thatare heterogeneous by nature multicloud  or on premises and you want a common featureset of traffic controlacross Cloud and on premisesor multicloud and you wantcommon policy managementand control  youcan deploy what scalled an Envoysidecar  which isa deployable piece ofcode that can deploy nextto your services And those sidecar will containtraffic control capabilities And if you wish  youcan use a component such as Google sTraffic Director  whichis a managed control plane thatcan push XDS configurationsand controls into your sidecars And it doesn t have to be Envoy It just has to be anXDS compatible service So if you do that  then youcan have some services sitbehind Google Cloudload balancersand other services that havea sidecar sitting next to themin Google Cloud oron premises or in multicloud And in that architecture  youcan handle a highly distributedset of systems runningacross heterogeneous systems but running as ifthey re runningin a homogeneous environmentbecause they re allstandardized using the Envoytraffic control capabilities They ll have common features They ll have commoncontrol planes They ll have commonobservability Let s run througha quick exampleof how this might be useful So in this case  imagineyou re an e commerce providerand you have a shared serviceacross many components  in this case  a wallet servicethat could be maybe a paymentprocessing engine Now  here we havean e commerce enginethat will use the externalglobal load balancerto access that service We also have potentially  a call centerusing an internal load balancer which is  again  hittingthe same common service And on the rightside of this picture you ll see the tension is pointof sale using regional loadbalancers  or maybe there snightly batch jobs runningon premises  mayberunning fraud protection So in all these cases you have a common servicethat these varioussystems are using So what happens ifyou want to roll outa new service  aversion 2 of the wallet Each of thesedisparate systems needto be able to test thatservice using somethinglike a canary test That s pretty complicated todo if each of these systemshas its own capabilities And some may or may not haveweighted traffic splitting for example  built in But if you re using somethinglike an Envoy platformeverywhere  then you can use acommon weighted traffic systemacross all these systems This will be the samecapability  the same controlplanes  the same observabilityto see if features are workingor if they re not working And in this way  you can goahead and test a secondarywallet service  say  version 2  and see if that serviceis working as expected And once it is  thenyou can push all trafficto that new service And so in this way  if youdo have a highly distributedset or a complicatedarchitecture thathas reusableservices  whether itbe just a distributed setof services or a systemlike service mesh  you can useadvanced traffic managementthat s built nativelyinto our loadbalancers and othercomponents to reduce the toiland overhead of managinga highly distributedset of services So now what I want to do isrun you through a quick demoto see this live in action In this demonstration we re goingto see how to rollout a canary test Here  we see fourdifferent services  in the upper left  aglobal load balancer lower left   regional loadbalancer  upper right  an internal load balancer and in the lower right we see a sidecar proxy Here we re rolling outthe various commandsrequired to be ableto do our canary test Now we re seeing that sometraffic is switching overfrom version 1 to version 2 And over the course ofabout 30 seconds or so all the various trafficmanagement solutions load balancer andsidecar proxies are beginning to send theirtraffic to both version1 and version 2 The purpose is to make sure thatversion 2 is handling trafficproperly At this point  we see a lotof traffic going to version 2 So now we re going to say thateverything is working properly And we re going tosend all of our trafficnow to the version 2and away from version 1and do our full rollout So again  we have thesame four services And we re going to runthe commands  whichare going to be common acrossthese various services to be able to switch our trafficonto the new  updated version 2wallet service And once again  it takesabout 30 seconds or sofor all the servicesto get the commandand to switch all theirtraffic over to version 2 And so now you see that allthe traffic at this pointhas been switched to version 2 And our canaryrollout is complete Now that you ve seen howthe next generation of loadbalancers can enable advancedtraffic management capabilitiesacross a highly distributedset of services I want to turn theconversation overto Babi  who is goingto run you through someof our capabilities aroundload balancing everywhere Babi BABI SEAL  So one of the goalsof our Cloud load balancingis to meet you where you reat in your journey to Cloud And as part of that  we havegone ahead and introducedhybrid load balancing So what is hybridload balancing You have clients that could beexternal to Google or internal They send traffic to ourexternal HTTPS load balancingor external TCP proxy or ourinternal HTPS load balancing These load balances haveback ends  of course on Google Cloud But now we ve enhanced themto also support back endson prem and in other clouds These on prem andother cloud back endsare connected to Googleeither via public connectivityacross the internet orvia private connectivitythrough HA VPN or interconnects What we re reallyexcited to sharewith you is that we havenow in public previewour hybrid loadbalancing with supportfor on prem and other Cloud backends via private connectivity And we support healthchecking of these back ends And we d really likefor you to try them out So how are some of our customersusing hybrid load balancing What are some of   a use cases The moststraightforward use caseis load balancing toworkloads on on prem More concretely  you can imaginean ERP application runningon prem  INAUDIBLE  regionalextension connected to Googlevia partner interconnects The second usecase is protectingyour on prem workloads againstDDoS attacks with Cloud Armor or you can use identity awareproxy to authorize accessto your on prem resources You can enhance the latencyexperience for your end usersby caching staticcontent on CDN The third use case usesweighted traffic splittingthat Adam had alludedto earlier to doa gradual migration of workloadsfrom on prem to Google Now we are working onadditional use cases such as supporting failoverfrom on prem to Google As we develop more and more we shall share them with you Now let us turn ourattention to howwe are enhancing our enterpriseusers with hybrid loadbalancing use cases So what we are focused on isreducing the operational toiland meeting the scaleand security requirementsfor our enterprise users In this regard  first letus look at Private ServiceConnect So what is PrivateService Connect In typical customerdeployments  youhave multiple teams producingservices for consumptionby other teams The groups offeringthese servicescould be part of thesame organizationor they could be third partyservices or services offeredby Google The way you typicallyset this up securelyis via privateconnectivity by setting uppeering between theproducer and the consumer But you run into a lot ofoperational toil and scaleconstraints with peering Now  this is where PrivateService Connect comes in It is our converged serviceconnectivity offering Because it does this throughprivate connectivity it is secure You don t have thescale constraintsthat you have with peering And it can scale tothousands of endpoints It is also very performant You get line rate because thereis no proxy in the middle Now  how does this PrivateService Connect actually work Well  you have aproducer and a consumer And the producer  you set up the servicebehind a load balancer anda service attachment pointin the producer s address space This service attachmentpoint is linkedto endpoints in theconsumer s address space The workloads in theconsumer  wishingto access the endpoints simply send trafficto the endpoints intheir own address space So how does load balancing workwith Private Service Connect Let s start with the producer The internal HTPS load balancerand the internal Layer 4 loadbalancer helps youscale out your serviceand makes it highly available Now  remember howI was referringto hybrid load balancing The internal HTPS load balancernow supports endpoints on prem So you can use thePrivate Service Connectto offer a serviceresiding on prem securelywithout having to worry aboutoverlapping IP address space On the consumerfront  we are nowworking on putting these PSCendpoints behind our HTTP loadbalancers  external or internal With the URL maps that areassociated with these HTP loadbalancers  you get fine grainedcontrol and selection of whichproducer servicesthat you route to You also can now logevery service invocationwith our HTPD loadbalancing logging In addition  for externalservices that access HBS loadbalancer  you canput a Cloud Armorand protect against DDoS attacksor use our managed TLS offeringto secure the service further Now I m excitedto share with youanother capability that we haveenhanced for our enterpriseusers And that is integrationwith third party appliances So a lot of ourenterprise users as they migrate toGoogle Cloud  wishto bring their favoritethird party applianceswith them as part ofthe lift and shift With our ILPS next hop  wehave made it very seamlessand scalable integrationof these appliancesby our bump in the wiremechanisms What we have done is gone aheadand enhanced that with threeadditional enhancements The first one is supportfor symmetric hashing When you have these multiplestateful firewall appliancesand you havebidirectional traffic you need to ensurethat the returntraffic hits the sameinstance on the way back You typically do this byconfiguring  INAUDIBLE   But there s a lot of operationaltoil associated with it But now  by using symmetrichashing with our Layer 4internal load balancerfor bidirectional flows you no longer have thatconstraint and toil The second enhancementthat we ve doneis we ve enabled allprotocols to flowthrough ILB as next hop not just TCP and UDP So now  by allowingICMP packets through we ll be able to supportping and trace routesso that you can bettertroubleshoot your flows Last  but not theleast  we ve alsoadded tagging supportfor your load balancers What this effectively doesis it allows you to groupa certain set of endpointswith a tag and associate themwith an ILB and route yourtraffic from those clientsspecifically to that ILB The most common use casehas been regionalization where you can tag allworkloads in a specific regionand associate them with thatILB in that specific region We have a very compellingroadmap of our load balancingofferings and features Let me share someof them with you If you look at thisroadmap across the board we have multiple flavors andfeatures of our load balancing What I d like tobring your attentionto is in the Securitycolumn  you llsee that we ve made TLS 1 3the default for our HTPS loadbalancers With perfect forwardsecrecy  gettingrid of old  lesssecure ciphers  wemade our solution more secure By cutting down on thehandshakes and the setuplatency  we ve resulted ina much more optimal end userexperience On the scaling front if you ll notice we ve added support forLayer 4 ILB  INAUDIBLE   What this allows you to do isto scale your back ends  morethan 255 of yourLayer 4 load balancer and  hence  scale outyour service much  now let me hand it off toAdam to wrap this session up ADAM MICHELSON  Thank you Thanks so much  Babi So I wanted to thank all ofyou first who joined us today And I hope youenjoyed our sessionand found it informative  and also wantedto present to youother networking sessionsthat are happening if you reinterested in other topics And of course  withour load balancers we d love it if you wouldhave the opportunityto try them out And we always areinterested in  to hear your feedback interms of which featuresare working for youand what enhancementsyou would like to see So with that  I dlike to thank you on my   for   on my behalf and then Babi s  as well and we do hope that you enjoythe rest of your Next sessions RAHUL HARPALANI  Hi This presentation isObservability Best Practicesfor ReliablyRunning Apps on VMs I m Rahul Harpalani  butmy friends call me Harp I m a product manager at GoogleCloud focused on observability and specificallytelemetry  whichis the capturing of metrics andlogs to enable ops management I m joined here bymy partner Ling Ling  would you liketo introduce yourself LING HUANG  Sure thing Hey  everyone My name is Ling I m so glad to be here today And I m working very closelywith Harp on the cloud opsinstrumentationefforts  driving mainlyfrom the technical sideas the TO of the team And you re going to seeme again at the demo timetowards the end ofthis presentation Harp  I m going to handit back to you now RAHUL HARPALANI  Thanks  Ling In this presentation  we ll bewalking through observabilitywithin virtual machines Recently  the 2021Report on Acceleratingthe State of DevOpsfound that teamswho excel at modernoperational practicesare 40  more likely toreport greater softwaredelivery andoperational performanceand 80  more likely to reportbetter business outcomes whenthey fully invest inmanaging operations at scale So  as you can see  settingup logging and monitoringand making sure you recollecting the righttelemetry data is thefirst important stepof day two operations This helps you run a reliableapplication and triangulate if when  and wheresomething went wrong As a foreshadow  we regoing to focus a loton the value of our agent There are many great resourcesfor deeper dives into the CloudOps Suite for visualization alerting  and other loggingand monitoring capabilities I recommend you goto YouTube and search Stack Doctor  to learn more There is a lot of buzz aroundcontainers and Kubernetes but many of us are stillrunning on VMs and planto do so for theforeseeable future Especially as morecustomers are doinga lift and shift migration fromtheir on prem environments we often hear  how canI set up my applicationsand VMs to run on the cloudwhile simultaneously learningall the features GoogleCloud has to offer Running apps reliablyon VMs does nothappen without good telemetry Data about your applications about your processes about your systems are essentialto delivering a high qualityexperience I m going to spend roughlythe next 10 minutes walkingthrough some of the challengesthat I constantly hear about I ll review theguiding principleswe use in VM observabilityto address these challenges Then I ll give anoverview of the OpsAgent  the single client sidesolution for loggingand monitoring We ll then get ademo from Ling  whereshe ll walk throughinstalling the Ops Agent usingpopular open sourceconfiguration tooling And finally  we ll leavesome time for questions All right Let s jump in As a product manager  somequestions I often ask our usersare  are you running VMs inmore than one environment Are you operating in hybridor a multi cloud environment What solutions are you usingfor monitoring versus logging Do you have to leveragemultiple client side agentsto capture telemetry And what tools are youusing to provision install  configure and manage your agents These questions usuallylead to one conclusion  complexity Simply put  usershave tons of VMsthat need to be instrumentedwith one or more agents These agents need to bemanaged with one or more tools and even thethought of migrationcan be stress inducing Let s quicklyvisualize this to putthis problem in perspective Often we think of onlyour own environments And despite this downscoping it still can be hectic When we start to think aboutall the permutations I justdiscussed beyondour own workloadsand think about thisissue at enterprise scalewith tens of thousands ofVMs  the problem of complexitybecomes glaring For this reason  the integrationbetween Google Cloud and CloudOps is a game changerin reducing complexity and why we at Cloud Opsbelieve having an integratedsolution that meetsyou where you areis very important in helpingyou manage and reduceyour ops burden When we think aboutsimplification we think about the agent For this reason  weembraced several principlesin making our latestagent  the Ops Agent industry leading in itsuser friendly focus Fewer agents   this startsright here at Google Cloud Up until a coupleof months ago  wehad two agents  one forlogging and one for monitoring We ve invested heavilyin a single unified agentto reduce the number ofagents and complexityour users must manage Number two  the ability tomanage our agents at scaleby investing in solutionsto easily support the mostpopular config managementtools that most of our usersare already leveraging Simple configuration  the Ops Agentonly has oneconfiguration file  makingit simple to manage bothlogging and monitoring Finally  third partyapplication support  here at Cloud Ops we re making the movefrom monitoring theplatform to also monitoringpopular applications We don t thinkthat requiring youto install extra agents or extrasoftware to capture this datais something that makes senseor is a good use of your time So we re building thiscapability into the Ops Agent One thing that snot on this slideand is crucial toour strategy isour investment in open source We know the overallstory to telemetryis not cutting it forall operations users And that includes our own users as well as everyone else s Customers are simplyjuggling too many agents too many protocols  and toomany ways to capture telemetry In this vein  we decided to gobeyond a new agent and stakeout a path forward that wouldmake capturing telemetryeasier for all of our users And we are doing this bycommitting and supportingopen observability standards The Ops Agent isbuilt to supportthe future ofapplication and systemtelemetry  which isbeing democratizedthrough open source We ve incorporated two importantopen source technologiesinto the Ops Agent First  Fluent Bit Fluent Bit is an open sourcelog processor and forwarder which allows you to collect logswith incredibly high throughputand resource efficiency The second technologyis OpenTelemetry This is a CNCF supported open source and vendor neutral technologyand is at the forefrontof unifying operations It has garnered the support ofmany vendors in the op space And the Cloud Ops Agentemphasizes our beliefthat this technology willlead to interoperabilitywithin the ops community We ve worked very close withthe OpenTelemetry community And through this effort  muchof which is frontier territory we are helping to guidethe greater telemetrycommunity to a place that isoptimized for you  the user I mentioned thesetopics earlier but I wanted tospecifically call outhow we are meeting ourusers where they are What I hear fromcustomers all the time isthat they are overwhelmedwith all of the toolsthey have to manageand learn to use That s why we made sure thatthe Ops Agent could be managedby the open source configurationand provisioning toolsthat you re already using  suchas Terraform  Ansible  Puppet and Chef  and of course  ourin house option  VM Manager In the demo  whichLing will run shortly you will see theinstallation of the OpsAgent handled by Ansible Additionally  Imentioned the supportfor third party applications Currently we supportNginx and JVM But we have an aggressiverollout schedulefor the rest of this yearto include Apache  MySQL Redis  and many more In 2022  we re goingto increase the supportby 50 additional third partyapplications directly builtinto the Ops Agent Finally  I want to reviewour integration with GCP While the Ops Agent is builton OpenTelemetry and Fluent Bitfor the efficientcollection of telemetry it is purpose built to supportour own operations suite which includes cloudmonitoring and logging With the operations suite  wewant you to have reliabilitywithout sacrificing agility As you ll see in the demo  datafrom cloud logging and cloudmonitoring is made availabledirectly in contextin Google Compute Engine And many other GCP serviceshave similar integrations Time for the demo Ling  are you ready LING HUANG  Ready for a demo So in this demo  we regoing to show youhow to use the GoogleCloud Ops Agent Ansiblerole to install the OpsAgent on the fleet of VMsand then configure Ops Agentto monitor Nginx applications And at the end ofthe demo  we aregoing to go to the console UI So that way  we can show youhow to pull the Nginx dashboardtemplate and create areal dashboard out of it So other detailsof this tutorialcan be found on ourpublic documentation siteas well  just so youcan check it out later Now  let s get started In this demo environment I have alreadyfinished installing some basicsetups  so that those parts aretedious and not fun to watch So I installed Ansible I installed the GoogleCloud Ops Agent role I set up our serviceaccount for the project downloaded the serviceaccount key to my workstation and set up an environmentvariable for it I also finishedsetting up my local SSHkey  which is what Ansible willbe using to talk to the VMs So let s go to theactual fun stuff First  let s take a look atthe Ansible inventory we have So here we are usinga plugin calledgcp compute to helpmanage the GCP VMs This plugin uses the serviceaccount to authenticate And it will list all the VMsin the lingshi sandbox project And it will also show thepublic IP of those VMs And Ansible will lateruse these addresses to SSHinto those VMs Before we kick off thisplaybook  let s firstensure that Ansible can actuallysuccessfully talk to those VMswe are targeting at So we can use Ansibleto quickly pingthem to make sure they are The connection is alive Cool Great The connections   looks OK So now let s dive a littlebit into the actual Ansibleplaybook I ll actually  let me double checkthat the installation of theAnsible role is successful Yeah I already installed this before so it s already installed as it shows The examples playbook we areusing today looks like this Looks pretty simple  right We are basically askingAnsible to install the latestversion of Ops Agent witha custom configuration fileon all the hosts that wehave in the inventory Now  you might wonder  what isin that ops agent config yml Well  that s a good question Basically  our OpsAgent by defaultcollects some systemmetrics and logs However  we can customize it tocollect more logs and metricsfrom third party applications And in this example  weare collecting metricsand logs from Nginx On the metricsside  I have alreadypre installedNginx on those VMs and their metrics expose viatheir stub status module So the key in thisconfiguration partis to make sure that Ops Agentknows basically which URL itshould be using to talk to theirstub status module of the Nginxapplication tocollect the metrics And then  on thelogging side  wecollect both Nginx accesslogs and Nginx error logs OK  let s quit this  andnow it s the exciting part Let s actuallyexecute the playbook So the start of theplaybook executionis going to take a few minutes Well  actually  I haveexecuted this playbook already so this should return just OK And while it s running  wecan go to the console UIand take a look atthe actual dashboard So you can reach this pagevia the monitoring dashboards And the sample library hasour Nginx template already So if you click on this  youcan go to this Nginx overviewand preview what thedashboard looks like Here  we re going toskip that and just importthe sample dashboard So it will automaticallycreate a dashboard for you It has a few metricsthat s tracking already And you can showthat it s trackingthe few VMs we created before And it shows the total requestscoming to the Nginx server It shows thecurrent connections It also shows the historyof the connections And of course  you canalso edit this dashboardor add more chartsto this dashboard because Nginx does ingest morethan just these three metrics On the other side  we mentionedbefore that Ops Agent alsocollects some system metrics So the most interesting onemight be the processes metrics If we go to the dashboardlist  there is GCP dashboard and one of themsays VM Instances If we go to thispage  it shows youour few tabs of differentproperty of the VMs You can see CPU metrics  memorymetrics  and most excitingly you can see processes metrics They will tell you thetop processes by memory the top processes by CPU So that way  we caneasily tell whatis using all theresources on the VMand what couldpotentially go wrong That s the dashboard part Now let s go back to see ifthat execution of playbook  OK  it looks great The execution of theplaybook was successful And that s actuallythe end of this demo Thank you so much RAHUL HARPALANI  Thankyou so much  Ling for that awesome demo As a reminder to thoseof you tuning in  pleaseuse thequestion and answer forum which is embeddedin this video page I want to take aminute to show youother interesting presentationsin the operation spacethat are happeningat Next this week We re launching a previewof fully managed Prometheusservice called ManagedService for Prometheus If you were runningKubernetes  chancesare that you re veryfamiliar with Prometheusand how difficultit can be to manage This new service takes away thepain of Prometheus at scale We are also launchinga preview of a servicethat combines logsdata with the powerand performance of BigQuery You can learn moreabout Log Analytics what it is  and the usecases in that presentation Finally  the integratedCloud Ops story where we talk abouthow Ops Tools canbe used to make your developmentteam and the apps theyrun more effective I d like to thankLing for the demoand thank our viewersfor tuning in CHRISTOPHER SANSON Hey  everyone Welcome to Dev 301 production gradebuild security fordeveloper workflows My name is Christopher Sanson I m a Product Manager at Googleworking on developer tooling And with me is Anthony Bushong Specialist Customer Engineerwho will be joining us a littlebit later to give a demo aboutsome of the newfeatures and conceptswe ll be talking about The increase in cyberand ransomware attacksover the last year including high profile casessuch as SolarWinds  has been awake up call for the industry Security is top of mind forcompanies large and smallacross all verticals In May  the White Houseissued an executive orderon improving the nation scybersecurity in faceof increasingly sophisticatedand persistent threatsto the public andprivate sectors In this 20 minutetalk  we re goingto introduce somespecific actionsthat you can take tocreate a more securebuild environmenton Google Cloud Over the last 20years  Google hasbuilt some of the mostsecure computing systemsin the world  pioneeringconcepts such as zero trust As part of GCP  we re bringingthese internal best practicesto our cloud customersthrough both open standardsand integration in a varietyof ways into GCP s managedservices In June of this year  welaunched Open Source Insights an exploratoryvisualization sitefor viewing the dependenciesof open source projects And in July  weannounced SLSA  S L S A an open framework based oninternal Google best practicesfor formalizing the criteriaaround software supply chainintegrity to help the industryand open source ecosystemsecure the softwaredevelopment lifecycle CI CD is one of the mostcritical paths for securingyour software supply chain It s the gatekeeper by whichcode changes are released outto your users and has many entrypoints for vulnerabilities pulling resourcesfrom a large numberof internal and external sourceswhile accepting contributionsfrom hundreds or maybe eventhousands of developersand other contributorsat your company In the SolarWinds breach  that in part  led to the White Houseexecutive order  it was actuallya compromise built systemthat attackers used to injectmalware into the candidaterelease that was ultimatelypublished to downstream users So in this talk  we regoing to focus inon the build processspecifically and talkabout how to improve thesecurity posture of your builtenvironment And we re going to do this byfocusing on three main areas One  network security Two  build permissions And three  binary authorization So let s start withnetwork security One of the risksto an organizationis data exfiltrationat build time Built by their naturetypically handlea large amount of privateintellectual propertyacross source code  databasesand artifact repositories Now  actually  wewant to minimizethe risk of any of this dataegress to the public internet whether it happens to bemaliciously or just simplyaccidentally The most common waythat companies do thisis by hosting resourcesinside their private networkbehind a firewall Traditionally this has meant thenthat you are self hostingyour own CI CD solution And besides just the additionaloverhead and total costof ownership arounddeveloper velocity of havingto install  manage  upgradeand scale this system it also means that you reresponsible for securing it And as we saw withthe SolarWinds attack bad actors can try to corrupt orattack the build system itself In the case of SolarWinds where at build time when a file was fetchedfrom the source repo it was actually swappedout with malicious codeand then sent to downstreamusers through the compile build and publish process So let s take adifferent approach Fully managed solutions like Google s Cloud Build offer a different tacticand are hosted and managedby Google Cloud running inGoogle s secure network taking the burden offof you and your teamfor having to secure the systemitself in terms of accessingthe workers that builds run on In June  Cloud Build launchedPrivate Pools  a new featurewhich lets you securelyconnect a fully managedbuild system into your privatenetwork using VPC networkpeering It supports private IPs on theworker  data regionalizationand static IP rangesfor IP allow listing Another common patternwith this firewall approachalone is that  inmany organizations it s treated as all or nothing There s inside the firewalland outside the firewall Then once inside  you canaccess anything that is alsoinside the private network Now  one of the best practicesthat we want to followis that builds canaccess only whatthey need to and nothing more So let s say  forexample  that youhave resources inmultiple GCP projectsthat your build needs to access How could weconfigure Cloud Buildsso that it can accessthese resources  but onlythese resources andnothing else that wouldbe sort of an attack factor So this is where VPCService Controls come in With Private Pool supportfor VPC Service Controls you can configureCloud Build to restrictaccess more granularlyto just the servicesthat it needs to access Additionally  you can use VPCService Controls on the projectthat the worker pool itselfis in to restrict egressand an additionallayer of securityagainst leakage of datato the public internet Using Private Pool s supportfor the GCP org policy you can enforcethat all builds arerun on these approved workerpools that were successfullyset up by your security team So now we talked aboutnetwork security let s move over tobuild permissionswhere we have two main goals Again  we want to grant buildsonly the permissions they needand nothing more And second  we want to avoidthe escalation of permissionsfrom the buildsystem to developers We don t want developers to beable to access our productionenvironments directly or decryptproduction secrets that thebuild system needs butdevelopers shouldn t reallyhave direct access to So as an example  let s say wehave two environments  stagingand production The first thingwe re going to dois enforce that all changesgo through our sourcerepo  or version controlsystem  which then invokesthe build automatically This has a number of benefits including change history audit trail  CI tests And using productivebranches  wecan enforce that only developersthat have commit rightsto the prod branchhave the abilityto invoke the productionrelease pipeline This lets us use pullrequests to add a approvalor manual review layerto all suggested changesfrom developers that don t havepermission to release directlyto production Likewise  we re goingto want to make surethat the staging build doesn thave permissions to accessour production environmenteither  which would allowanother escalationof permissionsto environments andsecrets that should notbe accessible fromour staging pipeline For that  we re goingto take advantageof a new feature of CloudBuild for user provided serviceaccounts In this instance  we regoing to create two serviceaccounts  one forour staging billand one for our productionbill  and grant themthe respective permissions thatthey need so that they can onlyaccess the resourcesthat are neededin each of those environments Next  we re going to useCloud Build s integrationwith Secret Manager formanaging the secrets With Cloud Build s nativeSecret Manager integration secrets are onlydecrypted at build time and so not available in plaintext in the build configor to developers And because you reusing Secret Manager it s much easier and fasterto rotate secrets frequentlyfurther  improvingyour security posture The last thing we re going todo is add a manual approval stepfor the production pipeline Using Cloud Build sapproval features we are going to makesure that an adminor other approved revieweris able to manuallyreview and approve allproduction deployments And all of this isin the spirit of again  preventing accessfrom individual developersto production environmentsand other secretsthat they should not beable to access directlywhile enabling the buildsystem to deploy to production So now we ve talked aboutbuild permissions as well Let s move on to the last topic which is Binary Authorization So we ve created ourbuilt pipeline that buildsour container and deploys itout   in this particular case Kubernetes Engine or Cloud Run  through our continuousdelivery pipeline whether that could be thenewly announced Cloud Deploy an open source solution likeSpinnaker or even Cloud Builditself But how do we enforce that onlyapproved builds are deployedto our runtime environments What s stopping a bador unauthorized actorfrom building theirown image and thendeploying that outdirectly or injecting itinto our CD pipeline For this we re going toleverage Binary Authorizationfrom GCP  which letsyou define deploymentpolicies for which containerscan and can t be deployed So in this case  let screate a policy thatsays only containersbuilt by Cloud Buildcan be deployed outto GKE or Cloud Run This will ensure that anyunsigned images or imagessigned by an untrusted buildsystem will fail our policycheck and not get deployedout to our production users whether trying to be deployeddirectly or through our CDpipeline Lastly  Cloud Build will createand assign an asset stationfor each image thatit builds sayingthat it was builtby Cloud Build allowing it to pass the binaryauthorization and verificationprocess and get deployedout to our environment So we talked quicklyabout Cloud Buildand how you can use CloudBuild to improve your securityposture to create a moresecure build environment while at the same time  improvingdeveloper productivityand reducing operationaloverhead altogether So we talked about some ofthe features that were newlylaunched in 2021 including Private Pools user provided service accounts Secret Manager integration manual approvalsand better supportfor Binary Authorization Now  let s handit off to Anthonyfor a demo of someof what we discussed Anthony ANTHONY BUSHONG Thanks  Christopher Let s dive into how we can putthese principles into practice Let s start by taking a lookat our Cloud Run application In this case  we re utilizing acontainerised Python Flask webapp that s accessibleat this URL And it s only accessibleby authorized users so it will provide a tokenwhen issuing a request And you ll see that we geta HTML page in responseto our request Our Cloud Runapplication actuallyhas BinaryAuthorization enabled As a reminder Binary Authorizationis the ability to only admit acertain set of container imagesto your runtime environment like Cloud Run or GKE In this case  we ve configuredBinary Authorizationto only approve images that havebeen signed by the Cloud Runattestor In this case  theCloud Run attestoris only signingimages that have beenbuilt with Google Cloud Build We can see BinaryAuthorization in actionif we try to deploy an imageto the Cloud Run service thatwas not signed by our tester So in this case  forour next 2021 app you ll see here that I havea container image that stagged rogue  and thiswas built out of band not utilizingGoogle Cloud Build And if I was to try to deploythis container image to CloudRun  we should actually seeour Binary Authorization policyin action by denyingthat container imageto be deployed to Cloud Run Now that we ve demonstratedhow we can handle imagesthat we don t trust let s take a lookat the automation thatCloud Build providesand how we can buildimages that we do trust To build our container image what we re actually utilizingare Google Cloud Build ssupport for triggers Both of thesetriggers are connectedto my GitHub repository titledNext 2021  which actuallyhas our application code The first trigger kicksoff builds automaticallybased on any changesto a feature branch In this case  it ll run buildsthat build a container image signs a container image and will actually open upa pull request against themain branch in the GitHubrepository The second triggeractually kicks offand deploys the containerimage to Cloud Runbased on any changesmade to the main branch One of the things wewant to consider hereis the ability to apply theprinciple of least privilege And in this case  we reactually able to do thisby separating the serviceaccounts that each of thesetriggers utilize You can see here  forthe first trigger we re utilizinga service accountthat is titled build next 2021 And this has permissionsto the Container Registryand things likethat that actuallyhelp it to perform thebuild that it needs to run And then the secondservice accountis called deploy next 2021 which has permissionsto deploy to Cloud Run While in this example I mutilizing a GitHub comrepository  it isimportant to call outthat Google Cloud Build also hassupport for self hosted sourcecode management systemslike GitHub Enterprise So you can actually seehere that in Cloud BuildI actually have a hostconnection to a private GitHubEnterprise server that Ialso utilize in Google Cloud And you can see that I veactually made connectionsto various repositories in thatGitHub Enterprise instance This means thatI can also createCloud Build triggers onthese private self hostedrepositories Now let s take alook at the actualbuild that our firsttrigger is running This trigger is lookingin the build folder And the firsttrigger specificallyis looking at the build yamlspecification for Google CloudBuild Our first build step is buildingour container image thatwill end up runningin Cloud Run and we re actuallypublishing thatto Google Container Registry And you can see here thatwe re utilizing Cloud NativeBuildpacks to automaticallygenerate that container image Once we ve signedthe container image our final build step actuallywill open up a pull requestagainst our GitHub repository And you can see here that theway that we re able to do thisis that we re utilizingthe GitHub API And in order toauthorize ourselves or the build  to be ableto perform this action we re providing a GitHub token And so one of the criticalcomponents of a securebuild environment iscareful secret management In this case  GoogleCloud Build allowsus to natively access secretsstored in Google Cloud SecretManager So you can see in thisscenario we re actuallystoring our token  our GitHubtoken  in Secret Managerand providing that back to thebuild itself as an environmentvariable This means that we are notproviding static credentialsto our actual buildor codifying thatinto our build ofcontainer images but rather these aredynamically accessedutilizing the service account which is provided to the build If we jump over to GoogleCloud s Secret Manager we ll see our secret our GitHub token And we ll see hereunder permissionsthat we ve actually grantedsecret access or permissionsto our build next 2021 serviceaccount  which is beingutilized by Google Cloud Build And the important thingto note is that thisis a granular permission It is not for all secrets thatare stored in Secret Manager but rather on a persecret access basis The next important fieldto call our attention toin the Cloud Build specificationis under options worker pool This actually allows forus to specify on a perbuild basis whether or not abuild should run in a CloudBuild private pool So what is a CloudBuild private pool Well  private pools extendthe user configurabilityfor the workers thatactually run a user s build So in this case  we can specifyadditional things like regions We can also specify and haveaccess to more machine types And perhaps  mostimportantly  wehave private VPC connectivityto access private resources either within our ownnetwork in Google Cloudor within the data center thatis connected to our GoogleCloud VPC So you can see herethat I have specifiedfor sandbox private poolto have connectivityto sandbox VPC privatepools which is actually runningin the GitHub Enterprise serverthat I was showing earlierin this demonstration Not only do we getprivate VPC connectivitywith Cloud Build privatepools  we actuallyhave the ability to removeexternal IPs from our workersaltogether And in general  cannow consider howwe can use VPC Service Controlsto secure our build perimeterrunning in private pools So now let s actuallytrigger this firstbuild that ll runon a private pool So I m going to changethe web applicationand just add a couple ofsmall  very minor changesby adding a coupleof exclamation pointsin some of the HTML And I m going to commit thisto the development branch and then what we regoing to want to dois create a feature branch thatwill kick off our trigger nowthat we re done withthe development branch And as you can see our build is nowqueued to run in US west1on a private pool And if we actuallydig into the build we can see our build steps which builds a container image attests a container image or signs a container image and then creates thatGitHub pull request And after a shortamount of time we can see that our build hascompleted  the final step beingthe pull request being created So if we actually jumpover to the repository we can actually see the PRfrom feature against main And so we can review that We can see that ourbuild is completed and now we can utilizeit as an opportunityto basically gate the next buildbecause we have the opportunityto either approve andmerge these changesor we can close the PR So in this case  oncewe click on merge we re actually going to kickoff the second trigger whichwill deploy theservice to Cloud Run So we re going to click that And that branch has nowbeen successfully merged So if we quickly glance at thebuild that is going to run we can see here thatit s a single step and we re just using GCloud as our build step and we are passingin the argumentsto basically deploy toCloud Run  our latest imagethat we built  Now let stake a look at our lastbuild from thesecond trigger thatwill deploy our new containerimage to Google Cloud Run One of the things to consideris that many teams will oftenintroduce a manual approvalstage for pipelinesthat deploy to aproduction environment and this basically allowsfor an operator or developerto have that final decisionto actually promote an imageinto a production environment So as you can see Google Cloud Buildhas actually introduced thenotion of manual approvals So we can see that thetrigger has created our build However  the build hasnot begun to executeto actually deploy to CloudRun because I have not yetapproved it So once we click off Approve we may have the abilityto provide contexton perhaps monitoringor certain documentation or aticket that actually captureswhatever metrics orheuristics that allowus to promote this production And we can clickApprove  and ourbuild will actually beginto kick off and run And after a shortperiod of time we ll see that our singlestep build has completedand we have now deployed the newcontainer image to Cloud Run So let s take alook at Cloud Run And we can actually seethat we have a new revision This was the failedrevision thatwas not signed by the attesterin Google Cloud Build so it failed to deploy dueto Binary Authorization But because we built thisimage with Cloud Buildand it was actually properlysigned by the Cloud Runattester that we had  it isnow successfully deployed And once we issue that requestagain to our application we can see our changes with theadditional exclamation points We demonstratedhow we can secureour build environmentswith granularbuild permissions a secure networkperimeter  manual approvalsand admission controlto targets likeCloud Run and GKE Back to you  Christopher CHRISTOPHER SANSON Thanks  Anthony That was a great demo Now  of course  what we justcovered today was reallymeant as an introduction anda high level overview of someof the specificactions you can taketo secure part of yourlarger end to end ecosystem If you re interestedin learning more here are someadditional talks hereat Next around securingyour software supplychain that we highly recommendyou check out as well Thank you for your time And we re happy totake any questions BRIAN RICE  Hi  everybody I hope you re enjoyingGoogle Cloud Next so far In this session  we regoing to talk to youabout the value ofgetting a Google Cloudcertification to helpyou in your careerand to help advance theprojects that you reinvolved in in the worldof cloud computing My name is Brian Rice  andI work on Google s Cloud sLearning Portfolio Team And I m joined todayby my colleague  Osaze OSAZE IMASUEN  Hello  everyone My name is Osaze Imasuen And I m a technicalprogram managerin Google Global Networking Really excited to be here andlooking forward to this talk BRIAN RICE  Thank you so much Here s the way we re going tostructure our session today I m going to kick it offby talking about the rolesthat are out therein the world of cloudcomputing and their demand andhow that s shaping and changingas we move further andfurther into the cloud era I m going to talk aboutthe learning resourcesthat Google Cloudmakes available to helppeople achieve a wide range ofGoogle Cloud certifications Then I m going tobring on Osaze And he s going to tell usabout how certifications helpedhim propel his career path Finally  we ll wrapit up by talkingabout the benefits ofGoogle Cloud certifications both to the peoplewho achieve themand to the teams thatthey re a part of Let s start off by talking aboutthe demand for cloud computingroles We recently did asurvey where leaderswere asked about the barriersthat they had to cloud success And over 80  of them identifieda skills gap on their teamas a top barrier And this is a reallysignificant problemif you think about the fact thatmore than 90  of those leadersexpect to have increased useof cloud over the next threeyears This tells us that skills area critical factor in companies move to the cloud And we know it s of growingimportance to the cloudcommunity  because jobpostings that were specificallycalling for cloud skillsgrew by more than 40 over the two years that endedin the first quarter of 2021 And of course  therewas a lot of changeand a lot of upheavalover that period This significant growthtells us about the factthat skills are fuel forcompanies moving into the cloudand revolutionizingthe way they work We at Google Cloud areespecially proud of the factthat  in a survey conductedby Global Knowledge the top two paying ITcertifications in the year 2021in the US were both GoogleCloud certifications We take this both as testimonialto the value of Google Clouditself and also tothe programs that weoffer for recognizingpeople s skillsand helping people trainthemselves up so that theycan achieve those credentials Now  cloud computingalways requires an approachto learning and credentialingthat is focused on job roles After all  the rolesthat people playis how people get valueout of cloud computing Google Cloud technology trainingis aimed at cloud solutionsrather than followingparticular products boundaries And it s key to the solutionsthat customers tried to pursue which was theirreason for gettinginto the cloud inthe first place Now I d like to talk with youabout the resources that wemake available tohelp people skill upand the certifications thatwe make available to recognizepeople s skill levels Now  when you re leadinga team of professionals you have to keep inmind that people are allstarting in different places Everybody on a teamstarts with a wide varietyof existing skill levels Also  people have differentpreferred learning styles The same kind oflearning experiencedoesn t work for everybody So that s why we offer a widevariety of choices for peopleconsuming our offerings Some people preferto consume trainingin a traditionalclassroom style format either with a live instructoror with virtual instructor ledtraining We have a worldwide network ofauthorized training partnersthat does just that Other learners want theincreased convenienceand flexibility ofconsuming online trainingwith virtual lecturesand online assessments In order to help thesepeople  we make our trainingavailable in onlinechannels as well with online training partners Both of those two modescontain the same hands on labs in which people can gain realworld skills using real GoogleCloud  not a simulation  thatgives them the confidenceto do new things on the job Finally  some learners want amaximum amount of flexibility They want a la cartelearning  snackable learning That s why we also makeavailable a wide portfolioof hands on lab exercises thatcan be consumed individuallyin a safe sandbox environment Now  all of theselead up to achievingprofessional andassociate level certifications These are benchmarks that a teamcan adopt to bring everybody upto a baseline of skill That s how they know theyhave the right foundationfor innovating andfor scaling up Now  the benchmark conceptis very  very importantin the way we buildour certifications Their content is definedby subject matter experts And they re keyto real world jobroles and the uses to whichpeople put Google Cloudtechnology That s why GoogleCloud certificationshave significant benefitnot just to individuals but to companies as well They re a well defined setof skills that fuels growth There s a wide varietyof certifications because we know that peoplestart at different pointsand because we knowthat people havea wide variety of interests The certifications rangefrom being targetedat people with noprior cloud skillsor experience all the way upto people who have three yearsor more of industry experienceand in a wide varietyof subject matter areas Our certifications are organizedinto three main categories Two of these categories containcertifications that are aimedat hands on technicalprofessionals  the associate level andthe professional levelcertifications Here s the difference Associate level certifications such as Associate CloudEngineer  challenge thecandidate for the cert to mapfrom technical requirementsto the way a customer wouldimplement a solution The professional levelof certificationsintroduces one more layer challenging the candidateto map from businessrequirements through technical requirements to the details of the waythe customerimplementation would work Finally  our credentialthat is aimedat anyone involved inor impacted by the moveto the cloud  CloudDigital Leader contains three main bodiesof knowledge that it covers  general cloudknowledge  knowledgeof Google Cloud inparticular  and awarenessof the business valueand role of a varietyof important GoogleCloud Services Let me give you moreinformation about the examsthat we make available They re a two hour experience And they re offeredin a multiple choiceand multiple select format The exams don thave prerequisitesin that you are not required tocomplete any specific trainingor any specificprior certificationsin order to attempt them We do recommend that peoplehave hands on experiencefor our certifications that callfor that in their descriptions And we also encourage peopleto structure their preparationand make sure they comein with the knowledgethat each credential calls for But no prerequisitesin the realmof training or othercertifications are enforced The exams are deliveredat your choiceof either a remotelocation  in which casethe proctoring for theexams is done online or you can go to aphysical testing centerand take an exam in anon site proctored system Finally  we make availableofficial study guides in addition to all ofthe other resourcesthat we make available These books are availablefrom major resellers of books At this point  I dlike to bring Osaze on Osaze  we wanted toask you to join ustoday to tell us aboutyour career journeythrough the lensof certification Could I ask you to start offby telling us about your careerpath before the cloud OSAZE IMASUEN  Sure  Brian So my career has spannedapproximately two decades And I ve had a varietyof roles  from hands onto technical leadership  tojust pure leadership roles And I would saythat certificationsand learning and relearninghas been a key component It enabled me toprogress throughfrom being an engineer upinto leadership positions Why do I say this Early on in mycareer  I started outtrying to buildinfrastructures for companiesand deliver their ITsolutions internally But I found that the momentI added certificationsand that learningexperience to my journey it accelerated  one  myknowledge  my learning and then also my scopeof responsibility So it s something that hasworked very well for mein my career And it s somethingthat I continueto use and reuse over and over BRIAN RICE  Super And tell us more aboutwhat happened when youstarted moving into the cloud OSAZE IMASUEN  Sure So before thejourney to the cloud my organization wouldprocure  build  and operateour own internal resources So think of private cloud And at some point  we realizedthat the cost and also the timeto delivery made us not meetcertain business requirements And we had to start lookingat the option of movingsome of thoseworkloads to the cloud Now  we didn t migrateeverything over all in one day We started out in astep by step processby doing the trial for settingnon critical workloads firstto get a feel for how thingswill operate in the cloudand also what the securityframework would be because that is ahuge considerationfor anyone trying to makethat journey into the cloud Over time  as ateam  we got moreconfident in usingresources and understandingthe products in the cloud We were able to moveeven critical workloadsinto the cloud And one of thebenefits  which I thinkis perhaps well knownin the industry is  when you run yourown resources internally you most often planfor peak demand And that means that a lot of times you have resourcesidling  not being used whereas in the cloud  inthe more pay per use model you re able to more effectivelyuse resources and also reduceyour spend if you planyour resources properly BRIAN RICE  Indeed Well  to sum up can you tell us nowabout how certifications playeda role in your own transitionto the cloud OSAZE IMASUEN  Sure So I ve always enjoyedlearning and relearning So for me certifications help mefocus my attentionover a period of timeto learn that newtechnology or learnwhatever is new in that space So an example withGoogle Cloud is  over a couple of years I was aware of usingGoogle Cloud platform andalso some other providers But when I made thatconscious effort to say I m going to get certified it helped focus and narrowmy learning to make surethat I mastered the contentand understood theproducts and offeringsand how they relatedto my business The end result is I ve beenable to achieve certificationsin some of theprofessional level engineerprograms like the ProfessionalCloud Architect  DataProfessional  and some ofthe other professional levelcertifications Why I find thesehelpful is it helpscover a range of topics that arerelevant to the business of ITand computing today And the certificationitself provides knowledge That is for sure It gives you theconfidence to beable to interact with eitherexternal or internal peersor customers It also gives youthat confidence And  as well  there salso the Google swagin terms of the goodies the bags  and allthe other nice thingsthat Google gives youwhen you re certified BRIAN RICE  So true OSAZE IMASUEN  Thankyou so much  Osaze for telling us your story Now  the last sectionof our presentationis focused on the benefits ofGoogle Cloud certifications both to individuals and tocompanies and the ecosystem We did a survey inthe beginning of 2020of people who had achievedGoogle Cloud certificationsusing an independentthird party organization Of the people thatwe surveyed  78 said that they had gottenadditional confidencein their professionalfuture because theygot this certification 82  felt like theynow had a special wayto prove their cloudskill competencyto potential recruiters 83  felt that theirresume or CV was nowmore attractive toprospective employers And finally  85  said that theythemselves felt more confidentin their cloud skills IT leaders have also beensurveyed about the impactof certification Out of those who authorizedtraining for their teamswithin the last 12 monthsin a different survey 2 3 did so specificallywith the aimof preparing their teammembers for certificationor re certification 93  of those leaders agreed thatcertified employees add valueto their organizationswell beyond the sheer costof the certification itself How much was that Well  63  of them werecomfortable estimatingthat the economic benefitof those certified employeeswas more than 10 000 USD a year which is a significantincrement for whatis comparatively a smallinvestment in skillsand knowledge As Osaze mentioned  there sa wide variety of benefitsfor getting certification And some of them are swag Folks who achieve ourprofessional levelcertifications do getexcellent merchandisethat allows them to show offtheir credentials in public Everybody gets adigital badge that theycan put on theirprofiles on social mediaand display in other ways Professional levelcertified peoplealso get access to specialevents at Google CloudNext and other venues They get access to the GoogleCloud certified communityso that they can exchangeideas and knowledgewith other certified peopleand seek out networkingopportunities as well Now  this talkwouldn t be completeif we didn t havesomething new to present In order to help thebroad ecosystem getmore cloud andGoogle Cloud skills today we are announcingGoogle Cloud skills boost It s a brand new way toconsume Google Cloud learning It makes more than 700labs  courses  and toolsfor preparing for ourcertifications available And for the nextmonth  we re makingit available to all ofour customers and partnersat no cost The URL isg co cloud freetraining And we hope thateverybody listeningwill take advantageof this offerand see what we aremaking available to helppeople take their careersto the next level At this point  Iwant to thank Osazefor joining us today andtelling us his story And I d like to thank everyonein the audience for joining us And we wish you all the bestwith your cloud careers MANU BATRA  Welcome  folks Thanks for joining the session Protect Your EnterpriseApplications with Backupfor GKE and Anthos We ll start withlooking at Historyof Storage Evolutionin Kubernetes get a deep diveinto new KubernetesBackup Servicefor GKE workloads We also have with usthe Broadcom team who will share theirexperience usingthe Backup for GKE service Before we get intodetails  let sstart with a quickround of introductions My name is Manu Batra I am a Product Managerwith Google Cloud and I own container nativestorage and data managementservices Apparao  do you want to goahead and introduce yourself APPARAO CHITIKELA  Hi  folks My name is Apparao Chitikela Currently  I am working asa Saas Platform   DeliveryEngineer at Broadcom MANU BATRA  Thanks  Apparao for the introduction Now  let s look at the evolutionof storage in Kubernetes We have seen terrificcustomer adoption of GKEfor all types of workloads This includesstateful workloads like databases  that wereuncommon in the earlierwaves of Kubernetes adoption With enhancements inKubernetes storage support customers are morecomfortable deployingdata intensive workloads  likedatabases inside containers The net result isthat GKE customersare managing far more criticaldata inside containersthan ever before But many Cloudcustomers are stillheld back fromgreater adoption bytheir service level objectivesand backup requirements Additionally  we haveheard from customersin more regulated verticals like finance  payments manufacturing  andaerospace  thathave strict data backupand lifecycle managementrequirements These customersare often blockedfrom expanding theiruse of GKE to includeproduction criticalstraightforward workflows Our goal is to unblocktraditional enterprisecustomers from running morestateful workloads with moreregulated datainside GKE customersand meet their backupand BR service levels With that objectivein mind  we reintroducing a new Googleservice  called Backup for GKE Backup for GKE is aneasy Cloud native wayfor customers running workloadson GKE to protect  manage and restore their containerizedapplications and data Now  let s getinto more details into what reallyis Backup for GKEand a bit more ofthe architecture The service operates atthree different layers The bottom most layeris the data layer wherewe take a copy of the data Today  we do supportPersistent Disk which is GoogleCloud s block storage In future  we willalso be supportingadditional types ofstorage  includingfile and other elements So the first part is wetake a copy of the dataand store it as apart of the backup Second is  as apart of the backup we also take Kubernetes objects And this is where a user candefine the scope of a backup A user can define a backupto be an entire cluster a specific namespacewithin a cluster or a set of namespaces Or a user can define a lotmore granular control and scopeof backup call applications And I ll talk moreabout applicationsin the following slides But once the user definesthe scope of backup we pick up all the Kubernetesconfigurations and objects depending on thescope of the backup We also orchestrate backupswithin an application And this is to supportapp consistent backups A lot of applicationsare crash consistent but some applications liketo flush their memory or do QS ing and un QS ingpre  and post backup To be able to accommodatesuch applications there is a flexibility to createapplication profiles  whichenables users to injectscripts at par level for QS ingor other application levelorchestration So as I ve touched backup essentiallyincludes two different elements One is Kubernetes objects  andsecond is the volume backup which is the PersistentDisk data that we backupas a part of the backup itself Now  as a part of backup there are multiple options As a user  you can choosewhere the backup is stored So if there are dataresidency requirements you can specify which region youwant the backups to be stored You also have ability to selectand skip certain resources If you re managingsecrets outside you can tell the backup serviceto skip and not backup secretsas a part of the backup package Same thing  you can alsodecide not to backup volumes if there are other waysyou are backing up data or it s more for transient datathat you don t want to backup Another feature which is very criticalto protect against ransomwareand malicious deletions is time lock The service allows youto time lock backups which will disable anyautomated or manual deletionof the backup  unlessthe time lock expires This is a reallycritical feature in case a ransomwareattack  or somebodytries to delete those backupswith a malicious intent And of course  all datais encrypted  by default Getting more into detailsabout protected applications So I touched upon a user candefine the scope of backup A user can define acluster as a scopeof backup  a set of namespaces Or  depending onthe scope of backup a user can define aspecific application And an applicationcan correspondto something like aMySQL  or a Redis Data or a Postgres database itself And this allows users to executeQS ing and un QS ing commandspre  and post backup We also support more complexdefinitions of applications like an HR dataprocessing application which could be a combinationof a messaging bus  a database and a web services agent The system allows you togroup all those applicationsinto protectedapplication groups And the backuporchestrates  knowingthat a particular applicationis part of a larger component and orchestrates the backupwith that knowledge in mind So essentially  as apart of the backup you can define a cluster  anamespace  or an applicationor application group asa scope of the backup Backups are fine recoveries are better This is where the toolprovides multiple usecases to restore the data You can backup a cluster restore the clusterinto a new cluster or into a new region The system also allowsyou to backup a clusterand restore a specificnamespace or an application This can be done in case therewas an accidental deletionof a specific data set or an upgrade failedand you want to restore the datain its previous healthy state The system also allowsyou to clone and scaleworkloads in different regions So all these aremultiple use casesthat can be supported viathe backup and the recoveryworkflows Getting into a bit moredetails of restore options As a part of therestore  the systemalso allows users tochange certain parametersbefore the backup is restored This may include changingstorage classes or replicaaccounts An example could be  you backupproduction environment  whichwas running PDS SDs But for test and dev  youare using PD Standard So this is where before you restore you can change storage classesto map to the new environmentthat you are restoring Another use case is sub scoperestores where you can alwaysbackup an entire cluster But at the time ofrestore  you can decidewhat is the scope of restore You may restoreonly one namespace or a set of namespaces or one application outof the entire backup Another feature thatwas added to the serviceis for delegated admin And this use case ismore around when  let s say a cluster backupexecutes at midnight But as an appadmin  you may wantto take an ad hocbackup at 4 00 PM just before a criticalupgrade is kicking in So this allows cluster adminsto delegate certain authorityto application admins toexecute an ad hoc backup justbefore critical upgradesor critical changes thatare about to make anapplication  namespace or a cluster Backup for GKE experience iscompletely integrated into GKE So as you log intoyour GKE framework you ll be able to see Backup andRestore in the left navigation Once you get in there you ll be able to seeall the backup policies  allthe clusters that are protectedwith the backup policies  whenwas the last backup executed and what was theduration of the backup The whole interface also allowsyou to create new backup plans restore plans  theintent being it sa completely seamlessworkflow as youoperate your workloads in GKE Now  I ll pass toApparao to talk moreabout Broadcom s experienceusing Backup for GKE service Apparao APPARAO CHITIKELA  Thanks  Manu So I m part of Broadcom team So my team is Saas Platform So this thing  this Cloud it shows as the platform So in the platform  wehave different components Those are infrastructure monitoring  Kubernetes networking  identity and access management While using this platform different products teamswill deploy their applications So Broadcom software uses afoundational Saas Platformarchitecture to manage multipleelements across regionsand lifecycle stages The core functionaluses Terraform modulesto describe the infrastructure node  networking and Kubernetes cluster usedto support Saas products beingdelivered to the customers The environment spansacross development  verify and production  and spreadacross multiple regions By using flexible modules we can create clustersto various sizes  whilemaintaining consistencyacross environments Yeah This is   it will give youmore details about the hostprojects and shared VPC All service projects areconnected with the Shared VPCproject and will communicateeach of the applications So the Saas PlatformFoundation takesadvantage of across project communication using Google s Shared VPC This enables separationbetween the GKE projects And product specific projectsare projects for dB serversthat have not yetbeen migrated to GKE Yeah  this slide talks about thestate of stateful applications So as with most ofenterprise companies we started ourKubernetes journeywith deployingstateless applications This helped us take advantageof the GKE scale and costoptimization Moving to GKE improvedour development velocity besides helping withoptimizing cost But the splitarchitecture createdsome additional challenges as our application componentswere split betweenVMs and containers Because they arestateful applications it takes a lot of additionaldata management servicesin the platform to trust itwith data  especially databases Modernizing applicationsduring the migrationby moving theoperators based versionsallows us to addoperational flexibility We had statefulapplications on VMsand statelessapplications on GKE creating application complexity In order to simplify theoperational environment we started exploring movingstateful applications to GKEas well So we ll continue So these statefulapplications  today  wehave about 150 statefulworkloads deployed in GKE This helps usstreamline operationsand remove a lot offunctional causesby non uniform involvement But a few challengesremain  likely abilityto issue backup and disasterrecovery service level Remote some workloads to GKE but with very fragmented backupsolutions The fragment backupsolutions madeit difficult tomaintain consistencyand createdoperational challenges Key requirements for criticalstateful applications Some of the use cases that willhelp for key stateful workloadsare ability to achieve criticalbackup and disaster recoveryservice levels  abilityto recover an applicationto non healthy state in theevent of fault or corruption Other use cases include abilityto quickly scale and migrateapplications Backup for GKE provides uswith an integrated backupand recovery platform toachieve these critical usecases  enabling us to migrateadditional workloads to GKE Key highlights from GKE backup We were particularlyimpressed with the simplicityof taking cluster backupsand ability to implementstandard backup policies acrossour enterprise workloads Backup for GKE also supportsapplication consistent backups besides crash consistent backup This enables us to breakany soils of backupsacross our organization Ability to takeincremental backuphelps us optimize cost withoutcompromising critical servicelevels So our experience onBackup for GKE service After  INAUDIBLE  introducedus to the new offering Backupfor GKE  it unblocked a lotof critical stateful workloadsto move GKE Before backup  we are usingstorage backup directly It made it verydifficult to adhereto service level projects We are using Backupfor GKE service and we ll integrate it partof our core platform services This will enable us tostandardize our backupand disaster recovery policiesfor the entire Kubernetesapplication fleet Thanks So I ll hand it back to Manuto continue the session MANU BATRA  Apparao thank you somuch for walkingthrough the use caseand how Broadcom isleveraging Backup for GKEto unlock additionalSQL workloadsand deploy them on GKE To recap  folks  Backup forGKE is an easy Cloud native wayfor customers running workloadson GKE to protect  manage and restore their containerizedapplications and data Please reach out to yourGoogle Cloud contactto get more informationon Backup for GKE and please let us know ifthere are any other details wecan provide Thank you again somuch for taking timeto look at our session today And we d love toget your feedback Thank you all KELLY STIRMAN  Hello  andwelcome to  Helping YouRealize the Full Potential ofPostgreSQL with Google Cloud  my name is KellyStirman  and I ma Director of Product Managementin the Databases group Later in this presentation we ll hear from Phil and Timfrom Wayfair on how they reusing Postgres on Google Cloud In this session we re going to discusswhy so many of our customersare choosing Postgres to buildnew operationalsystems and what we redoing to help addresssome of their challenges The team from Wayfair will talkabout their work with Postgreson Google Cloud So let s get startedand start by discussingexactly what Postgres is and abrief overview of its history Postgres is a modern open sourcerelational database  originallydeveloped at UC Berkeley  andits first release was in 1996 While commonly used tomanage relational data it supports a more comprehensiveset of data models like JSON  Key Value XML  geospatial and many more capabilitiesthrough its powerful extensionmodel This rich datamodel support meansthat Postgres is used in a widerange of database use cases Databases createdwith Postgres areused in every conceivabletype of application from the very simplestsingle user databaseapp to demanding enterpriseapplications supportingtens of thousands ofusers running thousandsof transactions a second It s found on the simplestsingle board computersto the largest Linuxservers with hundredsof cores andterabytes of memory managing petabytes of data So why have so manyof our customersadopted Postgres asa database of choice Well  organizations havestarted looking for alternativesto the commercial databasethey have historically used They ve done this as a meansof reducing the perceivedpunitive cost andthe aggressive salespractices that have somewhatsoured their experiences But given the number ofopen source databasesthat are available  whyshould you consider Postgres Well  Postgres has anumber of attributes thatmake that decision simpler It provides a similarand  in some cases better set of functionalitythan the expensive closed sourcealternatives If you re looking for a databasethat supports parallel query partition tables  storedlogic  replication  change datacapture  rich data models  andso on  all in a single product then look no furtherthan Postgres On top of thisimpressive functionality Postgres has demonstrateditself in demanding customerenvironments timeand time again giving others theconfidence to use itin their own applications All this  and a friendlyand engaging communitywith a rich set oftools and utilities has made Postgres an easy choicefor many of our customers But perhaps the mostimportant aspect of Postgresis its role as an open API While SQL is commonlyconsidered the language of data and is indeed responsiblefor the ongoing successof relational databases each relational databasehas its own implementationwith functions and operatorsthat behave slightlydifferently and more substantial differencesin programmatic APIs What Postgres has thepotential to deliveris a common SQL layerthat is consistent regardless of the databaseengine that underpins it Postgres   certainly its SQL layer  could lead to a standardizationof databases in the same waywe ve seen Linux change theenterprise operating systemworld Cloud SQL for Postgres is partof Google Cloud s fully manageddatabase portfolio It provides fully managed reliable database instanceswith features thatmake databaseseasy to set up back up  restore and provide high availability It s the sameversion of Postgresthat you can download fromthe community website with the ability tochoose between the versionof the database thatsuits your requirements Google takes care of theheavy lifting on your behalf patching and securingyour databaseswith very little downtime Cloud SQL aims to providethe best Postgres experienceanywhere in the cloud So given the manyoptions you havewhen deciding on where torun your Postgres databases why should youchoose Google Cloud Well  Google understandsthe importance of the rolethat open source playsin the enterpriseand is committed to providethe best experience for usersof Postgres We particularlyunderstand the importancethat Postgres willplay as a SQL standardfor databases in the future We offer a one of a kindplatform to run your Postgresdatabases  using the sameinfrastructure that runs manyof the Google applicationsyou use daily You get access to fastand scalable computeand high performance low latency networkswith a global reach Google continues to investin its own dedicatedglobal network backboneand standing updata centers aroundthe world to ensureyou have the fastest and mostsecure access to your data We will continue to offer thebest experience for developers whether you re building outyour own VM based applicationsor building them ontop of Kubernetes Google Cloud offers modernAPIs and frameworks whether you plan touse Postgres to buildmodern microservicesor more traditionalmonolithic applications We understand developersand are committedto making your teams asproductive as possible We ve created integrationsand management toolsto simplify how you makethe most of your data And we providerich and open APIsto protect yourapplication investments We ve been hard at workadding important featuresto Cloud SQL for Postgres Recently  we added support forlogical replication and IAMauthentication  makingCloud SQL easierto integrate with yourcore enterprise systems Cloud SQL for Postgres nowsupports over 140 flagsand 44 extensions We have tuned the operatingsystems on our Cloud SQLinstances fordatabase workloads Google Cloud wasthe first to supportPostgres 13 in the cloud And in just a moment  we lltalk about Cloud SQL insightsand cost recommendationsvia Active Assist Our database migration servicemakes migration simple  safe and secure DMS is serverless  so there snothing for you to provisionor manage DMS will do all thatheavy lifting for you More than 85  of DMSmigrations are underwayin less than an hour And a majority ofthese migrationsare coming from other clouds Observability is criticalfor managing your databasedeployments Recently  we launchedSQL Insights giving you best in classdatabase observability usingopen standards SQL Insights helpsyour developer teamsto understandperformance issues whether you re buildinga monolithic appor using microservices And now we providecost recommendationsthrough Active Assist  helpingyou right size your resourcesand optimize your spend potentially saving youthousands of dollars a month So thus far  we ve been talkingabout Cloud SQL for Postgres Now I d like to spenda few minutes talkingabout Cloud Spanner and somework we re doing with Postgres Cloud Spanner is therelational databasethat banks  game studios retailers and organizations and many other industriestrust with their mostdemanding workloads It s the same databasethat powers Google smost recognizable products Spanner offers globalconsistency and five ninesof availability atalmost any scale without painful sharding orre architecture as you grow Spanner s newlower cost of entrynow makes realizing thesebenefits even easier You can start small andbe assured that Spannerwill scale to meet your needs Even with Spanner and all ofGoogle Cloud s unique benefits our customers want options Spanner isn t the only manageddatabase service out there We want our customersto feel confidentthat they ll be ableto move their dataand applications elsewherewithout a disruptive rewrite The PostgreSQL interfacefor Cloud Spannerthat we announced earlier todaywill provide the scalabilityand reliability of Spannerthat enterprises trust  alongwith the familiarity andportability of Postgresthat developers love We re making it so you can bringyour existing skills and toolsaround Postgresto take advantageof the uniquecapabilities Spannerhas to offer in terms ofavailability and scalability So  to sum it up Postgres is becomingthe relationaldatabase of choicefor the enterprisefor many good reasons We believe Google Cloudoffers the best placeto run your Postgres databases And we re seeing a trend tomake Postgres s SQL layeract as an open API forapplications and deployments Now  I d like to handthings over to our friendsfrom Wayfair Tim and Phil please take it away TIM QUINNEY  Thanks  Kelly Hello My name is TimQuinney  and I leadthe product team for databaseplatforms here at Wayfair I m joined by PhilPortnoy  who sour lead architect andengineer for database platformdevelopment Wayfair is a platformthat s solelyfocused on the category ofhome across our six brands including our marqueeWayfair com  and our specialtybrands of Perigold  AllModern Joss   Main  Birch Lane as well as our Wayfairprofessional business We have more than 31million active customers shopping a catalog ofmore than 20 million itemsfrom a network of more than16 000 global suppliers Powering the systems tomake this all possibleis a team of over 3 000 softwareengineers and data scientists When considering the needsof our engineering community the use cases wesupport run the gamutfrom integrating thesuppliers to build our catalogto marketing theproducts and poweringthe customer facingshopping experience online all the way to managingthe delivery of ordersthrough our global logisticsand operational systems To accomplish this our database strategyis focused on providingwell paved paths for our usersto quickly provision anddeploy database solutions thatmeet their scale  resiliency and latency requirements all while providing a high tiersupport and subject matterexpertise Core to this strategy isPostgres and Google s suiteof database products Whether it be single regionreads and writesfor similar applicationsrunning on CloudSQL or a globally distributedsystem running on Google CloudSpanner  we have foundthat Google s suiteof database offerings enableus to drive innovation To share more about whywe re excited about Postgresand the Google offering I ll hand it overto my colleague  Phil PHIL PORTNOY  Thanks  Tim For me  as anenterprise architectfor a fairly largeengineering organizationof a couple of thousandsof engineers and analysts technology matters PostgreSQL  being an open sourceand free to use product is the type of technologythat enables large enterpriseslike Wayfair to have controlover their own destinyin the data space PostgreSQL is aproven technology Our developers can use it withany architecture and softwarestack of their choice without compromisingvelocity and portability There is a massiveamount of learningmaterials  documentation open source guides And community support isavailable around the clock which truly putsdevelopers first However  as most ofyou probably know considerations about databasesdon t end with developers Managing provisioning orchestration  monitoring and maintenance of physicalor virtual infrastructureis a massive componentof operating data stores Google Cloud Platform takesthe infrastructure managementchallenge out of theequation completely allowing us to focus effortand resources entirelyon product development Using open sourcesolutions helpsWayfair to transformour businessin much more ways than one But there is one benefitthat s hard to overlook and that benefit is cost In modern  fast pacedtechnological environments time to market is critical And PostgreSQL onGoogle s Cloud SQLallows us to minimize theeffort of preparing our productsand applicationsto serve productionworkloads  while significantlyreducing the overall costof the solution We ve discovered that supportingsimilar workloads on Cloud SQLand PostgreSQL is anywherefrom four to eight timescheaper than a comparablecommercial databaseengine provisionedon Compute Engine after accountingfor licensing costsand depending on theredundancy and distributionlevel of thedeployment  of course On top of thatdifference  with Cloud SQLbeing a completelymanaged service there are additionalcost savingsassociated withdramatic reductionin operating costs comparedto deploying on premisesor in an unmanaged fashion This makes PostgreSQL aperfect  cost effective solutionfor the majority of onlinerelational data storageneeds in Wayfair However  for somedistributed use caseswhere consistent  safewrites to the same data setare happening frommultiple locations spanning countries andsometimes continents a slightly differentsolution is required That s where GoogleCloud Spanner comes in a truly distributedand truly consistentsolution for relationaldatabase workloads thatrequire distribution We re able to provisiondistributed data storeswithin minutes and withalmost zero operational effortrequired from us Thanks to Google sTrueTime distributed clock Spanner is capable of providingthe same consistency guaranteesas PostgreSQL at a minimaladded latency cost switching from writing andstoring data in a single datacenter to persistingthe same datapoints in a globallydistributed database And having the dataavailable locallyfor clients aroundthe world requiresnext to zero development time And with technologieslike PostgreSQL Interfacefor Spanner  softwareproduct journeyscan start with a localPostgreSQL databaseand shift towards Spannerlater in the lifecycle providing global availabilityin an effortless fashion Back to Tim TIM QUINNEY  Echoing whatboth Phil and Kelly shared the value propositionof Postgresis highly resonant to us It s open sourceand cost efficient It has tremendouscommunity support with low barriers ofadoption for our users Google s investment in databaseproducts leveraging Postgres from Cloud SQL to the newPostgres interface in Spanner becomes increasinglycompelling for us because these products areoffered as managed servicesand enable portabilityto follow the lifecycleof an application I m really excited aboutthis concept of portability An application can start witha simple Cloud SQL instance Maybe it s a proof of concepton a Friday afternoon And before the team knows it it s a runtime critical system In due configuration  youcan enable scale read If it grows to the occasionneeding global distribution it can be migratedover to Spannerwithout the need of asignificant re architecture This saves us from overprovisioning infrastructurefrom perpetual rewritesof applicationsand from the significantoverhead of managinga sharded database environment For these reasons  teamssuch as our supply chain catalog engineering marketing technology are already leveragingPostgres on Cloud SQLfor some of their systems Additionally  Spanner isquickly gaining traction their home servicesand customs platformteams recently adopting As we look to the future we re excited to seewhat kind of performanceand operational gainscan be achieved as we considermoving other systems to CloudSQL and Cloud Spanner One that comes to mindthat s particularly excitingfor us is our user profileservice  which is currentlyon a shardeddatabase environmentacross many servers This is a prime candidatefor migration to Spannerand core to our customershopping experience And now I ll handit back to Kelly KELLY STIRMAN  Thanks Phil and Tim  for that And thanks to all of youfor joining us today We appreciate it Hope you have a great day JERZY FORYCIARZ  Hello  Next Welcome to ourbreakout session  wherewe ll talk about the sixincredibly powerful GKEcapabilities youmay have missed It s a fun session packedwith cutting edge techand a demo of ournewest capabilities My name is Jerzy And today with me is Roman Both of us are product managerson Google Kubernetes Engineteam Together  we will learnhow using GKE and adoptingthe right culture canhelp customers like Ubie a Japan based healthcare technology startupthat automatically generatesmedical records usingan AI powered patientquestionnaire app achieve 20  savingwhile deployingto hundreds ofmedical institutionsand improving availability Or Scoopwhoop  a media platformfor whom autoscaling on GKEhelps match capacity withthe real time demand The result is 99 uptime and 50  reductionin infrastructure costs  whileremoving operational burdens Now they can invest moretime on coding and lesson managing the resources As well as Mr  Cooper  anindustry leading mortgageservice provider that now canpack their nodes to achieve 90 CPU load In fact  Forresterconducted a studyin which they created acomposite organization basedon the GKE customersand concludedthe following total economicimpact numbers of GKEwith expected investmentpayback in under sixmonths for a typical customer Check their study to learn more Over to Roman ROMAN ARCEA  Thank you  Jerzy This is great Our team was on a criticalmission  a missionto empower you the user  to neveragain waste time and moneyon infrastructure youdon t actually use When you run workloads onGoogle Kubernetes Engine we know you ve madethe choice in favorof simplicity  efficiency security  and scalability But we also know that fallingfor the non efficiency pitfallsis easy when you are in chargeof running infrastructureat scale  with its endlessnumber of componentsand possibilities In six years ofGKE s existence  welearned that four majorchallenges make your life hard The first challenge is cultural Many teams that embrace thepublic cloud aren t usedto the pay as you gobilling style and frequentlydon t fully understand theenvironment their apps arerunning on  in thiscase Kubernetes The FinOps movement  whichis getting lots of attentionrecently  is all aboutevolving such a culture One of the FinOpsbest practices isto provide teams withreal time informationabout their spending andtheir business impact Small things like thishave considerable impacton companies  culture  resultingin a more balanced costoptimization equation The second challengehas been packing How well do youpack applicationsonto your Kubernetes nodes The better you pack apps ontonodes  the more you save Then we have theapp rightsizing the ability to set appropriateresource requests and autoscaleapplications deployedin the cluster The more precisely you setresources on your pods the more reliably yourapplications will run and in the vast majorityof cases  the more spaceyou ll save in the cluster And the last challenge isnode scaling down your clusterduring off peak hours Ideally  to save moneyduring low demand periods for example atnight  your clustershould be able to scale downfollowing the actual demand However  in somecases  scaling downdoesn t happen asexpected due to workloadsor cluster configurationsthat block cluster autoscaler By themselves  eachof those are tricky But together  thisis a complex problem In our experience  most ofour provisioned environmentshave at least two of thefollowing challenges at hand But fear not We re here to help Let s look at some of theincredibly powerful GKEcapabilities youmight have missedthat will take you a step closerto running GKE at scale costeffectively Jerzy  over to you And tell us about yourfavorite top three JERZY FORYCIARZ Thank you  Roman I certainly havemy own top three Let me focus on some of theexisting top of the line GKEcapabilities And here there are Autopilot Traditional Kubernetesexpects you  the user to understand and manage thecontrol plane  worker node security configuration  upgrade scaling  and availability GKE  on the other hand is a truly managed servicethat helps you focuson your business The best and most intelligentway of using Kubernetes todayis by activating GKE Autopilot a mode of operation thatautomatically managesthe most complex aspectsof your infrastructureconfiguration If  however  the user needsmore control of their clusters she can choose a standardGKE mode of operation You can think of modes of GKE asthe level of control customershave over a given GKE cluster And the best part yet You can mix and matchmodes of operationbetween different clusters So you choose GKE Standardfor advanced configuration flexibility over thecluster infrastructure or GKE Autopilot tohave Google provisionand manage entireunderlying infrastructure User is in full controland can break glass converting Autopilotclusters into Standard These are just a few examplesof how Autopilot does thingsdifferently Autopilot clearly wins as ahands off Kubernetes experiencebecause it optimizes forproduction like Kubernetesexpert  provides strongsecurity posture lets Google become your SRE reducing the day 2 operation improves resourceefficiency  and it s stillKubernetes  still GKE But how does this all playinto GKE cost optimization GKE Autopilot shares afour way scalability capabilitywith the GKE Standard Same  well known tech ashorizontal pod autoscalingor vertical podautoscaling apply and serveas a frame of cost reference Users are still in full controlabout how their workloads willscale and behave However  theinfrastructure is nowmanaged fully by the Autopilot And that has significantoptimization implicationsfor everything youplan to do with GKE You remember earlyin the conversation we talked about thefour main challengesof cost optimization With Autopilot  you pay for thecompute resources requested With our new pay per podmodel  paying for nodesremains a thing of the past That means that bin packingis not a problem anymore How cool is that Not only do you get amanaged hands off experiencethat lets you focuson your businesswhile improving your securityand operational posture you also get rid of one ofthe most challenging aspectsof cost optimization And if you thoughtthis is not enough  howabout not having to pay forthe compute overhead costsof running the OS and thecost of running Kubernetes such as Kube system components Scalability is a big subject We graduated 15k toGA in 2021  which for the vast majority ofusers  removed scalabilityas a constraintin cluster design Why does it matter Thanks to operatingin one large cluster you can simplify microserviceslifecycle management easier absorb largerspikes in resource demand and shorten the time to processdata for batch workloads As the result  you cancombine the servicesof your organization into oneor just a few GKE clusters thatprovide better utilizationand  in effect  will cost less Combine this withour cost optimizedmachine types suchas E2 or Tau for evenbetter price performance ratiocompared to general purposeVMs Next is the nodeauto provisioning a feature that is internallypowering all of Autopilot But what if you can tyet run Autopilot Bin packing  thatannoying issue thatrequires you to either makesure your workload fits wellinside of the machine sideor manually create and managemultiple node pools and playwith affinity  that can quicklyget out of control and becomean operational bottleneck But there is a simple option  configure nodeauto provisioning Node auto provisioning is thenext generation of our clusterautoscaler that not only knowsto scale your cluster usingthe chosen VM types  but alsoknows to create entire nodepools from scratchthat are best suitedfor your specific workloads Node auto provisioning makesGKE clusters fully automatic provisioning relevant machinetypes like GPUs and TPUs and removing nodepools when not needed Node auto provisioning is alsointegrated with vertical podautoscaling and provisionsoptimal node poolsahead of vertical podautoscaling updatesto ensure quick action In practice  thatmeans that you remore likely to have VMsthat are the right sizefor your workloadsthan what you getwith the manual provisioning All right  Roman Those are my top three What about yours ROMAN ARCEA  Thank you  Jerzy I definitely havemy own list as well Let me go through someof our latest additionsthat we believe areimportant for everyoneto know and understand We know that with Autopilotand node auto provisioning bin packing is a nonexistentor considerably smallerof a problem But what aboutapplication rightsizing While app rightsizing isa challenging dimensionto tackle  we ve beenmaking significant progressto make it simpler for you Let s look at the latest previewfor the multidimensional podautoscaling Horizontal podautoscaler is a termthat is familiar tomost everyone using GKE Horizontal pod autoscalingscales your pods horizontally adding replicas when certainthresholds or conditions aremet HPA works hand in handwith cluster autoscaleror node auto provisioner making surethat resourcesrequested by containerscan be accommodatedon an existing node If this is not thecase  new nodesare being addedto the node pool However  that means that theuser deploying containersmust have expert knowledge ofthe exact amount of resourcesto be configuredfor their workloads Based on what we know ofhow Kubernetes is used the right resource choice is atrue challenge that few master This is why we veintroduced the vertical podautoscaling  a capability thatunderstands the true resourceusage of your containersand recommendsor directly adjust therequests to get themas close as possible to whatyour containers actuallyneed to operate properly However  HPA and VPA did notalways play well together As HPA had targetssuch as CPU  VPAhad trouble understanding eitherresources under consumptionwas a choice or an effect This is why we ve introducedthe multidimensional podautoscaler  now in preview With a singleobject  GKE can nowmanage both horizontaland vertical autoscaling While HPA continues to addor remove pod replicas basedon CPU  vertical podautoscaling workson memory  one of the mostover provisioned resources to adjust it to requeststhat meet best your containeractual usage needs  with thisfurther simplifying the apprightsizing andthe off peak hourover provisioning challenges Now  what if we told you thatyou could improve your GKEcluster utilization meaningfullywith a single change Please meet theoptimize utilization profile now in GA If your applicationsare ready to toleratea more aggressive clusterautoscaler behavior this is the feature to try The optimize utilizationprofile is the secondin line of our autoscalingprofiles giving userschoice of a more activeautoscaling and schedulingbehavior The well known balanced profileoptimizes for availability It scales down idle nodesonly after 10 minutes Workloads are also spreadas much as possibleby the scheduler Optimize utilizationprofile  on the other hand strikes a balance betweenavailability and costefficiency Nodes scale down after1 minute of being idle Aside  schedulertries to bin pack podsfrom different deployments asmuch as possible to add scalingdown speed andstatistically maximizeyour chances of havingempty nodes thatcan be easily removed Where it takes the balancedprofile as much as 10to 30 minutes toscale down nodesafter an abruptstop in requests it takes optimize utilizationprofile 1 to 3 minutesto achieve the same result The way this isachieved is reducinga number of intervalsand thresholdsthat increase the sensitivityof autoscaling behaviorto set load thresholds such as scale down times delay after add and scan intervals In our tests optimize utilization profileshows up as being oneof the capabilitiesthat you could adoptfor significantlyimproved outcomes We ve now had a look atfive great GKE features thathelp improve the balancebetween cost and performance outof the box But how do you even know ifanything needs to be improved And what is the scale ofthe opportunity at hand How do you help yourteam take a step forwardinto solving for the culturalchallenges of GKE costoptimization I am thrilled to introducethe GKE cost optimizationinsights  nowavailable in previewacross all regionsand GKE cluster types GKE cost optimization insightsgives you a single entry pointinto the GKE costoptimization journey rightinside the user interface Let s see how it works Let s start with a blank projectand create the first cluster For this  we will usethe recently releasedcost optimizedcluster setup guide The setup guide helps builda GKE standard clusterwith all the best infrastructurelevel cost optimizationfeatures built right in The cluster is now created But how do we know where westand on the cluster resourceusage efficiency With the new Cost Optimizationtab  discovering thisbecomes a no brainer In one click  we seeall the key informationfor the clusters such as used requested  or allocatableresources for memory and CPU Cost optimization insights worksacross Standard and Autopilot should you have one orhundreds of clusters Let s add a number of otherclusters to see it better Now  bin packingis not a challengewhen you use Autopilot This is made veryclear in the view differentiating theStandard and Autopilotclusters on the type ofmetrics you should care about In an Autopilotcluster  only theused and requestedresources matter while with Standardyou should alsocare about yourallocatable capacity Spotting your cost optimizationopportunities out of the boxhas never been easier And with the timepicker  you can evensee the data across thetime horizon of your choice CPU and memoryaggregations in timegives you visibilityof your cluster sizesand running capacityat all times Cluster insights helpsspot bin packing app rightsizing  and potentiallyoff peak hour optimizationopportunities But which of the workloadsare the largest contributorsto resource consumption To help answerthis  you have nowaccess to cost optimizationinsights at workload level 2 Let s deploy a number ofworkloads to various clustersto see this in action There are now multipledeployments available Let s also top it offwith one of the offeringsfrom the marketplace As everything is deployednow  within momentswe start seeinginsights for workloads such as used  requested and resource limitsfor each individual deployment With this new view understanding applicationrightsizing opportunitiesbecomes considerably lessof a challenge By looking at used versusrequested capacity all teams can keep an eye onoptimization opportunitiesfor their individual workloads And that in turn helps with onemore cost optimization aspect  building a solid costoptimization cultureacross your entire team Watch this space We re just getting started And we re looking forward toadding more capabilities thatwill improve visibilityand provide recommendationsfor GKE cost optimization That was it from my side  Jerzy Take it from here JERZY FORYCIARZ Thank you  Roman And big thanks to our audiencefor staying with us today And remember  we re on amission to empower you to neveragain waste time and moneyon infrastructure youdon t actually use So here s our call to action If you re only gettingstarted with GKE check ForresterTotal Economic ImpactReport to see what impact GKEmay have for your business Are you already deployedand going full speed Follow these six thingswe discussed todayand see how this advancesyour GKE capabilities Have you alreadydone all of this Come partner with us formore in depth collaboration Get in touch with youraccount representativeor reach us via media channels If you want to learn moreabout GKE cost optimization follow these links Thank you CHRISTOPHER CROSBIE Hey  everyone I m excited to be here todiscuss how you can speed upyour software development withbetter insights and deeperdiagnostics with newcapabilities thathave been jointly developed fromthe BigQuery and Cloud Loggingteams I m Christopher Crosbie I m a product managerfocused on advanced analyticshere at Google Cloud Later I ll be joined by Charlesfrom the Cloud Logging team So a fundamentalrequirement for enterpriseswanting to transformthemselves digitallyis a need to store manage  and analyzelarge quantities of operationaldata from a variety of sources Logs are a critical datasource for troubleshootingmachine learning  dataanalytics  and security So while Cloud Loggingtoday can alreadyhandle massive amounts of data capturing petabytes with ease many developers still struggleto analyze this log data and they re unable to findthose symptoms in the log thatlead to deeper diagnostics And despite the largevolumes of data often they still find it hardto move from a retrospective loganalysis to predictiveinsights and prevent issuesbefore they happen So to help you achieve this nextlevel of software developmentwith better insightand deeper diagnostics Charles and I arethrilled to announceLog Analytics  a newcomprehensive  easy to use cost effective and performant toolwithin Cloud Logging thathas the performance and powerof BigQuery behind it This capability shouldhelp you use your loggingdata for bothbetter diagnostics as well as getting moreinsight into your business Now with the companiesI ve worked with I ve seen several teamsthat rely on logging data Yet the majorityof the users reallystruggle to analyze that highvolume of messy log data And that leavesorganizations with a lotof unrealized potential So the new Log Analyticsfeature is directlytargeted at those teams who arewrestling to extract insightthat they need from that hardto analyze logging information This is targeted at DevOpsteams who want to improve systemreliability  and they might bestruggling with analyzing logsat large scale and want acentralized logging repository Or IT teams that need insightinto how to better managetheir fleet  andthey re strugglingto analyze signals acrossmultiple clouds or even datacenters and look at thetrends over historical amountsof time Security teams thatwant to identifyattacks  and they want to starttaking advantage of the latestand greatest machinelearning and advancedanalysis techniques todetect those threats Or business users that want tomake better decisions but todaythey struggle toget logs integratedwith their visualizationand analytic tools and they struggle to combinetheir logs data and businessdata Now when I go andtalk to these teams it becomes pretty obvious whatthe source of their struggleis What makes loganalytics hard todayis the complexityof the data systemsthat organizations need to run So most of you watching you re probablyfamiliar with a similarconvoluted architectureat your own company Where possibly application logs they go into an elastic index and then that s where yourapplication performancemonitoring or Kibanadashboards are But then you havelower level operatingsystem network logs that flowinto a proprietary securityvendor s database Then of course there s other machinegenerated logs  like maybefrom sensors or IoT data And they get storedin a format that soptimized for timeseries analysis  whichis then downsampledinto a data warehouse And so there is a lot ofoverhead with all of this and you definitelypay an additional taxfor extracting and copyingall that data around So when I chat with theend users of these systems they tell me that today scommon architecture patterns it generates fivecritical issuesthat make this loganalytics so hard First is the latency issues There s just a lotof additional hops and that means data takeslonger to get to different endsystems Two  if no clearsource of truth Data transformation mighthappen in multiple places That makes it achallenge to understandwhat s happened to the data Three  moving tomachine learningrequires a lot ofadditional hoops It is not trivial to just takea TensorFlow model or a notebookand put that on top ofthese various systems Four  the reliance onmultiple interfaceswhere the same users needto learn different systemsjust has a whole mess ofdifferent applications and tabsthat users have to manuallyreconcile themselves And even with allthis user pain the IT teams stillhave to managea lot of high costand managementcomplexities of runningall these different vendordatabases and applications When I ask customerswhat they want it essentiallyboils down to havinga separation between thelog aggregation and the logapplications More of our tech savvycustomers have alreadystarted to adopt BigQueryas their centralized logaggregation store for datacollection  normalization and storage management taskslike auditing and compliance Now here at Google  itdoes seem intuitive to usthat customers arechoosing BigQueryas their next generationlogging analysis platform Dremel  which isthe query enginethat BigQuery is basedon  that was originallydesigned and built for logs And it s being usedfor over a decadefor analyzing logs and creatingsophisticated machine learningon those logs for some ofGoogle s biggest applications  Search  YouTube  Gmail And so if I startto compare BigQueryagainst typicallogging platformsthat customers are using todaythere s some clear advantagesthat I m seeing First is just the management Today s logging platformsneed more automationjust due to the applicationsshifting into ephemeral serversand microservice architectures It s just becoming waytoo hard to keep upwith all the variousIT managementtasks that are required forrunning the various loggingbackends BigQuery  fullymanaged  serverless Two  customers needdata lake scalability Logs are comingfrom all over now On prem  multiple clouds Kubernetes  real world IoTsystems What used to reallybe a telco scalefor a lot of thesesecurity vendorsthat relied on logsdata is pretty standardfor your average enterprise now And when it comes tosecurity analysis you need these logs ina centralized place Threats are not system specific A hacker got into GCP Guess what  they probablygot into AWS  too And you want to review andtrack that in one location It s equallyimportant to also havea full history of the logs That s becoming more important If you look at something likethe recent SolarWinds attack that went back over 9 months It doesn t workanymore to just have30 or 60 days worth of logsin your security database You need something that cancost effectively  independentlyscale the storage andanalysis like BigQuery can And speaking of security the security posturethat many of these legacyor on prem log systemshad relied on securitycontrols like maybe the networkas a primary layerof protection But now in ourcross cloud world things like networkperimeters are vanishing And you mix that with tougherregulations like GDPR you really need abackend now thatcan help you achieve asecurity posture thatmakes it easy to alignfor our digitalized worldand that you can trust withyour logging information Another big one that I mreally hearing from customersis a need for advancedanalytics and custom ML Customers are finding that thesepre canned outlier dashboardsthat their logs applications arecoming with just don t cut itfor the systems today I was working withone enterprise whomoved over their existing INAUDIBLE  to GKEand almost instantly got30 000 erroneous alerts So customers havetold me that theywant to do their own analysis develop their own algorithmsthat are designed for theirarchitecture  their business BigQuery is already being usedby some of the largest securityvendors out there to developXDR platforms as an alternativeto reactive approachesof the  INAUDIBLE   And they re doing thatwith GCP AI capabilitieslike BQML and Vertex Finally  there s justa lot of new use casesthat customers want to use that customers are realizing Customers  they understandnow that there sa lot of business value injoining all of this loggingdata to their businessdata  and then theywant to share it with others I m working with ane commerce company who sanalyzing weblog data alongsidetheir clickstream datato better understand how usersinteract with their site and then they can offer thembetter product recommendations And that s already led toincreased sales for them So having logs that can beeasily analyzed and joinedthe rest of your warehouse that just opens upa whole new set of use caseslike predicting customerbehavior  optimizingonline marketing campaigns and maybe evenunderstanding how employeesengage with different softwaresolutions in your organization So to unlock these usecases and help customersbetter understand the logs datathat they bring to BigQuery we re enhancing supportfor log analyticswith investments inthree core areas Semi structureddata  search indexes and time series analysis These new featuresand functions aregoing to let youexplore and analyzethe wealth of loggingdata that you havealongside your business data So with native JSON  there sgoing to be a new data typespecific to JSON And it s going toon a load shredthat JSON document thatcan be unstructured constantly changing  intoa columnar store formatso you can get the sameperformance and costeffectiveness thatyou ve come to expectfrom any other structureddata in BigQuery Along with that  you re going tofind additional JSON functions like a dot subscript notationto make it a lot easier to workwith these JSON data sets Second  the searchindexes are goingto provide text indexes thatprovide users the abilityto run what I call needlein the haystack typequeries on the log data This is when you want topinpoint a very specific dataelement So let s say Iwant to find ChrisCrosbie in a petabyteof logging dataso I can delete himfor GDPR reasons This search indexis going to letyou do that  or findspecific IP addressesor other specific data elements Finally  coming soonthere s going to be alsoTimeSeries joins and data types So we re going to supportjoining table by arbitrary timewindows  where wesample that data and you ll also be able tostore time periods  as opposedto just a point in time So here at GCP we re puttingour money where our mouth isand we re standingbehind these featuresin our own logs applications We are using these features asa base for our log analyticsapplication from CloudLogging  and so now I mgoing to hand youover to Charlesto tell you more about that CHARLES BAER  Thanks Chris My name is Charles Baer I m a product managerin Cloud Logging and Chris and I have beenpartnering on Log Analytics Log Analytics combines thepurpose built logging platformprovided by Cloud Loggingwith BigQuery s powerful dataanalytics platform And we build log analyticsand Cloud Loggingfor three reasons First  ad hoc log analysis To empower users to answersophisticated questionswhere ad hoc grouping candeliver useful insights For log aggregation   toanswer the types of questionsthat require log aggregationover large data volumes And to build aplatform upon whichboth our users and our productcan grow for the future Now Cloud Logging is uniquelypositioned for log analytics Cloud Logging provides anintegrated logging solution both for services thatare running on top of GCP as well as logs that aregenerated externally This includes log collection routing  storage  and analysis All the things you needin a logging platform So in Cloud Logging  logcollection is really easy Logs for serviceson Google Cloudare automaticallycollected  and it seasy to collect otherinfrastructure and applicationlogs if you need to Getting the right logs is easy It s a first critical step forthe logging analytics  as well The second thingis the logs router It provides powerfulrouting capabilities The built in routing routeslogs to Cloud Logging  other GCPservices like CloudStorage  and evento external third partytools with integrationthrough Pub Sub Now for log analyticsthe platform routeslogs to BigQuery The storage platform providesscalable log managementcapabilities These capabilitiesprovide controlsaround what logs are stored where they re stored how long they re stored  andwho can access them to meetyour compliance requirements For logging analysis  thismanages the storage and accessin BigQuery Cloud Logging already providesseveral tools to analyzelogs  each withspecial purposes The Logs Explorer is a tooloptimized for troubleshootinguse cases with greatfeatures like log streaming a log Resource Explorer  anda histogram for visualization You have errorreporting  which helpsusers react to criticalapplication errorsthrough automated errorgrouping and notifications And through log based metricsdashboards and alerting which all provide otherways to understand and makeyour logs actionable foroperational use cases Our new Log Analyticsfeature expands this toolsetto include ad hoc loganalysis capabilities Taken together  Cloud Loggingprovides a powerful platformspecifically designedfor working with logs So what is LogAnalytics exactly Log analytics extendsthe capabilitiesprovided by Cloud Loggingin several important ways First  log analyticsautomaticallystores logs in BigQuery witha simple configuration setup With a checkbox enable Log Analyticswhen you re creatinga log bucket you can use CloudLogging s pipelineto make it easy to storethose logs  which you can thenanalyze in Log Analytics Second  we re excitedfor our new purpose builtlog analytics UI  which providesaccess to the logs using SQL SQL is a well known  widelyuse domain specific language has powerful aggregationcapabilities and of course it swidely used in BigQuery When you need to group by or sum  or find a ratio Log Analytics supports thesame standard SQL functionsprovided by BigQuery Now the Log Analytics UIprovides an optimized resultsexperience designedspecifically for workingwith unstructured log data And you ll see more of thisexperience in the demo later Third  logs included inLog Analytics are madeavailable directly in BigQuery If users need to join logswith another data source they can run the samequery in Log AnalyticsUI and the BigQuery UI andjoin that data with dataalready in BigQuery So by making this log dataavailable where users need it Log Analytics can helpremove data silos So how can you benefit fromthese new capabilities Log Analytics canhelp reduce the needto send logs to somany different systemsto address theseparate use cases I ll cover three commonuse cases as examples First is for the DevOps team Resolving issues quickly iscritical to maintaining systemreliability  especiallywith user facing services Log analytics helps here byhelping users more quickly findpatterns by correlatingIDs across servicesso that you can reduceoutages more quickly Faster troubleshootingcan also help improvethe speed ofsoftware developmentby reducing time spent ondebugging and thereforefreeing up developer time The second is security Investigations area critical partof understanding andremediating security threats Log Analytics can helphere by finding logsacross large time ranges  whichoften means large data volumes The third is IT operations Managing a fleet ofdifferent servicesis a core part of IToperations  and it simportant to understandthe events and trendsacross the whole fleet Log Analytics helpshere by identifyingpatterns and the fleetperformance over time Now there are manyways that Log Analyticscan help enable businessdecisions  including analyzingbusiness data reported in logs And by unlockingvaluable insights Log Analytics bringsnew value to your logs And now  since there s no betterway to show off a new feature let s see a demo In this demo  I ll show youthe new log analytics UI an example ofcombining data storedin Log Analytics withother data in BigQuery I ll highlight the capabilitiesoffered by Log Analyticsto address two use cases One is the DevOpstroubleshooting use case and the other is asecurity use case For the demo I ve instancesof the online boutique which is an open sourcemicroservices demo app It s deployed to twoGK clusters  whichare generating logs which are ingestedinto Cloud Logging using theout of the box configuration I ve set up a loggingsink to capture the logsand send them to Log AnalyticsEnable bucket in Cloud Logging With a click of acheckbox  logs areavailable both in LogAnalytics and Cloud Loggingand in BigQuery First let s look atthe Log AnalyticsUI  which is a new UI CloudLogging optimized for viewinglogging data In the Log Analytics UI use the same standard SQLthat is available in BigQuery SQL queries are usedto view the log dataand can range fromsimple or complex There s a QueryBuilder here whereyou can enter your SQLin a results viewerto view the query results To start  let s take a lookat the completed Kuberneteslogs for the front endservice over the past hour Once the query is validI can run the query Now the LogAnalytics UI providestwo specific optimizations thathave been added to BigQueryand used by Log Analytics The first is thatJSON value function which allows you toextract JSON dataand compare it to strings  whichis really useful for querieslike this or when youwant to look at the stringresults of data stored in JSON And the second optimisationis the JSON dot notation Notice that the JSONpayload dot message fieldis actually usingthe dot notationto refer to a field insidethe JSON structure of a JSONpayload Both of these make it easierto work with JSON data The results are displayedusing a view optimizedfor working with log data You can see that theJSON payload is actuallystored as JSON data  whichmeans that any JSON structureddata can be stored in therewithout having to knowor specify the schema up front Log Analytics UI  theJSON data  is actuallydisplayed using JSON INAUDIBLE  formatto make it even easier to use Now by looking atsome of these logs I notice that there is avalue inside these logs  whichis the responsetook milliseconds which is the time therequest took to complete So one of thethings we can do islook at the min  max and average valuesof that over time These kind of queries can bevery useful in troubleshooting So let s take a look atwhat this would look like There we go OK So what we re doing here iswe re extracting this field and we re casting it as aninteger so we can do the sum We re also taking the hour And then what we redoing is we regrouping  looking at theminimum  the maximum and the average value But what we notice bylooking at the results hereis that around hour14  both the maximumas well as the average timeincreased substantially So this gives me great insightduring troubleshooting and it can help memore quickly identifythe signals  which can help meresolve issues more quickly Well  Log Analyticsprovides the flexibilityto look at individuallogs  as you see and run complexaggregations  s whichcan help surface criticalinsights for the DevOps teamwhen troubleshooting an issue Now let s take a look atthe security use case Again  staying withthe same scenario the app deployed to GKE This time let s lookat the security relatedlogs for applicationand infrastructure We can do that by usinga query like this The main differencehere is that we relooking at the protopayload field hereand we re lookingfor audit logs So why don t we run the query Here s what the audit logslook like in Log Analytics You can see that they are therecord type within BigQuery and you can see that there s adetailed structure of the auditlog within theproto payload field There s a lot ofinformation that sstored within an audit log and it s actually displayedin the UI using theJSON  INAUDIBLE  formatto make it easier to work with All of the fields that areactually stored as a recordtype are also displayedusing the JSON data typeto make it easier to work with Again  what s more interestingis to look at the aggregations So let s take alook at the auditlogs grouped by theprincipal and the IP addressfields  which arethe users as wellas the IP address from whichthose actions originated Great So now we can see both emailaddress and the IP addressand the number of calls thathave originated from both Now for security usecases  sometimes it suseful to search for a valuein a log without knowing whichspecific field The newly added searchfunction in BigQueryallows you to do just that The function searches across anentire table or set of columnsto find a value To demonstrate  let s pick an IPaddress and find all the auditlogs associated with the IPacross any field in the table And we can do that byusing the search function specifying the table  as wellas the IP address defined So by using thesearch function I vebeen able to find all of therecords that are associatedor include thisparticular IP address This is extraordinarily usefulfor unstructured data that isstored in JSON in other fields For example  it s not limitedto IP address or just for auditlogs It can be used generallyacross any of the fieldswithin BigQuery Now one of the powerfulfeatures of Log Analyticsis it also provides youwith access to the same logdata in BigQuery While Log Analytics providesan optimized UI and onethat evolves to addmore optimizations having access tolog data in BigQuerymeans that you canjoin the log datawith other data and BigQuery By clicking the Run inBigQuery link in Log Analytics the same query isloaded in BigQuery And running that willreturn the same resultswith the standardBigQuery formatting You ll notice it sthe same data However  the resultsappear differentlyfor the proto INAUDIBLE  format Now in BigQuery  we cancombine that IP address datathat we looked up inthe previous examplewith other data As an example  I ll combine theIP address with a public dataset and use thefunctions in BigQueryto produce a listof all the countriesand the number of logs for each We can do that with this query Here we have theSelect statement which will look at all of theaudit logs over the past 30days and select the IP addressesfrom them as well as the count And here we ll join thatdata with a public data set as well as use theBigQuery IP functionsto be able to get thecountry associatedwith each of those IPs So by combiningthe audit log datawith the geolocationpublic data set I ve been able to gain insightinto the requests comingfrom different countries Most of the requests havecome from the United States I do see that other requestshave originated from placesoutside the United States This combination canprovide powerful insights So to recap  Log Analyticsand Cloud Loggingprovides a powerful wayto analyze your logsbased on the power of BigQuery The Log Analytics UIprovides an optimized wayto view and work with logdata  and the log datastored in Log Analytics is madeavailable directly in BigQueryfor joining with other data While we re excitedabout Log Analytics we re already hard at workmaking it both easier to useand more powerful To make it easier touse  we re workingon adding a new visualizationresult type via charts so that users can more easilyunderstand their query results We re also working to simplifythe user query experience We know that not everybodyis a SQL expert  whichis why we re workingto make it easierto gain the benefitsof Logging Analyticswithout becoming a SQL expert We re also working to enableLog Analytics by default This will make iteasier for any usersto access the advancedlogging capabilities Now to make Log Analyticsan even more powerful tool we re working onlogging insights Logging insights can surfacedata that you would otherwisehave to search to find By analyzing dataproactively  Logging Analyticscan help uncoverpowerful insights such as proactiverecommendations We re also workingto expand the scopeto include otheroperational data By combining logs data withother operational data such as trace data  youcan gain even more contextfor a more rich analysis We re really excited aboutlaunching the Log Analyticsfeature today  and we reexcited to see the powerful waysthat you ll use thesenew capabilities So on behalf of the whole CloudLogging and BigQuery teams thank you for watching To learn more aboutLogging Analyticsand Cloud Loggingplease visit our Docsand sign up for thepreview to get started SCOTT ELLIS  Hi Welcome to ourtalk on how to takecharge of your sensitive data My name is Scott Ellis and I m a Product Manageron Google Cloud s DataSecurity and Privacy team Now  data powers yourbusiness  and datais one of your greatest assets But it can also be oneof your biggest risks And today we re goingto talk about how youcan take charge of your data And we re going tocover topics like howyou can discover andclassify your sensitive data We feel that understandingyour sensitive datais the first stepin protecting it And once you have thatkind of visibility we re going to talk abouthow you can manage your datarisk using thingslike security policy confidential processing and data obfuscation And ultimately  we want tobe able to do this at scaleso that you can focuson your businessand  again  reallyfocus on the outcomes Now  to do this automation is key And again  if you wantto know your data and if want to understand yoursensitive data so that youcan better protect it  on demandscans and manual reviewcan help However  the challenge isthat your data is growingas your business is growing And that means your datafootprint is growing and it s morechallenging to manage it And so you really need a wayto automatically discoverand classify your data And today we re goingto talk about howyou can get automaticdiscovery  automatic inspection automatic classification  andautomatic data profiling  really  automatic DLP And this is nowavailable for BigQuery and it s powered byour Cloud DLP platform And what that meansis that you canget an understandingof your data riskacross your entireBigQuery footprint And that meansyou can understandwhere you have  maybe low risk data  whichcould mean that there s noevidence of sensitive dataor that it s got extraprotections in place or where you havemoderate or high risk This could be where you haveevidence of sensitive data like PII  or evidenceof unstructured dataor free text that mightbe inside of your tables or where you have highlysensitive data  somethinglike PII  like credit cardnumbers  or financial data And what automatic DLP willdo is generate rich insightsfor each table andcolumn across your org And that s reallyimportant  because youwant to get that visibilityacross all of your tables  allof your columns so you can reallyunderstand your data risk Now  here s an exampleof a column profilefor a table where we re seeingdifferent types of metricsthat indicate the type ofrisk  the type of sensitivity And it includes theDLP s predicted infoType And this gives anindication in this tablethat we see evidence ofa credit card number an email address  and acouple of columns thathave evidence of a person name This kind of datatype can reallyhelp you understand yourdata or take the next stepto protecting it Now  this automaticvisibility with DLPreally means that you getcontinuous monitoring That means that tables arepicked up automatically So as you re creatingnew projects new data sets  and new tablesin BigQuery  Automatic DLPwill pick them up automaticallyand profile them for you And it s designed as afully managed serviceto have low overhead What that means isthat there s no jobsto manage or orchestrate  andyou can enable this directlyin the Cloud Console It s also been built withdata residency in mind And again  if you re turningon across your entire org you may have data indifferent regions different geographic regions And DLP will inspect thatdata in the geographic regionthat you have set foryour BigQuery tables It will alsogenerate the profilesand store them in thatsame geographic region And that way  you can ensurethat your data stays whereyou want it  even when it sbeing inspected and profiledby DLP And lastly  thisis Google driven And what that meansis that we figure outhow to inspect andprofile your data so you can focus again  on the outcomes Great Now  let s see this in action And now let s see a demoof this automatic DLP And for this  let s meet Angie Angie is a datacompliance officerfor a large onlineretailer that servescustomers across the globe Angie oversees how data iscollected  shared  and usedacross all departments Angie s company isgrowing  and their datais growing with them They ve chosen to use BigQueryas their data warehouseto power most of theiranalytics and business needs And usage of BigQuery is spreadacross several departmentsand project teams And again  these teams arecreating new projects  new datasets  new tables constantlyas part of their business And Angie wantsto know  how can Igain visibility into thesensitive data across my org And for this  we re goingto use our Automatic DLP What this means is you can now enableCloud DLP across your org andyour entire BigQuery footprint So what that means isthat you can turn DLPon across yourorganization  as we reseeing here  across specificorganization folders  maybe these representdepartments or different partsof your organization  or individual projects Now  BigQuery data lives insidetables and data sets thatreside inside these projects So depending on whereyou turn this on you ll get coverageacross all that data When this is enabled  CloudDLP will automaticallypick up new data pick up existing data profile it  and giveyou those insightsinto your sensitive data Now  enabling this is easyand can be done completelyin the Cloud Console UI For this  at theorg level  Angiewould navigate to Securityand then Data Loss Preventionand then click on theScan Org link here Now  let s go ahead andsee what that looks like I m going to switch overhere to my test project We re going to clickon Scan Organization Now  we can configurethis in one click and takecare of everything for you But let s go ahead andwalk through the customconfiguration  just so you cansee the options that you have The first optionyou have here isto choose whether you want toscan your entire organization or if you want tobrowse and selectindividual folders to scan For now  we ll just scanour entire organization Now  this service isbuilt with Cloud Data LossPrevention  or Cloud DLP So you can use your existinginspection templates or you can create a new one But this is powered by thesame infoType detectorsthat we have for Cloud DLP So you can select fromour built in detectorsor define customones of your own Additionally  you canselect the third option which will run a set of ourmost popular default detectors And just simplyselect that option And next  you cansee the permissionsneeded to run this scan Now  we re very transparentabout the permissionsthat are needed here We can also createthose for you or you can set those permissionsindividually   again  justdepending on what is bestfor your organization And next  let s talk a littlebit about data residency So data residency iscrucial to your data And we ve built this CloudDLP from the ground upto support data residency And that holds true forthis automatic solutionfor BigQuery What that means isthat when you havedata in a certaingeographic region configured in BigQuery to bein a certain geographic region DLP will scan datain that same region So the process ofscanning your datahappens in the samegeographic region Additionally  theprofile output willbe stored in the same geographicregion as your BigQuery data And again  this allows you tomaintain that data residency Additionally  wegive you the controlto decide where to storethe actual configurationdata  which is whatwe re configuring here Again  we want to makesure you have good insightand control of your datato know where and how it sbeing processed Now  if I clickCreate here  thisis going to enablethis new servicefor my entire organization which is great However  I ve already done that So I m going to goahead and click Cancel And let s take a look atwhat the results look like Now  in this particularorganization I only have thesesix projects here And each of these projectshas BigQuery data  data sets and tables inside them Now  here at theProject view  I mjust getting a high levelview of the tablesand a summary of the riskand sensitivity in them Let s go ahead and clickon one of these projects And now what I m seeing isevery data set and table thatis in that project and theassociated metrics at the tablelevel  including things likeour data risk assessments whether this data is public what kind of encryption it has row count And I can also clickon one of these tablesto go even further I m going to clickon this table and this will take me intothe column level profile Here we see detailed metricsfor every column in that table including DLP s predictedinfo type  which again is   if we canfigure out this columnlooks like aparticular data type we actually label thatcolumn here with that type So here we re seeing a columnwith credit card numbers  emailaddresses  and person name It looks like we have afirst name and a last name as well as metricsaround that for data risksensitivity and someadditional metricsaround whether ornot this column lookslike it has free text  orit s unstructured  maybea comment field  aswell as uniquenessand null percent estimates Now  these are important because imagine this columnof credit card numbers  we also see that there isa uniqueness score of 1 And that s a highuniqueness score which indicates that this columnlikely contains unique creditcard numbers in everyrow  which wouldbe different than a lowuniqueness score  wheremaybe that samevalue was repeatedover and over multiple times We also see that with anestimated null percent of 0  That means that this columnlooks like it s pretty wellpopulated  and there s not a lotof null or sparsely populateddata in these columns But again  this is thekind of rich insightsthat you get with thecolumn level profile as well as the table  andproject level profiles And all of this  again will be continuouslyrunning and generatingas new tables arecreated across your org Great Let s go back to Angie Now  Angie needed anautomated solution And Angie chose to useCloud DLP s new serviceto automatically discover inspect  and profiletheir BigQuery tables forsensitive data like PII And this automatic DLP servicewill pick up new projects data sets  andtables automatically And the outcome is that Angie steam now has clear visibilityinto their sensitivedata and canmake more informed decisionsabout how it s protected used  processed  and shared And that concludes our demoof the new automatic DLPfor BigQuery Great We ve now just shown you howyou can get org wide visibilitywith the new automatic DLP And again  this can giveyou clear visibilityinto your sensitivedata so that you canmake more informed decisions And that s nowavailable for BigQuery Now that you have that clearvisibility into your data let s figure out some waysto manage your data riskand reduce your data risk So first  how can I enablemore fine grained accesscontrol and protection So now that you know whereyour sensitive data is maybe you want to go in andemploy column level accesspolicy And using BigQuerypolicy tags  youcan enable column levelaccess enforcement which can help you strikea balance between accessand security So an example policyis shown here And these policies havea taxonomy or a hierarchyto them What that means is that we canput an element like credit cardnumber underneath or insideof a higher level policylike financial And that allows you to grantaccess and manage your policyat that higherlevel  while enablingyou to tag the data at the lowerlevel  at credit card number Let s see this in action So here s an examplehere where we reseeing a table where we havetagged four of the columnsin here with a tag for personname  email address  and creditcard number And we have turnedon enforcementof a policy usingBigQuery policy tags so that users can only accessthose sensitive elementsif they have theright permission And what that means is  ifI have access to this table but I don t have access tothose particular columns I ll still be able to querythe table and  in this case maybe run analysison the payment typeand the transaction amount But I won t be able toaccess the sensitive elementslike the PII here thatwe see unless I havethat additional access control And again that sthe way you can findthat balance betweenaccess and securityand  overall  reduce andmanage some of your data risk Another technique is to go inand use obfuscation or masking And this is something thatyou can do with Cloud DLP and it s a technique wecall de identification Now  de identificationincludes a set of transformslike redaction masking  tokenization format preserving encryption date shifting  bucketing and more And these transformationsare designedto actually manipulateand transform your datain order to reduce the risk So for example  in thisbefore and after animation thatwe re seeing  we re seeing thesetechniques applied to a table  in this case  a tablethat has four columns And we re applyingtransformationsacross the entire column inthese first three columns Again  our goal here is toreduce the risk of the data So we are manipulatingand transformingthe sensitive informationand replacing itwith somethingthat is obfuscated Now  in that fourthcolumn  we recombining the power ofDLP s inspection capabilitywith the masking sothat we can actuallygo into these unstructuredcomment fields find the sensitiveelement  and only maskthat sensitive element And this is somethingthat you cando because we vecombined that inspectioncapability with the data maskingcapability all in one platform Otherwise  you d haveto potentially redactthe entire Comment Field column But instead  youcan actually go inand redact only the sensitiveinformation in line Now  another questionis  how can Iimprove protectionand process my datain a confidential manner Again  now thatyou have visibilityinto where yoursensitive data exists you can useconfidential processingto process that more securely An example of that wouldbe Confidential Dataproc which offers encryptionin use  which can easilyadd security protections to yourmost sensitive data workloads Here s an example where we areturning on confidential computewith a simple command lineflag as we create and launchour Dataproc pipeline Now  ConfidentialDataproc is now in GA Now  Dataproc is a fullymanaged and highly scalableservice for running thingslike Apache Spark  Flink and Presto And Confidential Dataproc addsthat data encryption in use again  to protect yourmost sensitive data And it does this by usingconfidential VMs to providethat in line memory encryption And this is powered by AMD ssecure encrypted virtualizationprocessors And this brings us to the endof our talk on how to takecharge of your sensitive data And we ll leave you with a fewkey takeaways and action items First  automatic DLP isnow available for BigQueryand can give you insightinto your entire BigQueryfootprint across your org You can get started in theCloud Console by navigatingto Security and thenData Loss Prevention or you can readour documentationat cloud google com dlp You can also check outobfuscation and masking or what we callde identification also at cloud google com dlp And you can read more aboutconfidential processing which is availablewith VMs  GKE and now in GA for Dataproc atcloud google com confidentialcomputing Thank you again forjoining us today and we re happy to bepart of your journeyas you take care ofyour sensitive data Thank you MINH NGUYEN  Hi And welcome to  The futureof application developmentwith cloud databases  My name is Minh And I m a product managerfor Cloud Firestore Join with me today and Natan a software engineer from Dott Today s session will be focusedon resolving one key question  why is it so challenging tobuild and scale applications And more importantly what is Googledoing to make applicationdevelopment easier To answer thisquestion  we ll beginby reviewing two popularapplication architectures Choosing an architecturehas a large impacton how an application isbuilt and how it scales In the second partof the session you ll hear acustomer testimonialfrom Dott describing howthoughtful architecturaldecisions have impactedtheir business Finally  we ll round outthe session with new productannouncements and resourcesto learn more about Firestore Let s get started Imagine for a moment thatyou re writing a new applicationand decide to use amore traditional n tierarchitecture Today s example can apply toany general purpose application but to ground thediscussion let sassume it s an e commerce app At the heart of theapplication is a database The database isused to store data In front of thedatabase are typicallya set of microservices thatmanage your application sbusiness logic Let s assume that you havethree separate microservices  a service for yourproduct catalog a service for your productinventory  and lastly a servicefor your shopping carts As usage for your microservicesgrow and declines you ll need to loadbalance trafficwhile horizontally scalingand compacting capacity In order to secure accessto these microservices you ll need to addan API gateway Finally  to enablebroad distributionof your application  you llneed to develop clients  the web  Android  and even iOS Furthermore  you want to makesure that your users alwayshave the latest data To access your applicationwhen they re offline you ll need to addsupport for web socketsand an offline caching system As you can see  there s a lotof technical complexity here Unfortunatelytechnical complexitycan be the enemy ofbusiness innovation Let s examine the downstreamimpact this architecture has First  operationalizingall the servicesthat support an applicationcan be intensive Second  each layerof the architecturetypically representsspecialized teams which means complexdependency planningand therefore potentialrisks for project delay Third  sourcing talent foreach layer of the architecturecan be difficult andexpensive  especiallyin competitive hiringmarkets like today Let s figure out howGoogle is solvingfor each of thesechallenges  startingwith the operational challenge Google offersmanagement servicesfor each layer of the stack With Cloud SQL  forexample  we will assist youin operationalizing popularrelational databasessuch as PostgreSQL  MySQL or even Microsoft SQL With Kubernetes Engine we will assist youin operationalizingyour microservices With Apigee  we will assist youin operationalizing your APIgateway So the combination of thesemanagement services from Googlewill assist you inoperationalizingeach layer in the stackwhile still enablinghigh customization Alternatively  if you reinterested in takingthe surplus route you can use Cloud Runto run your microservices inFirestore as your database Choosing to go theserverless routeallows you to delegateoperational complexityto Google  freeingup your team to focuson product development OK We ve addressedoperational complexity but what about projectsor hiring complexity Let s examine how a deeperinvestment in Firestorecan address theremaining challenges Firestore is a serverlessdocument databasethat can also act as acomplete backend as a service Firebase Auth is aservice that allowsyou to delegate yourapplications auth and usermanagement to Google Firestore and Firebase Authare seamlessly integrated Using a combinationof these two servicesallows customers tofocus on UI development delegating secure auth andbackend service managementto Google Furthermore  you ll getbuilt in functionalityfor real time synchronization connection managementbetween clientsand your database and offline cachingusing the Firestore SDKs So by using Firestore asa backend of the service you can focus on UI development And UI teams are empoweredto make decisions as they no longer are dependenton changes from backend teams With universalapplication frameworkslike Google Flutteror Facebook React you can now writenative applicationsusing a single codebase and deploy themacross multiple client platformssuch as the web  Android and iOS Firestore fully supportsthese universal app frameworkswith an easy to useset of plain SDKs As a result  with Firestoreand universal app frameworksyou can focus on hiringfor Flutter or React skillsfrom a very largepool of developers The result of theseinvestments enablesyou to focus onbusiness innovationthrough a single autonomousteam and code base There are many factors that gointo application development And at Google  we re investingin a complete ecosystemfor app development This includes  one  coreUI frameworks and operatingsystems to buildapplications  two in app services from Firebaseto manage aspects like auth and three  backendservices from Google Cloudto manage aspects likeback office analytics usingBigQuery Firestore brings togetherthese portfolio investmentsin an intelligent way byenabling seamless integrationinto each of these portfolios Frequently applications relyon third party services and it s important that theseservices are also seamlesslyintegrated with Firestore One popular integrationrequest we ve receivedhas been an integrationwith Elastic Today  we re excited toannounce  in partnershipwith Elastic  anew extension thatstreams changes froma Firestore databaseto an Elastic appsearch instance You can install this extensionand many other third partyextensions from the Firebaseextensions directory So in summary  Firestoreallows customersto focus on business valueand product differentiation  INAUDIBLE  document databasethat can also act as a backendto service With over 250 000 monthlyactive developers you can have confidencethat the solutionis backed by a largecommunity of developers Furthermore  withFirestore applications nowsupporting 750 million monthlyactive end users using FirebaseAuth  you can be confidentthat Firestore willscale to your business s needs We ve covered thetraditional n tierin Firestore architectures Let s review thestrengths of each The traditional n tieroffers a high customizabilityand is compatible withpopular open source frameworksand libraries However  as we explainedin this session this normally comes at thetrade off of technical project  and hiring complexity Pricing for thissolution is oftendriven by a capacitymodel  whichmeans that you have to configurehow capacity is scaled and you re billed forcapacity even whenit s not fully utilized Firestore  on the other hand provides a fully integrated Google based solutionthat significantlyreduces technical  operational project  and hiring complexity Customers who chooseFirestore have the flexibilityto use it as abackend of a service in an n tier architecture  ormaybe a hybrid of the both Since it s serverless pricing for Firestoreis based on the actualoperations conducted  storageused  or network egress And with that I m excitedto introduce Natanfrom Dott  who will sharehow Dott uses Firestore  MUSIC PLAYING NATAN SAGOL  Good morning I m Natan  one ofthe engineers whojoined Dott at the verybeginning of our journey We are a micromobility operatorwith over 50 000 e scootersand e bikes across45 cities in Europe We are active in nine countries in capitals like London  Paris and Warsaw  andmany more cities I have been fortunateto participatein growing our companyfrom the very first scooterto tens of thousandswe operate today to see our engineeringorganization expand to over 30people  looking forward towelcoming many morebefore the year s end  to enable millionsto take clean rights At Dott  we deeply careabout the environment and we try to reduce our carbonfootprint as much as possible While Google Cloudis carbon neutral it s even better not toconsume any kind of resourcesin the first place That s why wechoose technologieswhich rapidly up and downscaleto precisely match our needs Firestore is one ofthese technologies It has been a coreto our solutionsfrom the very beginning We started on Firestore byconnecting our vehicles  whichfrequently send informationabout their locationand relevant telemetry We quickly moved touser facing features using Firestore as a backend Connecting to itdirectly from our appsallowed us to workefficiently and independently We were a small team back then Removing the need forany backend developmentallowed us to shiftfeatures quickly In a single day withreal time synchronization we were able to implementa vibrant live mapof available scooters Over time  we addedmany functionalities from basic ones likestarting and endinga trip to more complex  suchas payment processing  whereaccuracy is critical For thesefunctionalities  Firestoreprovided us not only with anability to develop rapidly but with an ability todevelop with confidence allowing us toensure correctnesswith the use of transactions The world changes all the time So does our business More sophisticatedfunctionalitiesrequire specialized services We have developeda lot of these and each time Firestore hasproven to be a good fit Some of these services replaceddirectories from our appswhen we needed complexitythat was easier to managewith a dedicated backend But we are gratefulwe didn t haveto implement all ofthis back in the dayswhen there werejust a few of us Some of the recentchanges we all experiencedwere challenging for individualsand businesses alike Shelter in placeorders  while necessary affected every one of us andaffected a lot of companies including Dott Our choice to utilize Firestoreand our serverless productshelped us navigateshelter in place With the lower usage came alower bill with the expectedone to one correlation Nowadays many lean more towardsindividual transportation such as with the use ofe scooters and e bikesdue to health safety reasons At Dott  we see many moreriders and trips in comparisonto pre COVID numbers  withutilization of our fleet75  higher in somecities such as Brussels Scalability ofFirestore enabled usto support this demandwithout requiringany changes on our side As regard to ourinfrastructure  we simplydon t have to worrywhat happens next Firestore willadapt to our needsand support us in our journey Now that you know a bitmore about our history it s a good time toshare with you whatI think about it in retrospect While at the beginning we wereconsidering other databases  relational  graph  anddocument oriented  today I know that wecouldn t have made a betterchoice than Firestore That s why it s such apleasure for me to be heretoday and give this testimony Thank you all And thank you  theFirestore team Back to you  Minh MINH NGUYEN  Thank you  Natan for sharing Dott s journey I m excited about Dott simpact on the futureof transportation We appreciate the opportunityto partner with Dott Next  I ll be sharingexciting product announcementsfor Firestore Firestore is committed tothe security and privacyof our customers To demonstrate thatcommitment  I mexcited to announcefour new features With Data Access Audit Logs privileged administratorswill be able to auditall operations conductedon a database With custom IAM customers will beable to alignFirestore permissionsto their organization sIAM roles You can get started with DataAccess Audit Logs and customIAM today With VPC ServiceControls  customerswill be able to reduceexfiltration risk using networklevel security controls And with App Check customers willbe able to ensure only signedand pre authorized clientapplications are allowed tomake requests to Firestore Both the previews of VPCService Controls and App Checkwill be rolling out in Q4 Firestore s developerexperience truly sets it apart and we are continuallyinvesting in this area With Key Visualizer customers willbe able to quicklyidentify performance issueswith a visualperformance profiling With the latestversion of our Web SDKwe have added treeshaking  whichenables the reduction of theSDK footprint by as much as 80  That means faster loadtimes for your application Both Key Visualizerand the new Web SDKare generally available today We ll also be graduating withUnity and C   SDKs to generalavailability in Q4 Both of these SDKs arefocused on making it easierto develop mobilegames using Firestore Lastly  I m excited toannounce the graduationof Cloud Functions Triggeringto general availability Cloud FunctionsTriggering enablescustomers to execute custombusiness logic in responseto changes to aFirestore document This session iscoming to an end but I hope this was just thebeginning of a shared journey If you want to take a moment totry Firestore out for yourself you can take a peekat our quick startguides  YouTube  INAUDIBLE series  and Qwiklabs If you have additionalquestions about using Firestore we encourage you to post thosequestions on Stack Overflow My name was Minh And I hope that youenjoyed today s session If you did  please takea moment to fill outthe follow up survey Bye for now JASON JOEL  Hi  welcome to Next My name is Jason Joel and I m a Product Managerwith Google Cloud I m presenting here todaywith my peer  Sai Gopalan And in this session we will deepdive into the tools and servicesto accelerate your migrationto Google Cloud There are severalmotivators thatlead customers tomigrate to the cloudand modernize theirapplications and infrastructure Many customers begin bywanting to use the cloudto increase thespeed of innovationand address customerrequirements faster But often  theproject is initiallyjustified by animpending financial need like an upcoming hardwarerefresh  a significant businessevent  such as an acquisition or an upcoming deadline like an expiringdata center contract But it turns out migratingand modernizationto the cloud is hard Enterprise migrationprojects oftenface delays and go over budget But this is typicallybecause customers do notunderstand the applicationsthat they are trying to migrate They do not havea good plan thatincludes movingall of their data and they generallyunderestimate the scale At Google  we have a provenfour step methodologyto help our customers andmeet them where they are and accelerate their path tothe Google Cloud platform Actually  it s notfour linear steps but this is moreof a process thatwill be followed iterativelythroughout the migrationjourney The first step isthe assessment phase And for this assessmentphase  Googlehas a tool called StratoZone And StratoZone enablesyou to do five thingsas part of your assessment First and foremost  it helpsyou discover what workloads applications  andvirtual machines that youhave that you want tomigrate to the Cloud and build an inventory as aplace to start  a baseline The second thing itdoes is it evaluateseach of your workloads forits suitability for the cloud gives you a CloudFit Score  and helpsyou identify whatremediations you may needto take in order to move it The third part of theStratoZone processis optimizing yourconsumption planand identifying whereare the best placesto move a particularapplication to Google Google offers many services and it s not always obviouswhere the bestlocation is to moveand would give you thebiggest impact for your money The fourth step isactually providing youwith an ROI  a real plan  areturn on your investment paperthat you can sharewith your enterprise and share with your executivesand your stakeholdersto get an idea of what thereturn on their investmentwill be  and thebenefits they willgain from moving to the cloud And the last step of theStratoZone consumption planis developing this toprovide you with insightsinto what you shoulddo first  second and third in your journey Let s look at some of the keyfeatures of the StratoZone DataCollector in itself It s agentless  andit does not requireyou to install any software ineach of your virtual machinesor on your physical machines All the informationyou send is encryptedin transit and at rest It s very lightweight  soit doesn t put any burdenon your environment And in fact  the datathat it transfersis roughly less than400 kilobytes in size You have the optionto anonymize any datathat you think mightbe sensitive  includingIP addresses and machine names And it s overall not intrusive You can control thescope of the discoverythat you run from asingle application to a single data center  to amulti data center or migration One of my peers  INAUDIBLE    will nowgive you a short demoon how StratoZone works SPEAKER 2  StratoZoneassessmentscan scale globally withmultiple data collectionsfeeding data from variouscustomer locations The assessment relies oninventory and telemetry data and is conducted withinthe StratoZone portal All data is encryptedin transit and at rest No actual businessconfidential datais collected  such as dataclassified as PCI or HIPAA Once the assessmentcollection has begun you will be able to seethe results populatewithin the StratoZone portal Results include overallutilization levels age of the machines  detailsabout operating systems and cloud migrationcompatibility scoring among others You will also be able toreview detailed inventoryin each machine s detail such as its name  IP address and configuration specs You will be able tofurther drill downinto utilization installed software and running processes Network connectivitymaps will beavailable for any point to pointdetected connections They can be viewedin graphical form as well as in the table form which represents detailsabout source and targetIPs  ports  and processes such as SQL Server in this case responsible for theconnection socket Detailed pricingviews are availablefor each assessed location application group  or migrationgroup Within the groups  machinesare evaluated individuallybased on therecorded utilizationand user definedright sizing profiles Each machine scost breakdown canbe viewed in detail todetermine compute  storage and egress costs Pricing can be generated for avariety of different catalogs including GC predefined types custom shapes  sole tenancy and    GCV     Committedand sustained discountsare included in the calculation Custom discounting can alsobe defined by the user Pricing can also be tailoredto relevant GCP regions StratoZone also storesindustry benchmark costsfor operatingon premises data centers which can be used incomparisons to devise the mostsuitable pricing scenarios At any point in theassessment  a customizable deckcan be generated in thereporting dashboard It can be fully customized toinclude the relevant workloadgroups  optimizations  pricingcatalogs  and discounts This assessmentsummary can serveas a leave behind customerdeliverable and a proposal This deck will coverthe assessment approachand findings  along withpricing information Findings representedfor all assetsor broken down byuser configurable groups Solutions such asdatabases and app engineswill be identified  alongwith potential GCP targets TCO reports will be generated toprovide meaningful comparisons JASON JOEL  After completingthe assessment stage you enter the planning phase Google has a pricingmethodology thathelps make initial progressquickly  reduce risk  and stayon budget StratoZone can help you arrangemigration candidates in orderof priority  based on businessimpact and the migrationeffort that s required A large migrationwill have many waves with velocity increasingas the migrations progressand experience in theorganization is gained By categorizing your workloadsin the waves based on these twoparameters  you can iterate learn from those iterations and then optimize your process With StratoZone sCloud Readiness Report you can see which assetsare high   medium  and low fit formoving to the Cloud This is based on characteristicssuch as capacity  performance age  operating system  andother machine based attributes Now Sai will explainthe tools that Googlehas to help you with theMigrate Phase of your journey SAI GOPALAN  Thank you  Jason Hi I m Sai  and I m a ProductManager with Google Cloud Let s jump right in When we look atmigrations  customershave specific paths tomigrating to Google Cloud Each migration pathhas an increasing levelof transformation and therefore change And we have a number of toolsthat can help you with that For example  in additionto moving your specializedworkloads or VMs to theGoogle Cloud bare metalSolution  or Google Cloud VMwareEngine  or Compute Engine or to GKE  you may want to moveyour unstructured archival datafrom an on premises filesystem to the cloud And we offer a fewoptions  includinga fully online storage transferservice and a dedicatedtransfer appliance that can beshipped to your data center Let s look at a fewtools in more detailin the subsequent section The first tool is Migratefor Compute Enginethat s delivered as a service Our latest version delivers aGoogle managed cloud servicethat enables simple friction less and large scale enterprisemigration of virtual machinesto Compute Engine withminimal downtime and risk Hundreds of VMs fast to start  thisis a replication basedtool  which meansthat it s predictable as well API driven and integratedinto your Google Cloud Consolefor ease of use  you can doyour end to end migrationthrough the GoogleCloud Console This service usesagentless replicationto copy data withoutmanual interventionand without VPN requirements It also enables you to launchnon disruptive validationsof your VMs prior to cutover You can rapidly migratea single applicationor execute a sprint with 100systems using Migration Groupswith confidence There are six easy steps You enable your API You deploy amigration connector on board andinitiate replication set your VM target  performyour test  and cut over Users can test theirworkloads pre cutoverby creating clonesof the workloadfrom the replicateddata in the cloudwithout disruptingthe source workload It also adapts the migratingworkloads automaticallyto run natively in the cloud And it includes networksetting and installationof cloud agents for seamlessintegration with Cloud Servicespost migration as well It supports migratingworkloads to different targetconfigurations on the fly For example  users cantest clone a workloadin project A while doing acutover of the same workloadto project B  Let usnow hear from Rackspaceand their experiencewith this tool MATT RICHINS  The tool thatwe love the most at Rackspaceis Google s Migratefor Compute Engine It s a super easy tool fortaking your environment that son prem in a VMware environment and migrating that to Google But what s amazing is thatit s not like some tools where it s like  hey here s your limitations and we can only do A orB  They can do everything It allows our customersto plan properly It let s our IT resourcesfocus on running their companyand helping their endusers  while we justuse the tool to do whatit s supposed to do which is migrate And then we re done It s very easy We love it I cannot say enough good thingsabout how well it works withthe technology environments SAI GOPALAN  What a great videofrom our partner  Rackspace Now  Migrate for ComputeEngine enables usersto group VMs based ontheir needs and scenarios and execute migrationoperations on a group level as well as a subsetof the group This flexibility allows for youto edit the migration detailsfor one or many ofthe VMs in the group and perform replication  testcloning  and cutover  as well Let us now look atthe lifecycle of a VM It starts withon boarding the VM  whichmeans telling the tool thatit s ready to start replication And once you start replication an initial sync happens All of this withoutdisruption to the source VMs This is followed by continuousreplication thereafter You can also performnon disruptive testingwith test cloning There is no disruptionto on premises VM And it s always a good ideato validate that a VM willrun correctly in the cloud Finally  you cutover to the Cloudduring a scheduledmaintenance window This is when the sourceVM is powered off a final sync of the VMdata is done to the Cloud and that s it Let us now look at aquick demo of this tool In this demo  I will walkthrough a quick overviewof Migrate for ComputeEngine that s deliveredas a Google Managed Service This means that youdo not have to managethe lifecycle of the tool I ll cover the sources the Migration tab and how you can performbulk operations with groups Here we see the maindashboard of Migratefor Compute Engine that scompletely integratedinto our Cloud console It shows an overview ofthe migrations  the events the sources  thegroups  and the target Let s go to the Sources tab In the Sources tab I can see that Ihave multiple on premdata centers thatact as sources ofvirtual machinesthat I want to migrate from For each one of thesesources  I can actuallysee that I have the VMinventory cached in the system This allows you to executeend to end migrationswithout everleaving the console Migrate for Compute Engineuses replication to migratethe virtual machine s dataover to the destination or the target As you can see over here there are virtual machinesthat are ready for replication There are virtualmachines that are active which means that there isperiodic replication that songoing in the background And there is a virtual machinethat s already been cutover Let s go into the Migration tab This is the place where I doall of my migration operations I can see the status ofall my migrating VMs I can go into theInfo panel and editthe details of mymigration virtual machine Here I ve selected w12core And what I can do over hereis I can edit the landingzone of the migrating VM And I can also select a VM andperform migration operations Actions include startingthe replication pausing the replication resuming the replication finalizing the replication It also includes testingthe virtual machinewith no impact to thesource VM in orderto validate that the workload inthe cloud is running correctly And finally  cutting overwith minimal downtime Many customers also wantto migrate virtual machinesin bulk The Group featureallows you to do that Here I see that I have actuallygrouped three virtual machinesin a group called myapp 1 Groups enable you toexecute operations in bulk such as bulk editing ofmigration details  testclone  and cutover Here I can see that I can selectall my VMs or a subset of them and perform bulk editingof migration detailson all of them I can also do testcloning and cutoverfor all of these at onego  or a subset of these This brings us to the endof a quick overview demo Now many customers want tomigrate their VMware workloadsto a native vSphere environmentrunning in the cloudto increase theiragility and reduce costs Google Cloud VMwareEngine solves this problemand gives you a fast forwardto your journey to the cloud It is one of the fastestways to migrate your VMwareapplications to the cloud And VMware HCX is thetool that enables this It s provided as partof VMware Engine and it abstracts theunderlying infrastructureand offers manyoptions  includinghot  cold  and warmmigration options in bulk Now  many customers also wantto fast forward to modernizingtheir applications With Migrate for Anthos you can transform your VMsto containers thatis managed by Anthos With migrate for GKE  youcan transform your VMsto containers that s managedby GKE in either Standard Mode or Autopilot Mode Finally  you cantransform your VMsto containers that smanaged by Cloud Runso you can get started with theserver less architecture rightoff the bat Once more  this happens ina simple six step process You qualify a VM  configurethe processing clusterto examine the source VM and create a migration planthat you then reviewand customize Then you use theprocessing clusterto simply executethe migration plan and generate yourdeployment YAMLs your container images your data volumes You verify that the workloadis migrated correctlyby running some tests  andfinally  deploy the containerartifacts to your testing staging  and productionclusters You can migrate from VMwareon premises  AWS  Azure or even within GCE Now once you migrated you want to beable to keep optimizingyour environment We have a tool calledVM Manager thatallows you to managelarge fleets of VMsrunning Windows or Linux You can manage your OS patches perform compliance reportingon OS patches  install  remove and maintain agents  apps and configurations or in other words manage your OSconfigurations  and also usethis service to collectruntime OS information We also have a technologycalled Activist Assist Now  Activist Assistis a portfolioof tools that uses data intelligence  and machinelearning to reducecloud complexityand administrative toil  makingit easy for you to optimizeyour cloud environment Whether you re interested insecurity  cost  networking compute  data  or operations  wehave the intelligence for you Some new Active Assistfeatures includeidle resource recommendations compute committed use discountrecommendations  and more So what are you waiting for You can start with aquick assessment that soffered at no additional cost You will receive adetailed inventoryof your virtual and physicalserver assets and databaseestate  and acustomized total costof ownership report formoving to the cloud as well as a detailedend to end migration plan This brings us to theend of this presentation On behalf of Jasonand I  we thank youfor watching this session RACHEL TSAO  Hello  and welcometo Top Use Cases to Start YourServerless Journey My name is RachelTsao  Product Managerfor Serverless at Google Cloud Today  I m joined by mycolleague Preston Holmes We ll cover thebenefits and use casesof using Google Cloudserverless compute We ll start with why modernizingis beneficial for everyone whether you re anestablished enterpriseor a fast growingdigital native We ll cover why GoogleCloud serverless We ll dive into whyserverless is a good fitfor pragmatic enterprises And then I will handit over to Prestonto talk about great workloadsfor your serverless journey Let s start with why modernize As we look towardsthe future  thereis a growing need forcompanies to create valuethrough application design Companies are looking to delightcustomers with responsiveness derive value from data and be insights driven respond faster tochange quickly and be secure and availablein regions globally They also want to be well runwith effective cost managementwhile being ableto scale quicklyto potentiallymillions of users These characteristicsare no longerjust a choice  but anexpectation from customers But companies facebarriers internally These include people andorganizational barriers like a lack of confidence in newtechnology  a need for control a lack of cloud nativeskills  a lack of automation It can also be resistanceto change   INAUDIBLE    lackof innovation  lackof cost optimization and the need to complywith security policies Overcoming these barrierswith a cloud native strategyis important asmodern applicationsare able to achieveoutstanding outcomes  includingbeing dynamic  scalable and intelligent These characteristicsare increasinglyimportant for them to meetfast changing user needs A cloud nativestrategy is possible whether your company ismigrating from virtual machinesor building newapplications cloud native When a company ismigrating from VMs they often face tight budgets security concerns  the needto maintain legacyinfrastructure and the need to migrateand modernize their APIs Modernizing to serverless canhelp meet many of these demandsin a way that is developerfriendly while consideringcosts and time to market Similarly  buildingnew applicationsrequires services to bebuilt in a way that reactsto market and customer demands For these newapplications  we developerswant to use thelatest technology We don t want tothink about upgrades We want to take advantage ofthe new economics providedby the cloud  and we wantarchitectural portabilityso we are not locked in No matter whereyou are starting serverless is a greatchoice and meetsdemand of modern applications Now  let s explore whyGoogle Cloud serverless Serverless is a setof characteristics It simplifies thedeveloper experienceby eliminating the needto manage infrastructure It is scalable out ofthe box  whether youwant to scale downto 0 with no traffic or up to millions of users It has a pay as you use model where you pay for exactly whatyou use instead of guessingthe capacity upfront If you don t have any traffic you don t pay for anything Serverless technology enablescompanies to quickly build develop  and deploy anyapplication in a fullyautomated environment It removes theoperational burdenof infrastructure managementwith built in security operations  andDevOps best practices As our customerneeds have evolved we ve built our serverlesscompute portfolioto enable them intheir journeys As many of you know  App Enginewas one of the first serverlesssolutions to market in2008  and early customerstook advantage ofserverless and App Engineas they scaled theirweb applicationsacross millions of userson Google s infrastructure We then added the ability torun functions in the serviceto our serverlessportfolio in 2017 With Cloud Functions  we givecustomers a simple developerexperience with the abilityto rapidly write and runtiny snippets of code In parallel  Google alsointroduced innovationsto the containermarket with Kubernetes and we started tohear from customerswho wanted to know ifwe could combine the twoawesome serverless attributesof autoscaling and developerexperience with theflexibility of containers In response  we introducedCloud Run  the next generationof serverless Serverless is no longer aboutevent driven programmingor microservices It s also about runningcomplex workloads at scalewhile preserving a delightfuldeveloper experience In fact  serverlesswith Cloud Runis about having a true developerplatform with the flexibilityto run any language any library  any binary Serverless offers thefollowing benefits The infrastructureis managed by Google meaning that youbenefit from deployingon the same infrastructureas Google engineers Your engineers do not needto think or spend timeworrying about scale Because you only pay forwhen there is traffic it often enablesgreat cost savings And finally serverless is secure Google managesinfrastructure level securityfor you  ensuring that youhave the latest patches You can choose from a suite ofGoogle Cloud security featuresto meet your business needs Google Cloud serverlessis the pragmatic choicefor enterprises and it s not justfor lightweight applications like this zippy golf cartyou might imagine here We re actually enabling allkinds of heavy duty workloads like this tractor In fact  this year  we unlockedseveral security featureson Cloud Run  making it evenmore secure for enterprises With Binary Authorizationand Manage Secrets you can ensure yourapplication is safe to run With Cloud Armor support identity aware proxy ingress settings and IAM ingress you can ensure theright folks have access And finally  you can understandwhat your application canaccess with restrictionegress out of VPC intelligent securityrecommendations and VPC SC networkegress settings We ve also unlocked newpricing options on Cloud Run Out of the box  youonly pay for what youuse for under 200 milliseconds And this year  we introducedcommitted use discounts meaning that if you committo one or three years you can save up to 17  And finally  we also introducedCPU allocation controls meaning that if youhave a workload thatneeds to run continuously  wedo not charge for service feesto start Cloud Run backup every single time In addition  all ourserverless productsoffer built in observabilityand monitoring without setup For example  requestsand container logsare automatically sent toCloud Logging from Cloud Run And error reportingis automaticallyenabled for Cloud Run No additional setup orconfiguration is required You can simply view logson the Cloud Run page and also in the CloudLogging console Choosing GoogleCloud serverless canhelp eliminateoperational overhead manage costs  all inan innovative buildingmodel with easier scalability Now I ll hand it overto Preston to seewhat workloads are a great fit PRESTON HOLMES  Thanks  Rachel So let s talk aboutsome of these workloads You ll note that inGoogle Cloud platform we have a range of optionsto run the compute portionof your application code And these can bebroadly laid outon a spectrum betweenconvenience and control So we ve got Cloud Functionsfor very small  lightweightsnippets of code We ve got serverlesscontainers with Cloud Run We ve got cluster basedinfrastructurein Kubernetes and GKE And then  for VMs  we vegot Google Compute Engine So  many of these workloadsare potentially a better fitfor one of these than another But rather than createa large decision tree we ve decided to pick outthree workloads that we thinkare a particularly good fitfor serverless and Cloud Run and those are web applications data processing  and APIs Now  I ll actuallyspeak to a little bitof a sub flavor of each of theseto give it a bit more contextof where you can get started And so  for webapplications  I mgoing to talk aboutintranet web applications So these are applicationsdesigned to be accessed onlyby company employees And what we wantto do is we wantto bring you a bit ofhow we get to interactwith our internal applicationsat Google  which Googlehas published some researchabout we call BeyondCorp And that is  atGoogle  we no longeruse VPNs to accessinternal applications Instead  we use very secureIdentity Aware proxiesthat can use contextabout an employeeto make sure theyhave access onlyto the right internalapplications Now  we have externalizedthis in Cloud platformas the IdentityAware proxy  and wecan put that in front ofour serverless Cloud Runapplications The idea of IAP isthat we only wantauthorized and corporateemployees to be able to accessthose internal applications And they should not beaccessible by the public But at the sametime  we don t wantto burden ourinternal developerswith having torecreate and reproducea secure posture for eachnew intranet application Instead  that shouldbe globally handledby a frontend systemthat will make sureonly our employees areaccessing these applications Now  this isspecifically deliveredthrough a composition andintegration of Google Cloudplatform products It starts with Cloud Identity which provides an identityservice so that we know who itis that s logging into our webapplications It can then be guarded withCloud Armor  a web applicationfirewall that does thingslike rate limiting  protectionof our applications  or canwhitelist certain IP ranges We then hit theIdentity Aware proxy This gives our usersthat user login flow So it makes surethat every accessto the actual applicationhas been authorizedand only identifiedusers are logging in And then  finally the load balancerdistributes these requests tomultiple different intranetapps So let s take a look at ademo of this in practiceand see what it can looklike at your company So here you can seethe intranet projectI have set up with severalexample applications alreadydeployed I have an Intranet Home app This acts as thehome page for listingall of my other applications And then a Birthdays applicationand a Vacations application You ll note that the ingresssetting for all of theseis set to internaland load balancing What that means is that theseare not reachable directlyfrom the internet You can see  while this URLis here  it s grayed outand is not directly accessible This is actually enforcedfor any new Cloud Runapplication deployedthrough organization policy So for the entire project ormaybe a folder of projects I can set that the only allowedvalues for this ingress settingis internal traffic only  orinternal plus load balancing So we are runningthese applicationsbehind a loadbalancer  and we havetwo backend services set up The first is really justused as our catchall to routeto that intranet home page The other is using a wildcard pattern in the host rule And what that wildcard pattern doesis allow us to use asingle mask applicationsnetwork endpoint group This networkendpoint group takesadvantage of a feature withits own URL mask there And this serviceplaceholder in the URL maskmeans that for every newintranet application Ideploy that has a servicename  I automaticallyhave the route set up to reachthat particular Cloud Runservice in thesingle load balancerwithout needingto manually createeach additional backend foreach new intranet application Now  the BeyondCorppart of this isthat I run these behindthe Identity Aware proxy This will force the login flowto reach these applications I actually have this setup to use our organizationonly as a source of identity So this is limited justto our organization The last thing we ll do here isjust check on public DNS Googleto see that the domain nameI have set up with my loadbalancer is resolving And it does It resolves to thisIP address  whichis the same as that set upin the load balancer here So with that  let stake a look at whatthe login flow looks like Again  this will be thefirst time login for a user And if I do that  I m givena standard login dialog here This is using  in thiscase  my Google account But we could also use acorporate SSO flow for IAP And now I m on theintranet home page Now  I m seeing a coupleother of the applicationsI can get to So for example  I canschedule some time off This will route me tothe Vacations servicethat has got its own hostnamepath behind this load balancer I could also go tothe Birthday service And again  this will routeto that individual Cloud Runservice Each of these can beindependently managedand deployed by differentgroups  or teams  or authors And in fact  you see that IAPwill pass through informationabout the logged in user So applications can make use ofa already authenticated valuefor the currentuser to do thingslike user specificprofiles or updates or further rule basedauthorization Now  subsequent loginsto this will notrequire any particularauth flow  except subjectto sort of any timeouts policymy org has set up because thisis using cookies toremember this loginfor some period of time If I try to log in as a userfrom some non organization useraccount  you ll seethat I don t evenhave the option of loggingin because the entire authapplication is limited toonly organization users So this will be anotherlayer of defensethat prevents anyone fromaccidentally enablingspecific permission to a user That s an option on IAP thatyou can set up and either allowthat or not allow that So you re not forced to do that But if you do want to keepthis as an organization onlyintranet setup  you can do that And so deployinga new intranet appis pretty muchjust as easy as nowdeploying anothercontainer based Cloud Runservice Each of these scales downto 0 and makes it very easy Great  now that we ve takena look at internet webapplications  let s takea look at data processing Now  data processing is reallya catchall or umbrella termthat might cover any number oftypes of changes or mutationsyou re going to maketo the data you have which include convertingdata  packaging data validating data So it really covers awide range of use cases So let s take alook at somethingthat s a bit morespecific  whichis performing somelightweight data processingat the time of ingest Now  on Cloud Platform we have a productcalled Cloud Pub Sub which is reallygreat at delivering messagesto many of our backend dataprocessing systems And as an architecturalcomponent it s often very useful todecouple the producers of datafrom those systemswhich will consume it But there are times thatyou need to bring datainto your platform This might be videogame telemetry Might be device data It might be some sortof external log ingestwhere what you want to do isnot completely decouple thisat the point of ingest So you might want to providecustom authenticationat the point of ingest Or you might want to validatethe contents of each messageand make sure that only goodand verified data is making itonto the Pub Sub topic And so Cloud Run acts as a verynice  scalable point of ingestwhere you can bothreceive the data into GCP as well as perform somelightweight data processingat that point of ingest Another one of the usecases we want to talk todayis about REST APIs Now  REST APIs are nota brand new use case In fact  they vebeen around a while But they still remaina real bread and butterof many enterprise IT projects This is often where youhave multiple surfacesin the backend composed intoa single API service  whereeach backend microservicemight be only contributingone element of anoverall API experience So in this case  we have ahypothetical weather servicewhere you might havealerts  forecast  locations each being fulfilledby a separate backendand independentlydeployed microservice Now  these are allrouted through some sortof API managementsystem  often servingthe need of authentication or rate limiting or quota management And then these reach the enduser application and eithera client or a mobileweb application One of the customers using CloudRun in this case is ecobee ecobee uses Cloud Run for APIservices  driving experience and functionality intheir customer facing weband mobile applications This has allowedengineering teamsto ship new featuresto their customersfaster  as new servicescan be turned up in days and iteration is made fasterby using the same serverlesstechnologies to simplify QAenvironments  streamliningthe path from codeto production By using fully manageddatabases togetherwith serverlesscompute  servicesscale smoothly andreliably as a whole These technologies have led togreater operational efficiency removing many ofthe incident pageswhich were due purely toinfrastructure issues insteadof the applications themselves And with that  Iwant to thank youon behalf of Rachel andmyself for joining us today LEIGHA JARETT  Hi  andwelcome to our session wherewe re going under the hoodfor Google Cloud s datatechnologies I m Leigha Jarett and I m a developeradvocate here at Google Cloud and I focus on data analytics But today  I m also joinedby my colleague  Derek who focuses onoperational databases Today  we re going to betalking about two technologies First  Spanner onthe operationaldatabases side of things on theleft hand side of our screen And then  we realso going to focuson BigQuery  our data warehousefor analytical processing And these are just a few of thepieces in the broader GoogleCloud data ecosystem Integrations betweenboth of these productsand other productsthat support datafabric  artificial intelligence and business intelligenceall go into making it simple fordata practitioners and businessusers to power data driveninnovation using Google Cloud So the story of bothSpanner and BigQuerystarts with the evolutionof the separationof storage and compute Historically database systems havebeen architected with tightlycoupled storage and compute so there s a dependence onlocally attached storagebecause attachingstorage to a networkwould result in higher latency But more recently these constraintshave been lifted  largelydue to faster networks which we re going totalk about momentarily So now  with architecturelike BigQuery and Spanner we have a separationof storage and compute and this allows usto seamlessly scaleto meet your highthroughput data needs New compute nodescan easily be addedto handle largeworkloads  and wedon t need to rebalance the dataand ensure that a local replicais stored on new nodes So let s take a closer lookat how our distributed storagesystem  Colossus  isalso faster  reliable and cost effective for you So Colossus is Google snext generationdistributed file system It manages  stores and provides accessto all of your data forboth BigQuery and Spanner Its design enhancesstorage scalabilityand improves availabilityto handle the massive growthin data needs Colossus introduced adistributed metadata modelthat delivers a more scalableand highly available system With all this datastored remotely if a single node fails there s no data lost  or evenmade temporarily unavailable Not to mention  remote storageis dramatically cheaperthan local storage And the proof is in the picture So not only does Colossushandle your first party datastored in GoogleCloud  but it alsostores the information neededto run vital Google systems thatare used by so manypeople all over the world Like we mentionedearlier  the separationof storage andcompute would not bepossible without the evolutionof networks  which increasedthe speed at which computeclusters can pull datafrom storage and reduce latency Just like Colossus Google Cloud serviceslike BigQuery and Spannerleverage the same networkthat powers the underpinningsof Google s systems And we call thisnetwork Jupiter So Jupiter is made up ofan in house custom network hardware and software and it connectsall of the servers in ourdata centers together powering our distributedcomputing and storage systems So we discussed our filesystem and the networkthat connects the storageand compute together Now  leading all theoperations in our data centeris Borg  Google s internalcluster management systemand the predecessorto Kubernetes So Borg goes ahead andprovisions the needed computeresources to handle workloadsfor Spanner and BigQuery running hundreds ofthousands of jobsand making computingmuch more efficient This allows ourdata centers to runat a really high utilization So now  let s see howthese pieces come togetherthroughout thelife of your data So let s take a look atCymbal Gaming  our fake gamingcompany who uses Spanner tosupport transactional or OLTworkloads So a new user might loginto the Spanner gameand enter their information This data is then sentto Colossus for storage but it s going to be inSpanner s file format Next  this data is requested ina query sent through Spanner scompute engine And finally  theresults of the querywill be returnedin the applicationto populate the newuser s profile whenthey load the page So Spanner s storageand compute are bothgoing to be optimized forthose transactional workloads And Derek is going to talkmore about that in a minute So even though Cymbalis using Spannerfor transactionalworkloads  they alsowant to perform analyticson this informationto understand how users areactually playing their game But for analyticalworkloads  they regoing to instead use BigQuery So once the user s data lands inSpanner  they have two options First  they can replicate thedata in BigQuery s storageby pushing new rows intothe native BigQuery table This is going to provideoptimized storagefor analytics On the other hand  theycan use a federated querysent from BigQueryover to Spannerso that they get real timeaccess to informationfrom their application Either way  they ll benefitfrom BigQuery s query engineto perform scalableanalysis  includingbuilt in machinelearning capabilitiesthat power businessintelligence reportingand real decision makingfrom business users So now that you understand howBigQuery and Spanner both cometogether to supportyour data needs let s talk about howSpanner is actuallyoptimized for thosetransactional workloads So  Derek  over to you DEREK DOWNEY  Great Thanks  Leigha Cloud Spanner is Google s fullymanaged distributed relationaldatabase Spanner provides customers withminimal operational overheadwith no downtimefor schema changes or even maintenance windows This allows Spanner toprovide an industry leading99 999  SLA Because Spanner isdistributed database it provides seamlessreplication and scalingacross regions in Google Cloud Spanner processes overone billion requestsper second peak  and all ofthat as a relational databasethat provides strong externalconsistency  made possibleby the technologiesmentioned earlier Borg  Colossus  and Jupiter To set up an example imagine that Cymbal Gameshas developed a game thatwill be played by peopleall around the world These players must be ableto sign up  authenticate and play the game Players really don t liketo have to redo actions so this activity has tohave traditional consistencyguarantees Another thing playersdon t like is downtime If they can t play  they getupset  and it better be fast So Cymbal Games will need tohave the ability to scale upto meet user demand With availability  consistency and scalability in mind let s see how Cloud Spanner canhelp them achieve their goals When you create aSpanner node  youget access to Spanner resourcesin three or more zones Zones are Borg cells These cells are a collectionof Spanner resourcesthat are physicallyseparate from other cells Spanner resources consist ofSpanner servers and Colossusallocations of data storage as discussed previously Spanner servers providethe compute capacityfor your cluster  and theycontain the span process Paxos  and other board tasksto enable Google SREs to managethe environment The zones can be within asingle region  as shown here or across multiple regions If you have a regionalconfiguration you have zonesclose together thatprovide the lowest readand write latencies but you only get an SLAof 99 99  availability In a multi regionalconfiguration your zones are locatedfarther apart  whichcan increase latencies but it providesglobal scalability and anSLA of 99 999  availability Now  imagine that CymbalGames has a table of users The user data is stored onColossus and encrypted at rest A copy of this data isin at least three zones and replication of the rightsis handled synchronouslyby Paxos consensus So their data willsurvive zone outagesand allows for zero downtimemaintenance as well To understand consistency  let sgo over typical transactionlife cycles When a new user signsup  Cymbal Gamesissues an insertstatement to Spanner Since nodes existin multiple zones where the insertstatement hits dependson which zone is thecurrent Paxos leader In this example zone 2 is our leader A single endpoint isexposed to the clientand Spanner internalshandles routing of the query When the leaderreceives the write it acquires locks and generatesa true time commit time stamp then  it sends thewrite to other nodesin the configuration These nodes write themutation to somethinglike a durable transactionlog and confirm the writeto the leader After a quorum of nodesapproves the write then the leader tells thenodes to commit the data it releases the locks and returns the successto the client All of this usually happenswithin a few milliseconds We can t do asession about Spannerwithout mentioning TrueTime TrueTime is a highlyavailable internal servicebacked by a network ofatomic and GPS masters This service can be calledto generate a guaranteedmonotonicallyincreasing timestampwith microsecond granularityacross all nodes In Spanner  these timestampsare generated for writes It is preciselybecause of TrueTimecommit time stamps thatallows a distributeddatabase like Spanner to ensurestrong external consistency So let s see how theyare used for reads Once a user accounthas been created they d want to be able tolog in and start playing So Cymbal Games needsa consistent readto provide authenticationfor the user In this example the read is goingto be executed againstzone 1 since it sclosest to the clientmaking the request But since zone 2 isthe leader  zone 1checks in to make sure ithas the latest commit data If the data is the latest  itcan be returned to the client Otherwise  the read waits untilthe node receives updates How does the replica knowthat it has the latest data With TrueTimecommit time stamps The replica comparesits own TrueTime statewith the latest commit timestamp to know if it s enoughup to date to serve the read But not all reads of the userneed to be strongly consistent Since user data doesn t changethat often  Cymbal Games canchoose to issue atime bound read otherwise known as a still read To do this  theyprovide the max agethat is acceptable for theirquery  making it time bound This allows the Spannernode closest to the clientto respond to the query withoutincurring additional roundtrip latency  communicatingwith the Paxos leader No matter how fastthe Jupiter networkis  if you can cut out latencyacross zones  you should With availability andconsistency covered Cymbal Games still needsscalability out of Spanner So let s look at that As a first step  CymbalGames gets closer to launch they can go ahead andincrease the number of nodesfrom pre prod levelsto prod levels simplyby updating the Cloud console Next  as the userdata grows  Spannerwill start to createsplits of the data It does this both bydetecting CPU loadincreases and hot spots And we ll split the data intochunks to address these issues These splits can be hostedon different Spanner serversbacked by differentColossus allocations  whichhelps distribute the load Data splits and increasingthe number of Spanner nodesgives Cymbal Gamesthe ability to scaleas much as they need to But to avoid headachesdown the road they need to be sure to placerelated data in the same splitusing interleaving Imagine user activity datathat relates to the user data By interleaving the useractivity table with the usertable  Spanner willchange the splitting logicto make sure that the datastays together on Colossus In this way  Spannerscales easilywhile also keepingperformance highby avoiding additional lockingrequired to read or writeacross multiple splits Managing thisdistributed databasefor transactionalworkloads is only possiblebecause of technologies likeTrueTime  Borg  Colossus and Jupiter So now  Leigha is goingto take it over and talkabout analytical workloads LEIGHA JARETT  Great Thanks  Derek So now that you understand howSpanner is great and optimizedfor transactionalworkloads  let stalk about how BigQuery sarchitecture supportsanalytical workloads So let s start byadding the new userdata into a nativeBigQuery user table As soon as you push datainto a native BigQuery table the data is going to betransformed into a file formatcalled Capacitor Capacitor uses what we callcolumnar oriented storage meaning  eachcolumn in the tableis going to be stored in adifferent area of the file So the amount of thecolumns you queryis going to be proportionalto the amount of datathat s actually scanned Now  if you think about it this is optimal for analyticsbecause you re often aggregatingjust a few columns over lotsand lots of different rows Plus  each columnalso has independentlycompressed information Here  we use differenttypes of encodingto optimize storage  whichmakes it easier for Dremel our query engine  to quicklyfind and use the data it needsfor your query Even better  you can chooseto cluster or partitionyour data stored in Capacitorso that Dremel can easilyeliminate certain filesor blocks from the query Let s look at a quickexample of this So let s say wehave our user tableand we decided topartition based on the datethat the user was created We re also clustering or sortingour table based on the game ID So now  when we filter our databased on created date and gameID  Dremel is going to usethe file headings in Capacitorto reject the entire fileor file block if it s notneeded for the query This means that less datais going to be scannedand your query is going tobe both cheaper and faster So in Colossus  your datais stored in Capacitor and it s going to beautomatically compressed encrypted  replicated and distributed So 100  of your data is goingto be encrypted at rest  justlike Derek mentionedwith Spanner And Colossus is also goingto ensure durability usingsomething callederasure encoding  whichstores redundant chunks of dataon multiple physical disks So immediately  uponwriting data to Colossus BigQuery is going to startthis geo replication process mirroring all of thedata into different datacenters around the specifiedregion or multi region depending on how you setup your BigQuery data set This means that you havebackups of your data storedin case of disk failureor data center outages OK  so we talked a littlebit about BigQuery storage Now  let s talk about compute So Dremel  BigQuery squery engine is made up ofsomething called slots and these are Google sproprietary units of CPU  RAM and network  and they re usedto execute the SQL queries So BigQuery willautomatically calculatehow many slots arerequired by each query depending on the data size andthe complexity of the queryitself You can think of aslot as a worker and slots are going to beresponsible for executing partsof the active query independently and in parallel and they re going to performpartial aggregations They ll also read andwrite data back to storage So the awesome thing aboutBigQuery s architectureis that you can actuallypurchase slots and reserve themfor certain workloadsin your organization This means that you canscale your throughputand your runtime as needed So now  let s go a little bitdeeper into query execution First  when you submita query  BigQueryis going to break it down intodifferent stages  which willbe processed by those slots Intermediary results aregoing to be written backto what we call theremote memory shuffle Since data here isusually stored in memory it s really fast for slotsto read and write datafrom the shuffle Now  each shuffled rowcan be consumed by slotsas soon as it s created This makes it possible toexecute distributed operationsin a parallelpipeline  which meansthat we can analyze hugeamounts of data really fast And if a worker were to have anissue during query processing another worker couldactually simply pick upfrom where theprevious one left offbecause they re able to readfrom the previous shuffle So this adds an extralayer of resilienceto failures within theworkers themselves OK  so let s saywe re running a queryto count how many usersjoined Cymbal Games yesterdaybefore feeding theminto our ML model In the first stageof the query  slotswill read the columnar filesfrom Colossus in parallel Each slot processes a column orfile block  filters the data and then writes the partialaggregations to the shuffle In the next stage  a slotreads the data from the shuffleand combines those partialaggregations together Then  it s going to writethe results back to Colossusso that it can be served as acache table back to our user So how do these architecturaldistinctions of BigQueryactually translate tobusiness value for you Well  first of all we talked about howBigQuery s optimizedstorage and compute allowsfor super fast queriesover huge data volumes meaning your businesswill be empoweredto make informed decisionswithout ever missing a beat Next  we showedhow BigQuery helpsyou save money on datastorage and analysisso you spend lessresources answeringbusiness critical questions We also talked about howBigQuery s storage ensuresreliability anddurability  meaningthere s a verylow risk of losinginformation and interruptingbusiness processes And finally  the ability topurchase and dedicate slotsto specific workloads means youcan scale throughput and runtime so that you continueto support your businessanalytics as your dataand your use cases grow And don t just takeour word for it Google Cloud actually hashundreds of BigQuery customerswith petabyte scaledata warehouses So thanks  Derek  forwalking us through Spanner And thank you all somuch for joining us todayto dig into Spanner andBigQuery under the hood Here  we providedsome resources to helpyou get started usingboth Spanner and BigQuery and I ve alsolisted some sessionsthat are going on at Nextrelated to data analyticsand databases So thanks again for joining us And we hope youenjoyed this session and that you enjoy the rest ofyour time spent here at Next  MUSIC PLAYING JEANA JORGENSEN  Welcome toCommunity Day at Next  21 My name is Jeana Jorgensen And I m here with Urs Holzle Today is unique in thatit will span an entire 24hours to bringtogether communitiesfrom all around the worldin local time zones But before we getinto those details we wanted to share somereally exciting news with you URS HOLZLE  That s right But first  I d like to thankyou all for your incrediblecollaboration andcommitment to Google Cloud because developers arereally the core inspirationbehind everything that we build Your constant pursuit ofexcellence motivates us And your feedbackpushes us to be better In a developerkeynote yesterday I announced the GoogleCloud Innovators Program With this program  we wantto bring all of Google Clouddevelopers togetherto co create  to learnfrom each other  and tounite our global technicalcommunities where we resupporting Google Cloud And we also want it toreignite the human connectionthat many of you have missedover the last 18 months And that can be dismissed oroverlooked in online forums And finally  we know thattechnical communitiescan sometimes beintimidating to new members And so our program sprimary design principleis grounded ininclusion and in servingthe community  regardlessof your knowledge baseor your status earned So I want to personallyinvite each of youto join us to become aGoogle Cloud innovator So come share your experience coach and grow others Most importantly learn from each otherto build things thathave a meaningful impacton the people who use them JEANA JORGENSEN  Terrific To join us  just sign up atcloud google com innovators We re really excitedto get to know you We also wanted to let you knowabout a dedicated technicalcommunity event we willhost for the first timethis coming February Twice a year in differentregions around the world we ll bring the users andcreators of Google Cloudtogether to brainstorm learn  and get handson to move projects forward Google Cloud innovators willbe eligible for invitationsand speakingopportunities early So please sign up tobe an innovator today and we ll share more detailsabout the event in the comingweeks Urs  we ve got a few morepieces of good news to share URS HOLZLE  All right Yes We continue to investheavily  of course in developer experiences throughintuitive in product usabilityor through community engagementto offer deeper recognitionof your expertise or throughaccess to tools and resourcesthat help you get your job done And another key investment isin our learning experiences So  for example to meet the demandsof cybersecurityprofessionals  Googlehas pledged to train100 000 Americans in fieldslike data privacy  dataanalytics  security and IT support And additionally  Tech News World cited that over90  of IT leadersexpect to expand CloudServices over the next oneto three years  but over80  identified a lackof internal skills andknowledge as a top barrierto Cloud success And according to another studydone by Global Knowledge Google Cloud skills arein super high demand commanding the top two payingit certifications in 2021in the US So I m very happy to announceGoogle Cloud Skills Boost Google Cloud Skills Boost isthe definitive destinationfor Google Cloud learning including 700 labs  courses learning paths  skills badges and certification resources allauthored by Google Cloud The platform isavailable globally todayin English  Japanese and Spanish with more languagesto follow soonto enable millions ofpeople to build and validatethe skills that they needto thrive in an evolvingCloud first world And finally  toreaffirm our commitmentto accessibility ofthese learning resources we are offering access at nocost for 30 days to GoogleCloud Skills Boost So visit the website toaccess free training today JEANA JORGENSEN  Fantastic OK Before we get on to the detailsof how Community Day will run I have got twoquestions for you OK  first question Google Cloud hasbeen talking about empatheticengineering for quite some timenow Can you tell us a little bitmore about what that meansand why you think it strengthensdeveloper experiences URS HOLZLE  Sure By empatheticengineering  we reallymean  how are weputting ourselvesin the shoes of ourusers  the developers and ensure that we buildproducts and services thatreally start withyour needs first We ve always been anengineering led company So what we neededto learn was reallyhow you wanted to use ourservices because users comewith different challengesand different backgroundsfrom differentindustries  some of themnew to Cloud technologies So that s where customerempathy comes in As an engineer  do you reallyunderstand the experiencethat real users havewith your products Are you really walkingin their shoes So we implemented alistening and learning systembased on customer empathy So for example teams test productsby performing common userjourneys just the same waycustomers would without internalhelp  without internal tools without insider tips We gather the feedback We improve the servicesor the documentationand then test itwith real customers So we keep iterating untilwe re confident that we vebuilt intuitiveservices that areeasy for developersto implement and buildgreat solutions around JEANA JORGENSEN  Great Thank you for that So here s my secondquestion for you If you were starting yourcareer in technology today where would youfocus your learning URS HOLZLE  Wow that s a huge question Because technologyevolves so quicklythat it can bepretty overwhelming especially if you reearly in your career and unfortunately because ofthe digital transformationof many industries  thesuccess of those organizationswill increasingly depend onthe services and products thatare built by its developers so learning reallyis super important Based on what I veseen  I would probablystart with these three things First  you have to dedicate timefor learning every single week right So I d start out by determiningwhat forms of learning workfor you independentlyof the subject and thenreally allocate a set of hoursto grow your knowledge base So for some  reading works best For some  it s videos For some  it sinteractive events You just got to understandwhat works out for you And so next  I would picka couple of topic areasthat really interestyou and startlearning the fundamentals Joining onlinecommunities or forumscan be a really good way toget different perspectiveson the subject that maybe aren tin the standard traditionallearning paths And then finally  ofcourse  the questionis  well  what subjectsshould you go after Now  there s hundreds of them And so that s reallysomething that depends on youand your preferencesand your situation But I d say threeareas kind of cometo mind for me  firstcustomer empathy I already talked about it It s not somethingthat s taught in schools but it s really really importantthat we build technology thatpeople actually want to use right And so when designa product  and youstart with a deep understandingwhat the customer actuallyneeds  then you can reallybuild more than just a solution You can create a viablebusiness and a viable productthat really lasts Second  learning what ittakes to ship a product also not broadlytaught in schools it s really invaluable inalmost every job that I ve seen So that includes things likethe fundamentals of releaseprocesses  lean oragile development and how you best design productsfor self service deliverybecause you want todesign your product suchthat users can startusing it quicklyrather than depend on somecomplicated instructionsor deployment process And then last butnot least  security You ve all heard aboutthe growing cyber threatsthat so manyorganizations are facing And they re only going to getmore sophisticated and moreinvasive And so security issomething that youneed to build into your productand in your development processfrom the beginning You can t bolt it on later So learn about the fundamentalsof securing a productor system  and thenkeep up with the trends So that will giveyou the confidencethat you need to build long termviable products or companies JEANA JORGENSEN  Right Really great advice Thank you for that I know it ll be especiallyimportant for new developersand early in career folks So thank you I mentioned earlierthat I d give youa quick rundown of Communitytoday and how it would unfold So let s do that now Right after this  there isa special spotlight sessionfeaturing JimHogan  a health carestrategist in GoogleCloud alongside VintCerf  the co creator of TCP IPand who has often been coinedthe father of the internet The two will have acandid conversationabout accessibilityin the workplace We think it s goingto be a great session And we really  really areexcited to have you join us After that  we ll head intothe heart of Community Day You can join us in an immersivegathering experience calledKumospace  which creates virtualcampus with meeting rooms smaller spaces to hang outfor one on one conversations and then larger auditoriumsfor more common topics Every single partof the day willbe powered by GoogleCloud  including Kumospace So the day kicks offwith conversationsled by Google DeveloperGroups across Europe  MiddleEast  and India  andthen the rest of the day we ll have a varietyof experiencesand community gatheringswith a few topicsavailable at each time block The first timeblock  you can attenddiscussions led by Googlersand community memberslike our Google DeveloperExperts and expert innovatorsdedicated to specifictechnologies like Vertex AI secure software supplychain  Flutter  or Workspace And then the second block  youcan follow a specific communitypathway and spend time withspecialized groups like Kaggledata scientists orWomen Techmakers or evenregion specific groups And the third and finaltrack throughout the daywill have dedicatedspaces for learning  how you can earn aCloud certification develop new skills inhands on lab sessions or even apply your skills forgood and philanthropic relatedcompetition withother attendees We really lookforward to seeing youand hope you enjoythe rest of the event Join us next for theDEI spotlight sessionwith Jim and Vint Thank you so much for spendingthe last few days with us Remember to sign up forGoogle Cloud innovatorsand become part of ourGoogle Developer family We re excited to get to knowyou and spend more time with youin the future Enjoy the rest of Next DOUG KELLY  Welcometo our session uplevel your data expertise professional data engineerand professional MLengineer certification I m Doug Kelly and Inow oversee learningofferings for machinelearning and AI solutionson Google Cloud Previously  I worked as an MLengineer and data scientistat Google Cloud and oversaw datascience content at Coursera I now spend my timethinking deeplyabout industrializingmachine learning developmenton Google Cloud and drivingthe creation of trainingto prepare learners such asyourselves for data and machinelearning engineering careers In this session  Iwould like to start offby discussing two key data andmachine learning challengesthat enterprisesare facing today The gap between data platforminvestments and realizing valuefrom them and the scarcityand corresponding high demandfor data and machinelearning engineering talent This will frame our discussionon how Google Cloud isaddressing these  both throughdata and machine learningplatform innovations as wellas through our industry leadingprofessional certifications the professional dataengineer and professionalML engineer certifications I hold both ofthese certificationsand will share myperspective with youon the benefits ofthese certificationsas well as therecommended learningpath and resources for each Lastly  I willgive you a previewof what s coming nextfor data and machinelearning training fromGoogle Cloud learning My goal for thesession is that youwalk away excited about buildingdata and machine learningsolutions on Google Cloudand you start or levelup your learning journeytowards certification today One of the biggerchallenges for data teamsis turning into acompetitive advantagethe immense  fast growing and fast moving amount of datathat our organizations  ourcustomers  our partners our ecosystem is creating According to IDC  175zettabytes of datawill be created inthe next five years One zetabyte equaling1 trillion gigabytes  that s 21 zeros Our data productionhas more than 10xedin the last eight years In fact  90  of thedata in the worldhas been created injust the past 10 years Yet  when it comesto turning datainto business outcomes andbusiness value for companies the situation is quite alarming Over 2 3 of the data producedis never actually analyzed And almost 70  ofcompanies say they reunable to realize tangible andmeasurable value from data The key takeaway is this delivering value with analyticsand machine learningrequires a modern dataplatform such as Google Cloud Data engineers are essentialto build  operate  and maintainthese data platforms toenable business value And machine learningengineers  in turn build and deploy machinelearning solutionson top of a robust data platformto drive decision makingand the realization of value Data engineers and MLengineers work closely togetheron data teams toclose the data valuegap by tightly integratingdata and machinelearning infrastructure In industry  we knowthat machine learninghas had the most business impactby scaling real time decisionmaking and personalization That means we have to act fast To do so  we need data engineersenabling data processingand analytics speedand ML engineersto deliver predictions thatarrive at the point of actionand real time That s the difference betweendiscovering and preventingfraud  purchases andcart abandonment and proactive andreactive customer service Google Cloud is simplifying dataanalytics and machine learningthrough each ofthe platform designpillars outlined on this slide We deliver serverlessanalytics  allowing customersto keep development velocityhigh and their focus purelyon analytics versusinfrastructure We offer a comprehensive setof data analytics workloads from batch and streaming dataprocessing to data warehousingto business intelligence We provide the servicesfor our customersto operationalize machinelearning solutions from pre built ML APIs toAutoML to BigQuery ML to customTensorFlow and PyTorch models We enable the best open sourcedata technologies to give youflexibility and choice And we build for enterprise dataand machine learning workloadsat any scale Google Cloud is already arecognized industry leaderwith a purpose built  end to enddata analytics and machinelearning platform to supportthe entire data team anywhere on premise or on Google Cloud Data engineers are able toleverage services like Pub Sub Dataflow  and Data Fusion toingest  prepare  and transformdata at any scale They can also leveragenew data serviceslike data plaques andanalytics hub and marketleading analytics services such as BigQuery and Looker to manage and quicklysurface enterprise datasets and insights tothe entire organization Announced at GoogleI O this year Vertex AI is also rollingout as the successorto AI platform for machinelearning development It provides ML engineers withan integrated suite of servicesto help enable each stageof your ML workflow from gathering data to featureengineering to building modelsand  finally  deployingand moderneeringthese models in production For example  newVertex MLop services like Pipelinesand Feature Store give ML engineersreusable ML workflowswhile Vertex Predictionsgives ML engineersa scalable  serverlesssolution to getmodel predictions intodecision making UIs faster Depending upon how you definethe machine learning market if you look at platforms likeQwiklabs  Kaggle  Coursera and LinkedIn  thereare only about 300 000global professional dataengineers  data scientists and ML engineers today Yet  at the same time  there shuge demand for data engineersand ML engineers in the market According to Courseramarket research in just the past12 months  there sbeen over 200 000 job postingsrequiring data engineeringand ML engineeringskills  and that demandhas been trending upwards Over the past threeyears  there sbeen a 91  increase in jobpostings requiring these skillsets Many enterprises arestill in the early stagesof data platformmodernization and AI adoptionand are really struggling tofind data engineers and MLengineers to make this happen To bridge the skills gap  GoogleCloud learning has brought someof the best of Googletogether  its top engineers it s solutions and expertisein building large scale dataand machine learning systemsto produce two industry leadingcertifications in the jobmarket  the professional dataengineer andprofessional ML engineer These certifications arenot just about Google Cloudtechnologies In fact  the initial stages ofdesigning each certificationare product agnostic And just like thereal world examineesare expected toknow the vast amountof foundational concepts  designpatterns  and technology tradeoffs that they may encounterin their day to day jobs So how have these certificationsbeen received by learnersand recognized inthe job market Think of GoogleCloud certificationsas valuable tools tovalidate your learningand help you build your career get promoted  and earn raises In a recent survey of GoogleCloud certified individualsconducted by anindependent third party we heard some incrediblepieces of feedbackfrom learners across the jobmarket on how Google Cloudcertifications have enabled andboosted skills for Cloud roles 78  of respondentswere more confidentwhen talking about theirfuture opportunitiesafter certification 82  of respondents feltproud to have a proofpoint about theirskills and abilitieswhen working with recruiters 83  of respondents told usthey felt more marketable And finally  85 of respondents justhad the boost of confidencethat they both wantedand needed to reallyget themselves out therefor Cloud opportunities So in short  if you re askingyourself  what s in it for me I hope these anecdotalshares and statistics drivehome how learnerslike yourselveshave benefited from GoogleCloud certifications Next  let s take alook a little bit moreclosely at thevalue propositionsfor the professional dataengineer and ML engineercertifications First  let s talk about theprofessional data engineercertification A professional dataengineer is the enablerof data driven decision makingby collecting  transforming and publishing data This is an intermediateprofessional certificationthat requires three plusyears of industry experienceand one plus yearsdesigning and managingdata solutions on Google Cloud We offer a dataengineering learningpath with hands onpractice through Qwiklabsthat culminates in atwo hour multiple choiceand multi select examto validate your abilityto design  build  andoperationalize data processingsystems Upon completion  you earn ashareable credential and gainaccess to Google Cloudcommunity  swag  and events So why do I think you shouldget certified as a Google Clouddata engineer Data engineering is achallenging applied disciplinethat is best learnedthrough practice Through following therecommended learning path you will complete 30 plushands on data engineering labsin a sandbox GCPproject environmentto practice buildingand deployingdata processing solutions Further  the Google Clouddata engineer certificationwas recently namedas the number onetop paid global IT certificationaccording to global knowledgemarket research You will learn some of themost valuable and in demandenterprise skillsin the market and beindispensable to organizationsmodernizing their dataplatforms and looking toadopt machine learning Third  you will learn dataengineering best practicesfrom Google  which broughtHadoop and Beam to the worldand currently processes over2 5 exabytes of logs permonth on planet scalecompute in network Through this certification you will differentiate yourselffor career success by learninghow to choose the right datastorage and processing services identify the right balanceof trade offs forvarious data workloads and practice implementingproduction tested dataprocessing design patterns To prepare you for the exam we designed a learning paththat covers foundationaldata conceptsand data pipeline designpatterns as well assteps you progressively throughkey data ingestion  storage and data transformationservices on Google Cloud For aspiring dataengineers  the learning pathis designed to be completedin about three and 1 2 monthsand about five hours per week For existing data engineers the Coursera preparationcertificate isstill a great placeto start to learn Google Clouddata services that you can thensupplement as needed withadditional hands on practicethrough Qwiklabs I recommend the certificationto ML engineers  as well and personally startedmy certification journeyhere first Data managementand data processingare key foundations of modernproduction ML pipelines Now  let s discuss theprofessional ML engineercertification This is a moreadvanced certificationthat also recommends three plusyears of industry experienceas a data scientist ormachine learning engineer including at least oneyear designing and managingmachine learningsolutions on Google Cloud A professional MLengineer designs  builds and production houses ML modelsto solve business challengesusing Google Cloud ML servicesand knowledge of proven MLmodels and techniques So why do I think you should getcertified as a Google Cloud MLengineer The Google Cloud professionalML engineer certificationis continuously updated byleading Googler and industrysubject matter experts tobest reflect the ML engineerskill set in the job market This gives aspiring andcurrent ML engineersthe opportunity to validatetheir expertise in machinelearning in Google Cloud againsta rigorous  practical industrybenchmark exam recognizedin the job market Google is a globallyrecognized leaderin AI research andapplied machine learningwith over a decade ofexperience productionizingmachine learning systems aspart of applications servingbillions of users per day Google Cloud is theplatform by whichGoogle delivers this pioneeringML research and productsolutions to customers In fact  GoogleCloud s AI platformwas recognized by Gartner asan industry leader in its 2020Magic Quadrant for CloudAI developer services My key takeaways on the MLengineer certification for youall are this The Google Cloud professionalML engineering certificationis a rewarding learningjourney for youto build your development skillson an industry leading dataand machine learning platform toboth differentiate yourself aswell as position yourselffor success in the rapidlyevolving field ofapplied machine learning The learning path is designedto augment your studythrough a structured progressionof role based curriculumthrough GoogleCloud fundamentalsthrough applied machine learningand up to ML ops content It s designed to be completedin about four monthsat about six hours per week The content isavailable on demandthrough a Coursera professionalcertification certificatewith additional hands onpractice opportunitiesavailable through Qwiklabs For aspiring ML engineers I have two additional tipsfor you First  take advantage of  300in free credits in the GoogleCloud free tier on productssuch as the ML APIs AutoMLand BigQuery to start buildingML applications today Second  learnsocially and publicly Form a study group to shareyour learnings and projectswith others GitHub  blogs  or theGoogle Cloud learning forumsare a great place to start I personally like opensource contributionsto projects  tutorials and documentation Take a look at open issues onsome of your favorite emailprojects like TensorFlowand Google Cloud samplestagged as help wantedor good first issueas an approachableplace to dive in Lastly  I d like to sharewith you all of what scoming next for Google Clouddata and machine learningtraining It s an exciting time to bebuilding data and machinelearning solutions So far in 2021  Google Cloud salready industry leading dataand machine learningplatform hasseen its most significantenhancements in the past twoto three years In the June 2021 data summit Google Cloud engineering teamslaid out their visionfor the data cloudwith new services likeDataplex and Analytics Hubto bring new data management andanalytics sharing capabilitiesto enterprise data teams Together  along with newintegrations between BigQueryand Booker  data engineers canfocus on further breaking downorganizational data silosto more broadly socializeinsights across organizationsto enable business value Furthermore  atGoogle IO in May 2021 Google Cloudannounced Vertex AIs the successor to AI Platform a unification of servicessuch as AutoML andcustom trainingfor developing MLsolutions faster There are over 20 plus majorupdates and net new servicesreaching GA over the nextyear  particularly relatedto ML ops  opening upfundamentally new waysto build and deploy MLsolutions and incorporate theminto decision making systemsto deliver business value Vertex AI  BigQuery and Looker  as well asAI industry solutions empowerdata scientists or dataengineers and MLengineers with new toolsto close the data value gapwhile enhancing developmentand productivity  the abilityto scale your workflowand your decisionmaking with your data as well as acceleratingtime to value My colleagues and I inCloud Learning servicesare hard at work bringing DataCloud  Vertex AI  and industrysolutions content to youthrough Qwiklabs and Coursera Stay tuned forsignificant new dataand machine learningtraining rolling outin the second half ofthis year as well asa major update to theprofessional ML engineercertification for Vertex AIin the second half of 2022 Now  I m keen to switch overto the Google Cloud consoleto share with you some of thenew capabilities of Vertex AI Welcome to the Vertex AIML development platformin the Google Cloud console To get started  firstenable the Vertex servicein the API and Services tab Vertex AI is a platformthat contains servicesfor each phase of the MLdevelopment lifecycle from data sets topredictions  enablingyou to go from experimentationto production MLworkflows to deliveringbusiness value faster The Data Sets tab containsa list of all your datasets in your project withsupport for image  tabular text  and video data types Manage data sets enable you tocreate a link between your datasets and your models fortraceable and reproducibleexperiments You also get descriptivestatistics on your data setas well as the ability toconfigure automatic or customdata splitting The Notebooks tab contains newfeatures such as Managed VertexNotebooks Vertex Notebooks are interactiveJupyterLab developmentenvironments that enableyou to quickly build custommachine learning solutions They provide supportfor multiple frameworksflexible compute and easyaccess to acceleratorslike GPUs and TPUs They also have newer tighter integrationswith key GoogleCloud data services like Cloud Storage andBigQuery in a single window Check out some of the newerintegrations with Data PROCas well  which alsomakes Vertex notebooksa great single window fordevelopment of Apache spark as well In the Training tab you can alsosee and create your trainingpipelines using the samemanaged data set for both AutoMLand custom training jobs You can leverage pre builtcontainers with your model codewith frameworks like TensorFlow Scikit learn  PyTorch or XGBoost For maxima  flexibility you can alsoleverage custom containers  aswell  leverage in any frameworkyou d like with anyadditional dependencies upload it to artifactregistry  and use itfor model  training  prediction or as a pipeline step AutoML is a great way to quicklybuild highly accurate modelswith built in support forstandard evaluation metrics feature attributions quick model deployment as well as metadata thatcovers the model architecturesearch as well as thehyperparameters thatwere searched  as well In the Experiments tab  you cancreate a managed TensorBoardinstance to organize and comparemodel performance across modelruns and data splits For predictions  youcan deploy new endpointsfor online predictionsor scheduled batch jobsfor bulk predictions  as well A model can be deployedto multiple endpointsor can serve multiplemodels on a single endpoint With multiple models on a singleendpoint and traffic splittingcapabilities  you now have somenew model prediction evaluationcapabilities  suchas A B testing For production ML workflows the new Vertex pipeline serviceenables you to automate monitor  and governyour machine learning systemsby orchestrating your tasksin a serverless manner Clicking into anindividual pipeline run you have a graphicaluser interfaceto view and debug eachstep of your workflow from data set creation model training  evaluation and deployment You can expandupon your artifactsto be able to deep diveinto individual pipelineruns and their artifacts  suchas model evaluation metrics In this example  you also havea conditional model deploymentstep  which youcan use to ensurethat you re only pushinghigher performingmodels to production This concludes aquick walkthroughof just some of the newfeatures available in Vertex AI I encourage youto also check outservices like data labeling ML metadata for trackingyour development artifactsacross your project as well as the upcomingFeature Store  whichenables you to reusefeatures across ML use cases For hands on on practice in aGoogle Cloud project with someof the features I justintroduced  check outthe Vertex AI labs on Qwiklabs A great place to start isthe Vertex AI Qwik Start where you will walk throughan end to end custom trainingworkflow on Vertex AIto train and deploya TensorFlow model to predictcustomer lifetime value To start your data andmachine learning engineeringlearning journeys today  checkout our recently launchedGoogle Cloud SkillsBoost  our new destinationfor Google Cloud learning withover 700 labs and courses This will be available to youall the entire month of Octoberfree of charge It s never been a moreexciting and productive timeto be building data systems anddoing applied machine learning Thank you for learning with us MICAH BAKER  Hello  andwelcome to our session This session isintended for customerswho are new to CloudRun for Anthos as well as ourexisting customers whoare excited to learnwhat s new in the product I m excited to walk usthrough our agenda today including a demo of thenew capabilities of CloudRun for Anthos thatwe ll explore laterin the presentation I m Micah Baker  Product Managerfor Cloud Run for Anthos and I m joined by mycolleague  Jeenal JEENAL SHAH  Hi  I m JeenalShah  Engineering Managerat Google Cloud My focus is buildingsimplified developer experienceon Managed and HybridServerless Compute Platforms I m very excitedto share updateson Cloud Run for Anthos We have been focused onbringing Cloud Run for Anthosto all Anthos platforms Micah MICAH BAKER  Yeah  I thinkfor a lot of the customers whoare new to Cloud Runfor Anthos  maybe weshould give a littlebit of backgroundon what is Cloud Run forAnthos  what makes it special and maybe some ofthe building blocks Could you tell us alittle bit about Knative JEENAL SHAH  Yes  absolutely So Cloud Run for Anthos isbuilt on top of Knative What is Knative Knative is anopen source platformwith a very strongcommunity thatbrings built inlearnings from Googleand many other companies So what values does it bring So Kubernetes is a raw power It s a ubiquitous platform  butit s not easy for developers Knative was initial step adding convenience and toolingbuilt on top of Kubernetes It provides adeveloper centric APIto stand up scalable  secure stateless services in seconds On top of that  you run Knativeanywhere Kubernetes runs so you never have to worryabout vendor lock in So  Micah MICAH BAKER  Yeah  I lovethat explanation of Knative Thanks so much  Jeenal I think of CloudRun for Anthos aslike the Google supportedmanaged Knative It allows us to give all thepower of Kubernetes that youmentioned  as well asthe benefits of Knative but further create ease ofuse  add more features to it and bring that Google support So it also makesit possible for usto use part of the AnthosEcosystem as needed For example  we useAnthos Service Mesh now which is our managed versionof open source Istio Also because of Anthos  we candeploy Cloud Run for Anthosanywhere that Anthos ServiceMesh can be installed So it gives us alot of flexibilityand enables these hybridand multicloud deploymentsthat our customershave been asking for But Jeenal  can youtell us a little bitabout how thedeveloper experiencein Cloud Run forAnthos relates backto that Knative building block JEENAL SHAH  Yes  absolutely So what we are doing here is weare delivering all the powersof Kubernetes along withdeveloper focused featuresof Knative  but withoutmaking developerslearn either technology Cloud Run for Anthos abstractsall the complex detailsof platform  enabling developersto focus on what matters  that is  writing code andbuilding applications Micah MICAH BAKER  That s areally great outcomefor our developers that they don thave to become Kubernetesexperts or Knative experts They can just focus ondevelopment velocity So I think that s a reallygood background on whatCloud Run for Anthos is Maybe before we continue just a quick pitstop into what is Anthos andhow that Anthos stack actuallyenables these outcomesfor developers So I ll just explain alittle bit about Anthosas an abstraction of Kubernetes So Anthos is aKubernetes platformthat can run on Google Cloud It can run on premand it can alsorun on other Cloudproviders and stillhave a single pane ofglass for a platform teamto manage that infrastructure And then Cloud Runfor Anthos runson top of Anthos further abstractingKubernetes from the developers So it creates aseparation of concerns  platform teamscare about Anthos that s the platform theyrun  and then the developerscare about Cloud Run forAnthos because that stheir lens thatthey see the Cloudand on preminfrastructure through Jeenal  could youexplain a little bitabout how it worksfor a developer A little bit about what their  aday in the life of a developer would be JEENAL SHAH  Absolutely and I m happy to sharewhat all features Cloud Run forAnthos comes out of the box Cloud Run for Anthos is tightlyintegrated with Cloud Builderand under the covers usesindustry standard Cloud nativespecification that allows youto build applications directlyfrom source code and deploy You can also directlydeploy your Dockercontainerised applications You don t need to learnkubectl or Knative commands It is directly integrated withG Cloud CLI or Google CloudPlatform UI The other features that Iam really passionate aboutis talking about gradualrollouts  rollbacks and trafficmigrations  and why I mso excited because itbrings out of the boxGoogle s SRE s best practices Cloud Run for Anthosallow you to roll outnew revisions of yourapplications or servicesyou perform withsimple A B testing canary testing by specifywhich revision should receivewhat percentage of traffic It allows you to roll backto our previous versionsgradually throughoutyour revision or split traffic betweenmultiple revisions And with this  you candirectly either useour continuous deployment orintegrate that with your CI CD It also comes tightly integratedwith Cloud Monitoring and CloudLogging out of the box It provides a metricscollection dashboardso that you can gain visibilityinto your performanceavailability  health of yourservices  and infrastructures Because it uses anOpenTelemetry agent it is possible toalso send metricsto any compatible thirdparty monitoring solutions So we have someamazing features thatcomes right out of the box And  yeah  we will seesome of this during a demo Over to you  Micah MICAH BAKER  Well thanks  Jeenal I love thatdeveloper experience and especially the focuson it works out of the box but there are alsoways to customize And I think that reallyleads well into what sin it for the platform team The platform teamsget all the benefitsof Kubernetes and thestandards that it brings It s just such a richecosystem for platform teamsusing Kubernetes They can take advantage ofthe hardware customizationthrough Kubernetes but not have to worryabout exposing thatcomplexity to the developers There s lots of waysto configure and manageKubernetes and make surethat clusters are conformingto policies  but wedon t need to exposeany of the YAML and otherconfiguration scriptsto the developers So thanks to Anthosand Kubernetes it s possible for the platformteams to target workloadswherever you need them There might be latency sensitiveapplications thatneed to be veryclose to customers and so you might use amix of different providersto make sure that you haveyour workloads running as closeto your users as possible But the developers don thave to think about  oh  am Ideploying my workloadin the correct provider They just say  I mdeploying my workload and it can go automaticallyto the correct places So I think thatthe platform teamgets a full separationof the concernsso that they canfocus on the platform And then the developerteam  the application teamgets to have aseparation of concernsso they can focus onjust the applicationlogic and increasingdeveloper velocity doing lots more A B testing and handling those rollbacksthat you talked about  whichsounds really exciting I think it s a reallygood time for usto give a demo ofsome of these thingswe ve been talking about So I d like to switchus over to someof these new features showingyou what s possible nowthat Cloud Run forAnthos can be deployedon even more destinationsthroughout Anthos So let s take a look So we have severaldifferent Anthos clustersto showcase the hybridand multi cloud deploymentcapabilities ofCloud Run for Anthos The platform team hasalready benefited from Anthosby making it possible torun these GKE clustersacross clouds and on prem So let s look at thesecond half of our storyto see what thedeveloper and operatorexperience is on top of that Our demo starts justas we re finishingrolling out the blue revisionof our blue green app And we just haveone last clusterto deploy to  which isthe GKE on AWS cluster and that will completethe initial rollout So we re doing thingsmanually for demo purposes but all of the thingsyou re going to see herewould be part ofcontinuous deploymentif this were aproduction workload So that means thatthese servicescould be deployed toall of the clusterssimultaneously as part ofyour delivery pipeline So let s get started byclicking this Create Servicebutton  where wecan manually selecta target for the service In this case  we re going toselect that GKE on AWS cluster We ll leave thenamespace as default We ll call it the sameservice as before We ll deploy theblue app revision And I m also going to set theminimum number of instancesto 1 That way this startsright away and wehave an app that we can viewas soon as it s deployed I m going to click Create And now it s pulling downthat container image JEENAL SHAH  So the appURL at the top  Micah if you can highlight that Yep  so that appyou URL at the topis using the IP addressof Anthos Service Meshingress gateway with asimple DNS reflection serviceto provide a quickway to test apps But for production our customerswould use theirown custom domain So  Micah  can you talk aboutthe domain a little bit MICAH BAKER  Yeah  I appreciatethat call out  Jeenal The services deployedby Cloud Run for Anthoshave this format thatyou ll see consistently So it s broken up like this First  you see the service name In this case  it sblue green app And then you ll see thenamespace that we deployed to which is default  And thenthe last part of the URLis whatever yourdomain would be So for production use  youwould have some actual domainname mapped here And there s a setting foryou to tell your clustersthat all services deployedshould use your domainand not the IP addressthat we re using here But this is a quick and easything to do for demo purposes Imagine this being replacedwith your own domain nameif you re deployingthis in the production So it looks like ourapp is rolled out Let s click on that URLand see what happened This is the firstlaunch of the pod so it ll go through that AnthosService Mesh ingress gatewaythat we set up andaccess that blue revisionof the blue green appthat we just deployed And if everything is good we ll see a blue app And here it is The blue app is up andrunning and everythinglooks good from anend user perspective So I ll close this and we ll go back here So now that we ve rolledout the initial revisionthat we were targeting which was the blue app we can go back toour services list and we see thatall of our clustersfor hybrid and multi cloud nowhave this initial revision But now we want to do an A Btest with our green app  whichis the cutting edge  new appin our blue green app product And we re just going to jumpback to our GKE cluster And it s juststarting this processof deploying anew service again But this time  we re going toclick this Edit and Deploy NewRevision button at the top Since we have an existingservice already deployed we re going to tellCloud Run for Anthosthat we d like toadd something to it So in this case  we already hadthe blue app container image I m going to replace that withthe green app container image And is there anything elseI need to change  Jeenal JEENAL SHAH  Yeah  solet s make sure to disableautomatic rollout sowe can use trafficsplitting for granularcontrol of the trafficto a new revision MICAH BAKER  OK JEENAL SHAH  That s the one MICAH BAKER  Right here So  yeah  we want to startoff with 0  traffic to thissince our intent is an A B test That will say create this revision but we re not going toserve anything  right JEENAL SHAH  Correct MICAH BAKER  Allright  let s deploy And now it s grabbingthat container image OK  so that looked easy We can use thisManage Traffic buttonto control the traffic split And notice thatwe re already serving100  from the initial revision which was our blue app Jeenal  what do yourecommend for this demoversus what we would do inproduction for this trafficsplit JEENAL SHAH  Yeah so to make it easierto see the traffic splittingat work  let s go with 50 to each revision What do you think MICAH BAKER  That sounds good Oh  yeah  look at that I changed the new one to 50 and automatically balancedout the percentages 50 50 is great because for demopurposes of the blue green appthat we re doing  thatwill make it really easyto see the traffic split JEENAL SHAH  Yeah Yeah  and for theproduction  of course we wouldn t recommendthat  and would insteadfollow the GoogleSRE best practicesfor the class of applicationand potentially plan a five daygradual rolloutthat begins with 1  and then graduates dailyto a larger percentage MICAH BAKER  Iappreciate the callout to Google SREbest practices  whichwe are certainly notfollowing in this demoif we jump straight to 50  ifthis was production traffic But this will be a greatway for us to just showcasethis traffic splitting So let s see what happened I m going to open up the app And look  we alreadygot the green app I m going to refresh And look at that The first refresh switchedme to the blue app So both of them arerunning  and it s justalternating betweenthe two as it randomlydoes a 50  split forany incoming requests So that s a prettyquick and easy wayto do any kind of A B tests Let s go back here then Jeenal  what do you think weshould do next since we   itlooks to me from an end userperspective like the 50  splitis working Is there anything else thatyou would want to check JEENAL SHAH  Yeah  solet s look at the metricsand log to confirm that whatwe see here  we like it and then progress the rollout Yeah MICAH BAKER  So I just jumpedright to a one hour viewsince that makessense to me of whatwe ve just been working on Yeah JEENAL SHAH  Yeah  andcan we also quickly  or  Micah  can youquickly tell mebefore we go to adifferent tab here that how am I seeing these metrics MICAH BAKER  I wishthat I could claimthat I built thisdashboard myself but I actually am happierto say that this was justsomething that sbuilt into the productand it works out of the box So I could do custommetrics and logging but this is actually CloudMonitoring and Cloud Loggingworking right out of the box So pretty convenient forI can just immediately seethese key details of my apps JEENAL SHAH  Yeah  and ithas all the basic metrics like you said  Micah That looks great Next  can we jumpquickly at logs And I want to see howmy green app is doingand if I see any error or not MICAH BAKER  It looks like it sgiving us success on the mostrecent deployment of this JEENAL SHAH  So it seems likethere are no obvious issues So let s progress the rolloutto the green app to 100  MICAH BAKER  OK That sounds good So I m using that sameManage Traffic buttonthat I used before We re going to justtype in 100 here And it automaticallytakes the blue app to 0 So I m just going to click Save Did my changes And  yeah  lookslike that s done Let s go back Let s go verify this This should no longer show blue So if I refresh  I expect  yeah  now it s green I ll refresh a few more times I m getting nothing but green It s super green OK  that looks likea success to me That was a reallyfun experience I didn t even have tobe a Kubernetes ninjato do any of this I was able to justuse this interfaceto accomplish my goalsand not really thinkabout the Kubernetes part of it JEENAL SHAH  Yep  sothat s the experiencewe are targeting the CloudRun for Anthos  Micah And it s possible to break aglass and customize things but it s also a greatexperience right out of the box MICAH BAKER  That s great So the platform teamgets all this valuefrom Anthos at theplatform level and the application teamgets this additional valuefrom Cloud Run for Anthosat the application level That sounds like apretty great combination That concludes the demo If you d like to learn moreabout Cloud Run for Anthos please visit any ofthe links listed here We re excited to help bothour existing customersand new customers continuetheir journey into Kuberneteswith Anthos andCloud Run for Anthos It s been my pleasureto talk to youand have this discussion aboutthe exciting new featuresin Cloud Run for Anthos Also  thank you so much  Jeenal for sharing this time with me JEENAL SHAH  Thank you  Micah And thank you viewers  for tuning in I hope you enjoy therest of your next 2021 Thank you  MUSIC PLAYING LEIGHA JARETT  Hi  everybody I m Leigha NIKITA NAMJOSHI  And I m Nikita LEIGHA JARETT  And welcometo the Data Cloud live demo If you re just joining usfrom this morning s keynoteand spotlight  you heard someof the latest innovationswe launched Earth Engine  Spark on GoogleCloud  PostgreSQL Interfacefor Spanner  andVertex AI Workbench all to help organizations solvetheir most complex data drivenchallenges In this live demo  we re goingto take complex  fragmenteddata systems and simplifythem  bringing togetherdata on a massive scaleto explore a real worldscenario for climate andsupply chain analytics NIKITA NAMJOSHI  Inorder to do this we re going to needthe whole data team Say hi  everyone BRAD MIRO  Hello  I m Brad DEREK DOWNEY  Hi  I m Derek BRAD MIRO  Now  a quickreminder  this demois truly live and you canengage with us directly So  ask questions  sayhi in the chat window and hit those emojis LEIGHA JARETT  I think Isee some coming in now I see lots of hearts and fire A lot of people sayinghello in the chat Thanks  everyone for joining us today So  if you can t seethe emojis or the chat just go back to thenext event websiteand click on the blue Join TheInteractive Experience button NIKITA NAMJOSHI  OK  let shave a little fun with this Today  you re actually joiningus for our weekly standup at Cymbal Superstore A fictional US basedgrocery chain focused on sourcingfrom local producers Like many differentindustries  weneed to bring togetherdisparate data sourcesto build resilience Specifically for us drought in the Western UShas impacted our producersand our supply chainhas been disrupted withdecreased shipments weekover week We need to better understandhow to manage inventoryto prevent stockouts  and ensurewe can scale in both howwe analyze informationand how we run our transactions LEIGHA JARETT  All right  team So  to address the supplyand scale challengesthat Nikita just mentioned  wehave four different workloads First  I m going to evaluatethe best climate informationto identify at risk producers Then Brad will matureour data pipelinesto transform that data Next  Derek will dig intoscaling our transactionprocessing And finally  Nikitawill show howwe re gaining new insightsby evolving our data sciencecapabilities NIKITA NAMJOSHI Let s get into it And let s keep hearingfrom all of youout there  our data pros Leigha  you re up first LEIGHA JARETT  Great Thanks  Nikita So  in order to gain insightinto our supply chain first  I need toevaluate the risks So  I m focusingon figuring out howdrought impacts the verysource of our grocery products And to do this  I turn toGoogle s platform for Earthscience data andanalytics  Earth Engine which has a huge catalog ofsatellite images for thingslike climate  weather croplands  and much more There s currently over like  50 petabytesof data in the catalog plus more and more imagesare added each day So  let me just goahead and bring upthis cool little applicationthat I built using the EarthEngine code editor So  I can walk youthrough the threedata sets we ll be using today First is going to be CropData from the USDA NASSprogram  which I can grab rightfrom the public data catalogby searching So  this data set is going tocontain one image of the USfor each year  goingback all the way to 1997 And each 10 meterpixel in this imagehas actually beenassigned a crop type Let me just close out of thishere  bring you back to my app So  the second data setcontains the outlineof agriculturalfields  and we regoing to use Earth Engine toassign each one of these fieldsthe crop type that is mostprevalent among the cells thatcover it So  in this view thatI m showing right here the pink fields are alfalfaand the dark green fieldsare almonds NIKITA NAMJOSHI  Leigha  thesevisuals are super helpful but could we exportthis data for analysisin other environments LEIGHA JARETT Yeah  I m actuallygoing to push this intoCloud Storage and BigQueryin just a sec  butfirst  I want to addin some additionalsignals on drought So  the third dataset that I m usingis a collection of bothshort and long term droughtindicators Meaning that we cansee what drought lookslike for each field over time Now  using data sets like thesedoes require some deep subjectmatter expertise So  special thanks toour real world partners at Climate Engine who build solutionson top of Earth Engine sAPI  helping organizationsground their analytics projectswith scientifically acceptedmethodologies OK  so  you just saw howwe use geospatial data And I want to know how youuse it in your analysis So  go ahead and answerthat poll so we can see All right  everyone So  with theselayers in place  wecan start exploringdrought risk Let me just go aheadand zoom to this field And here I m going to showan almond orchard Northeastof Modesto  California And what we re looking at isthe long term drought indicator So  this orange tintthat you re seeingmeans that thisfield has experiencedpretty high levels of droughtover the last five years Now  if I go ahead andjust Zoom out a bit we can actually seethat the entire CentralValley in California ispretty drought stressed So  my last step was tocalculate drought risk for eachof the fields I showed before And this will tell uswhich farms and producetypes are potentially atrisk for not meeting customerdemand  which is just oneconsideration to informCymbal s resilience strategy Better yet  we can incorporatethis geography datawith other spatial stuff  likestore and customer locations that are alreadyinside a BigQuery and anyone who knows meknows that I love BigQuery If you agree  let mesee in the emojis All right  so  theCode Editor hasbeen great for interactingwith Earth Engine but making thesecalculations at scalerequires a different approach So  I went ahead andcreated Earth Engine Tasksfor batch processing  torun these calculationsover all the data in five dayincrements for the past 10years I also set up aCloud function to runevery five days going forward So  that we ll alwayshave fresh data Now  just as a quick recap we jumped into Earth Engineto evaluate Earth sciencedata that indicates climaterisk for our farm fields Now  we ll have freshdrought data availablein Cloud Storage So  just think of thepower of integratinghuge amounts ofspecialized data giving us a complete picture ofsupply risk for our business Oh  and I think I can see thepoll results coming in now It looks like lots of peopleare using geospatial datafor all differentkinds of things And if your answerwasn t in the poll just go ahead and letus know in the chat So  next up  Brad willtransform and push this datainto BigQuery for geospatialanalysis with our existingdata So  Brad  over to you BRAD MIRO  OK  that waspretty cool  Leigha Thanks All right  so  togain more insightinto how drought isimpacting production my main focus has been buildingout the data transformationpipeline to push farm fielddrought indicators from CloudStorage into BigQuery Speaking of batchprocessing  as youknow  to assist with inventorymanagement  price optimization and product assortment we use Sparkto standardize our largescale data processing And we use Google Cloud Dataprocas our long running clusters For you data prosout there  we dlove to know how you reusing Spark today So  let us know while I go aheadand pull up the Cloud Console So  here is a Dataproccluster we currentlyuse to schedule PySpark jobs And here I m connectedto a Jupyter Notebook running on the same clusterwith some PySpark code This code is typicalboilerplate codeyou d write to do basicdata transformations In this case  we are processingthe Earth Engine filesfrom Cloud Storage performing type checkingand mapping to ensure properingestion into BigQuery usingthe Spark BigQuery connector LEIGHA JARETT  Hey  Brad So  this is prettycool  but I thinkhaving a batch workflowthat s scheduledon a continuouslyrunning clusterseems like it canbe kind of wasteful So  is there any way we canmake this more efficient BRAD MIRO  That s afair point  Leigha There is still someoverhead in management here and for hardenedbatch cases like this it would be preferredto have these jobs runon their own discrete resources But good news  with the newserverless option for Dataproc we can submit a PySparkjob without spinning upany infrastructure These automagic capabilities I can spend more dev timewriting code and less timeon managing infrastructure LEIGHA JARETT  All right  Brad First off  I m pretty sure thatyou just set me up for that And second  you definitelymade up that word  automagic BRAD MIRO  Yeah  can I getthe emoji love for that Anyway  I ll now go aheadand export this notebookto a Python fileusing nbconvert OK  and next  let s goand use the Cloud SDKto submit a serverlessPySpark job We can use the DataprocAPI and call batchesto submit a serverless job So  I m showing youhere a PySpark job but I can also submit Sparkjobs written in Scala  Java  R or SQL With auto provisionand auto scale no infrastructureconfiguration is needed This helps those lessfamiliar with howto fine tune theirSpark jobs to utilizeits powerful distributeddata processing model With these capabilities there s no needto manually createintune clusters So  once I vesubmitted the job  Ican show it listed in theserverless Spark consoleand I can click on thejob to view its logs Now  serverless Sparktakes about a minuteor so to autoprovisionthe resourcesas well as startwriting out log output So  what I ll goahead and do hereis show you the log outputfrom a completed jobthat I ran earlier So  if we click inhere  we can see OK  so  I added some printstatements into this job And here  it s showing me thatthere s some short term droughtdata that s processed  somelong term drought data and then somegeographical data as well Now  in addition to this  I canalso show you the Spark historyserver  which is where themore details of the Spark jobitself will be outputted oncethe job has been completed So  we can go back and debug And then additionally we can alsogain access to metadatavia Dataproc Metastore Now  in additionto all of this  weare also lookinginto a new solutionfor creating long runningclusters or a place whereour infrastructure team canmanage Dataproc clusters Now  fortunately  we feel thatSpark on GKE fits our use caseand lets us utilize ourexisting GKE infrastructure So  clearly  it sbeen a busy weekfor me using Spark serverlessto create pipelines to pushdrought data into BigQuery But the efficiencyimprovements we llgain by not needing tomanage cluster overheadwill be worth it Now  you can letme know in the chatif you re as excited aboutSpark serverless as I am NIKITA NAMJOSHI OK  let s see howeveryone s using Spark today Looks like notebooksare pretty popular LEIGHA JARETT  Oh  andautomation through Airflow That s cool too OK  great Thanks so much  Brad So  now that we have greaterinsight into supply risks we need to scale our transactionsystems to handle more ordersand allow us to understandthe potential businessimpact of these climate risks while hopefully lowering costs So  this brings meto Derek  our DBA Derek  you ve beenfocusing on this effort So  can you let us knowwhere you re at today DEREK DOWNEY  Yeah absolutely  Leigha We have a new grocerychain that hasstruggled to maintaintheir on Prem databasesas they re grown There s an opportunity tosolve their underlying scalingproblems with Cloud Spanner I spent the last few weeksvalidating Spanner s abilityto scale and theease of maintenance while keeping strong consistencyfor this point of saleworkload I migrated some of the newchain stores to Spanner already Using granularinstant sizing  I onlyprovisioned a portionof a Spanner nodeto handle the required load As we move theremainder of the stores we can easily scale up withoutdowntime or app changes NIKITA NAMJOSHI Quick question  Derek We ve been reallyfocused on reducingthe cost and complexity ofbuilding scalable applications So  I m wondering how does addingSpanner impact that effort DEREK DOWNEY  Yeah  that sa great question  Nikita One way we ve been doingthis in other environmentsis to standardize on Postgres Postgres is a well establishedopen source databasewith an active ecosystem thatour development and ops teamsare already familiar with I tried to get the teamto add an elephant emoji so y all could show some lovefor Postgres  but anyway Spanner just announceda PostgreSQL interface This gives us thebenefits of Spannerwithout having to completelyretrain our application teams Having a standardAPI for our datawill help us maintainvelocity for new features even as our databaseneeds evolve Here  let me show you So  I ve connected intoour Spanner instanceusing the PSQLcommand line tool From here  I can explorethe schema just like Iwould any other database any other Postgres database You can see that boththe Postgres datatype and the Spanner data typeis available to the informationschema So  this will makeit familiar to anyonewho already knows Postgres LEIGHA JARETT  Awesome Thanks  Derek This is really cool I m really lookingforward to the scalabilitythat this is goingto bring our team DEREK DOWNEY  Absolutely And as a quicksummary  I highlightedhow Spanner allows usto get started smalland scale as needed usinggranular instant sizing And how the newPostgreSQL interfacereduces friction for our team salready familiar with Postgres Nikita and Leigha  you can treatthe existing data as goldenand I ll keeponboarding more stores Nikita  I believe you need thisdata for some of the BigQueryanalysis you were working on You ve been busy figuringout how to evolve our abilityto make sense of all this data Can you show us how that works NIKITA NAMJOSHI Yes  absolutely I was hoping that you d ask So  to ensure thatwe re managing stockto meet customer demand my focus this weekwas on evolving our datascience capabilitiesfor exploratory analysis I combine the transactionaldata that Derek sbeen working on withthe drought datathat Leigha and Bradpushed into BigQuery This way we can gaininsight into whichproducts are most at risk ofnot meeting customer demand So  my first stepwas to integrateall of our transactionaldata within BigQuery primarily using Federationwith Cloud SQL and Spanner This provides aunified environmentfor aggregation and analysiswhere we can join transactionsback to producer details and theassociated drought risk scores Now  as you allknow  we recentlymigrated to VertexAI Workbench  whichhas really  really helped withour basic compute and resourcemanagement DEREK DOWNEY  A quickquestion  Nikita Is Vertex AI Workbench anotebook for Vertex AI NIKITA NAMJOSHI  That is anexcellent question  Derek Vertex AI Workbench containsrecently updated managenotebooks  which bringforth more integrated dataengineering capabilities intoour data science environments So  we can ingest andanalyze data and deployand manage ML modelsall from one spot Now  let me show you a littlebit about what this looks like After I provisioned anew managed notebook my first task was to analyze thedemand on the high volume SKUs And for that  I use the BigQueryconnector to view and querythe sales data With Vertex AI Workbench  Ican inspect BigQuery metadata preview tables  and automatebasic SQL construction entirelyfrom my notebook environment OK  it s my turn on the emojis I want to see the fire emojifrom all the data scientistsout there who areas excited as Iam about being able to accessBigQuery from the notebookinterface Looks like I m not the only one BRAD MIRO  Hey  Nikita Hold on Sorry to interruptyour emoji fest but I see you re workingout of the lakes project but are able to pin otherprojects you have access to like the ops project Is that new NIKITA NAMJOSHI  Yes that s correct  Brad Vertex AI Workbenchactually enablesme to interact with allservices via my own identity The BigQuery plug in alsoprovides some templated codeto help build out queriesand project resultsto  INAUDIBLE  data frame So  here  we can see   oh  lookslike I just need to refresh Sorry about that  team I m just going to refresh myJupyter lab instance here I guess I had itopen for too long LEIGHA JARETT  And I guesswhile Nikita is doing that Like  who uses Jupyter now Let s see the fireemojis lighting up I see some now I personally use it alot in my day to day NIKITA NAMJOSHI  All right Thanks  Leigha So  perfect Now  you can see a simpleview of all transactionsby the day of week And Vertex Workbenchactually also allowsme to launch different kernelsentirely in the same instance So  Brad  I thinkyou were working outof a PySpark notebookearlier  is that correct BRAD MIRO  Yeah So  data Spark clustersare also supportedas a back end as a partof Spark on Google Cloud We can access all supportedkernels on the cluster including PySpark NIKITA NAMJOSHI Well  for my analysis I just needed a Python kernel So  let me show you what sgoing on in this notebookand what I ve beenup to this week You can see here that I vedone a deep dive into the data I ve plotted thetransaction volume I ve created a heatmap of purchase countsacross various departments And I ve also plotted theshort term drought indexacross various crop types like corn  dry beans  pears and rice But my last task  andthe most important task was to map SKUs to suppliersand calculate a risk score basedon the aggregate field data So  to do this  I started withour mapping table  which youcan see a sample of right here This helped me to determinewhich farm fields actuallysource ingredientsfor a particular SKU And then I calculateda weighted risk scorethat takes into accounteach product s demandand the associated farmsoverall drought risk And you can see each of thesemeasurements in this data frameright here Ultimately  this score will helpus to prioritize managing itemswhere we re most at risk ofnot meeting customer demand So  as a quick summary  Iused Vertex AI Workbenchmanage notebooks toconnect back to dataaccessible from BigQueryand create a weighted riskscore for each oneof our products combining both climateand demand data Leigha  I went aheadand put all of this backinto BigQuery  just tokeep it centralized LEIGHA JARETT  Awesome Thanks so much  Nikita That risk score is going tobe huge in helping our teamunderstand supply risk So  to make sure the broaderteam has a trusted view of allthis information  I startedincorporating everythinginto a Looker dashboard Now  with Looker  we defineour metrics  like average risk using Look ML Looker s data model And Looker uses thisto compile SQL queriesand send them back toBigQuery on our behalf so that ournon technical users canexplore the resultsof these data efforts So  let me showyou this dashboardthat I ve been working on And first off  you cansee this custom map layerthat I use to visualizedrought risk across farmregions in the West And we re able to takeadvantage of BigQuery sgeospatial functions anddrill from this aggregate viewall the way to producer oreven grocery product level Let me show you So  I ll drill to field ID And then I can jump rightinto Looker s Explorerto bring in some morefields  like maybe Iwant to specificallylook at productsthat are at risk in this regionwhere we re quickly expanding NIKITA NAMJOSHI  Making all thisdata accessible to the broaderteam is really important So  can we make sure that themarketing team is aware of thisso they know how to hold offon coupons or advertisementsfor these products LEIGHA JARETT  Yeah Actually  we can set upa schedule from Lookerso that the regionalmarketing teams are alwaysnotified if new productsseem to go at risk So  putting thisall together  weused Earth Engine to processnew Earth science data signals We used Spark to efficientlytransform geospatial dataand push droughtindicators into BigQuery Spanner to scale ourtransaction systems  ensuring wehave the infrastructureto handle future growth And Vertex AIWorkbench plus Looker backed by the power of BigQuery to evolve our data sciencecapabilities and surfaceimportant trends backto key decision makers So  with integrationsacross these services anyone is able toexplore and take actionon insights that combineclimate data and transactions Now  we can understandwhere productsmay go out of stock due toincreasing drought conditionsand identify areasto build resilienceagainst changes in our climate So  with thisgeospatial information we re given a wholenew set of data so that we can not only beconsiderate of our customers but also find ways to be moreconsiderate of our planet And with that  it looks likewe re right about at time So  thank you so muchfor joining us todayas we ve shown how GoogleCloud can offer a world classexperience for creatinga scalable  unified dataplatform And a special thanksto my colleagues Nikita for showing usVertex AI Workbench Derek for introducing usto Spanner and its Postgresinteroperability  andBrad for walking usthrough Spark on Google Cloud NIKITA NAMJOSHI  Andthank you  Leigha for showing us Lookerand Earth Engine Last but certainly not least thank you all out thereso much for your engagement We love those emojis andseeing all of your commentsin the chat So  stay tuned forour live Q A  whichis going to cover everythingfrom our spotlightthrough this demo LEIGHA JARETT  Thank you NIKITA NAMJOSHI Thanks  everyone DEREK DOWNEY  Yeah  thanks Bye ANDI GUTMANS  Hi  everyone I m Andi Gutmans general managerand VP of engineering fordatabases at Google Cloud Thank you for joining usfor Google Cloud Next We have some exciting productannouncements to share with youin this session Digital innovatorsare disruptingtraditional industries in newways because of their abilityto use data and software todeliver new digital customerexperiences Customers expect personalizedand seamless experiencesacross their devices  in allplaces  and at all times To deliver these next generationdigital experiences developers need to beempowered to build apps fasterby leveraging fullymanaged databases use their existingskills and investments and build their apps forsuccess with databases thatwere designed to power themost demanding workloadsin the world We offer best in class databasesfor always on applications with global reachand unlimited scale Google Cloud is committed toopen source and open standards We offer managedservices that arefully compatible with the mostpopular open source engines such as MySQL Postgres  and Redis We manage the complexityof running your databaseso that they are simple touse and operate  and makeapp development fasterand easier for you Leading organizations inall regions and industrieshave placed theirtrust in our databases And our databases remainone of the fastest growingsegments of Google Cloud In the newlypublished Gartner 2021 Scorecard  for operationaldatabases  Google Cloudreceived an overallscore of 90 out of 100 which is higher than the scoreof the largest alternativecloud provider Google Cloud met 100  of therequired criteria outlinedby Gartner Also  Gartner predictsthat 75  of databaseswill be deployed or migrated toa cloud platform by next year Imagine 40 yearsof databases placedon premises being supersededby cloud databasesin such a short duration It s amazing Our customers journey to undergodata driven transformationtypically involves three steps The first step migrate  is thereto help customers moveexisting applicationsand databases to the cloud frequently on a tight timeline Customers need afast track approachto lift and shift what they rerunning today to the cloud This as is migration alreadyadds tremendous value even if it doesn tprovide the full benefitsof cloud native capabilities Once migrated  manyof our customersseek to modernize theirdatabase environmentsby transitioning offexpensive legacy databasesand onto open source databases For customers looking to buildnext gen applications entirelyin the cloud  they rein the transform phase This is about unlockingnew possibilitiesin competitive differentiation It s about delivering newtransformational customerexperiences and innovativebusiness models For your existing databasesthat you need to easily migrateto a fully managedcloud service we offer Cloud SQL  withfull compatibility for MySQL Postgres  and SQL Server It is one of the fastest growinglarge services in Google Cloud Cloud SQL offerseasy integrationwith existing apps andGoogle Cloud Services like Google KubernetesEngine and BigQuery More than 650 000 GKE podssecurely connect to Cloud SQL And BigQuery usersquery over 125 petabytesof data in Cloud SQLon average in a month using our federatedquerying capabilities We ve invested heavilyin improving Cloud SQLand making it enterprise readyfor your demanding workloads In the past 12 months  wehave introduced capabilities such as cross region replicas point in time recovery customer managed encryptionkeys  VPC Service Controls and IAM support We have significantly improvedthe maintenance experiencefor customers and giventhem maintenance controls Cloud SQL maintenanceis  on average 80  shorter than itwas 12 months ago And maintenance forMy SQL and Postgresis now shorter than thatof competing offeringsfrom other cloudproviders  basedon figures published inonline documentation asof August 2021 Earlier this year  we launchedindustry leading observabilitywith Cloud SQLInsights for Postgres It empowers developers toaddress database performanceissues themselves atno additional cost This has become oneof the fastest adoptednew capabilities in CloudSQL  and we look forwardto deliveringequivalent functionalityacross more database engines In addition  we recentlyintroduced Cloud SQL CostRecommenders with ActiveAssist that empowers developersto better manage costs We have made migrations toCloud SQL easier and fasterwith the DatabaseMigration Service This offers a simple experiencewith minimal downtimefor reliable and predictablemigrations to Cloud SQL It is serverless secure  and offeredat no additional charge More than 85  of all migrationsare underway in under an hour and a majority of customersare migrating databasesfrom other clouds Database MigrationService currentlysupports MySQL andPostgres databases with support for SQL Servermigrations coming soon For customers bringing Oracleworkloads to Google Cloud we offer our bare metalsolution for Oraclethat allows you to run Oracledatabases the same way youdo it on premises but cost effectivelyand with less than 2milliseconds latencyto all of your GCP services Bare metal solutionenables you to continueusing your existing Oracletechnologies  such as RAC Data Guard  and RMAN For workloads that requiremicrosecond latency we offer Memorystore Memorystore is fully compatiblewith Redis and Memcached and offers an easy migrationdestination for your Redisand Memcached workloads Memorystore is a scalable secure  highly availablein memory service that is usedby 90  of the top 100 Cloudcustomers We recently announced apreview of Datastream a serverless change datacapture and replication service With Datastream  you caneasily and seamlesslydeliver change data streams fromyour Oracle and MySQL databasesinto Google Cloud Services  suchas BigQuery  Cloud SQL  GoogleCloud Storage  and Spanner The serverlessarchitecture of Datastreamtransparently scales upor down  as data volumesshift in real time Now let s discuss ourtransformative databasesand how they can help youtransform your businessmodels and the digitalexperiences for your users Today  your business criticalapplicationsmay need to serve millionsof users across the globeand process data volumes thatare an order of magnitudehigher than justa few years ago New  always on business modelsrequire these applicationsto be reliable  secure and continuously available Your applications need toscale seamlessly and sustainhigh performance  even duringunexpected spikes in demand We give you access to the samedatabases that power the mostdemanding Google applications like Search  YouTube  Gmail Maps  and Payments Google had to inventthese databasesbecause traditionaldatabases were notdesigned to handlethese demands for scale availability  and performance We offer not one  but threesuch transformative databases  Spanner  Bigtable and Firestore All three of themhave a common setof highly differentiatedcore attributes like industry leadingfive 9s availabilitySLAs  no maintenancedowntime  unlimited scale automatic sharding zero touch global replication automatic failover superior price performance and enhanced securityand compliance Spanner is our fullymanaged relational databasethat offers thefamiliarity and developerexperience of arelational databaseand the scalabilityof a NoSQL database Spanner enables unlimitedscale  so you can easilyaccommodate future growthwithout painful architecting adjust your capacity on the fly and pay for only what you use It allows you to deliverhigh performance transactionswith strong consistencyacross regions and continents so you can simplifyoperations and place dataclose to your users Our enterprise customers infinancial services  gaming retail  technology  andmany other industriesrun some of their mostdemanding operational workloadson Spanner With Spanner  youcan spend less timeworrying about how toscale your databaseand instead focus onscaling your business As wonderful andunique as Spanner is it wasn t the most accessibledatabase for developers We wanted developers to be ableto start small with Spannerso they could design theirapplications for successfrom the get go To this end  werecently introducedmore granular instancesizing for Spannerto democratize access to it With more granularinstance sizing you can start at 1 10 thecost of regular instances equating to approximately 65 per month This means you canstart small and easilyscale your Spanner instanceas your workload grows with no downtime Developers love the scaleand availability of Spanner but they wanted to takeadvantage of Spannerusing their familiar open toolsand their existing skill set Therefore  to continue thistheme of democratizing accessto Spanner  we reexcited to announcethe preview of a Postgresinterface for Spanner starting today This empowers developersto take advantageof Google s transformativerelational databaseusing standard tools and skillsfrom the popular Postgresecosystem This interface combinesthe power and reliabilityof Spanner with the familiarityand portability of Postgresto enhance flexibility By providing aPostgres interface we re democratizing accessto Spanner for millionsof developers and ISVs Starting today  you cancreate Spanner databases thatuse PostgreSQLdialect and connectwith its OpenWire protocol This interface supportsSpanner s rich featureset using most popular Postgresdata types and SQL features Postgres interfacefor Spanner isavailable at noadditional charge What this means is that foryour Postgres workloads we offer you not one  buttwo fully managed services For maximum compatibilitywith open source Postgresand for the easiestlift and shift migrations you have Cloud SQL for Postgres For your mostdemanding workloadswhere you need unlimited scaleand five 9s availability SLAs we offer Spanner with itsnew Postgres interfacethat allows you to leverageyour skills and investmentsin the Postgres ecosystem We re super excited aboutour rich Postgres offerings Now for apps that requireconsistent single digitmillisecond latencies and needto support millions of requestsper second  we offer Bigtable Bigtable is our fullymanaged NoSQL databasefor large operationaland analytical workloads And it s HBase compatible Bigtable is uniquely capableat powering use caseslike personalization atscale and fraud detection We re excited to share thatautoscaling is coming soonto Bigtable This will allow Bigtable toautomatically add or removecapacity in response to changingdemands of your workloads so you ll only pay forthe capacity you need We talked about theimportance of the velocityof your application iteration For rapid appdevelopment  we offerFirestore  our fullymanaged scalableserverless document database Firestore is belovedby developers becauseof its ability to serveboth as a document databaseand a backend as a service Firestore enablesoffline supportwith real time synchronization An example of howmuch developerslove Firestore isthat Firestore hasmore than 250 000 monthlyactive developers and Firestoreapps power more than 750million monthly active usersusing Firebase authentication Not only is Firestoreextremely developer friendly but we re also focusingon making Firestoremore enterprise friendly We re introducing anumber of enhancementsto Firestore for greaterenterprise readiness With that  I d like to introduceone of our most innovativecustomers who hassuccessfully accomplisheda data driven cloudtransformation It is my pleasure to inviteKumar Menon  SVP of data fabricand decision sciencetechnology at Equifax Kumar  thank you forjoining us today We would love to hearabout Equifax s innovationthat you and your team have beendriving in partnership with us KUMAR MENON  Thank you  Andi I wanted to start off by givingyou some background on Equifaxand the transformationwe are under Equifax is a leading dataanalytics and technologycompany and is one of the threebiggest credit rating agencies We are present in 24countries worldwide In 2018  we embarked ona massive company widetransformation tomodernize our technology This was a result of anunfortunate breach incidentin 2017  which broughtthe global spotlightonto our company Due to the oversight thatwas placed on us and the needfor us to ensure we are goodcustodians of the high fidelitydata we have  we realizedthat patching and modifyingour legacy systems would notbe a sustainable solution We made the bold decision toadopt a cloud first strategyand rewrite all ofour data platformsand associatedapplications  leveragingcloud native architecture Our legacy systemswere operatingin silos  which made itdifficult to effectivelyinnovate and monetizeour differentiated dataassets through newmulti data products So we also took thisas an opportunityto reorganize our data under acommon enterprise domain model onto a singleglobal data fabric with the security andautomated governance bakedinto the architecture The goal was to make it easierfor our data and analyticsand product teams toinnovate and delivernew productsfaster  thus helpingour customers and consumersmake better decisions Here s what our end to enddata flow looks like The data fabric is at the heartof the Equifax value chain If you look at thisillustration from left to right the data fabric helps us ingestdifferent domains of datawe bring into our ecosystemin a consistent  reliable highly performant  governed compliant  and secure manner so that our product  data andanalytics  and business teamscan leverage the synergiesof all the data assetsto innovate and deliver productsthat our customers need In short  we want globaldata at our fingertips Equifax Cloud will enable ourproduct innovation  growth and competitiveness The data fabric platform ispivotal to the Equifax businessmodel going forward Why is the Equifax datafabric a gamechanger Let me give you a few examples Equifax has moved from80 plus data silos enabling multiplesources of datato be standardized and organizedinto a seamless data fabricwith logical separationin governing groups The structure drasticallysimplifies our architectureand gives Equifax theability to act quicklyto address customer regulatory  and security needs We will achieve bolt onmergers and acquisitionswith better and fasterintegrations than ever before accelerating ourgrowth tremendously Now let s look at a moretechnical view of the datafabric and see how we haveleveraged GCP managed databaseservices to deliver ouranalytics and operationalworkloads Let me break those downfor you from top to bottom The data fabricservices shown herehelp curate theraw data we collectacross the various domains  suchas credit  employment  wealth utilities  et cetera to name a few We then key andlink the data  whichis a form of entityresolution processthat helps us identify a singleunique instance of an entity  for example  a uniqueconsumer identity  across all the domainsof data we collect This data is then madeavailable for predictive modelsand real time decisioning as aset of purpose specific viewswith the regulatoryrules applied thus ensuring permissible use This low latency high value ecosystemleverages Cloud BigTableas the core Datastoreand the pipelines arebuilt in Cloud Dataflow The Cloud BigTablescalable architectureand its support forcustomer managed encryptionkeys help us provide aresilient  reliable  and securedata platform to support highlystringent millisecond SLAsthat our customersdemand from us The data fabricis tightly coupledwith our Ignite and ouranalytics services  whichis used by our data scientistsfor feature engineeringand building predictivemodels  whichcan be deployed intoour decisioning systems facilitating a fullML ops ecosystem The data lake that powersIgnite is built on BigQuery Our enterprise catalog servicesare built on Cloud SQL This is what you see inthe middle of this picture It houses all of the metadatafor our data featuresand models  plus the rulesthat are applied on our data This provides the auditabilityand the automated governancecapability This design gives Equifaxa leading positionas a data analyticsand technology companyin a highly regulated space Finally  I wanted to leaveyou with some metrics of wherewe will be when we are finallymigrated onto this platform You re looking at 75 plusdata fabric services deployedin seven regions supportingthe 24 countries we operate in We would have 10 billion plusof observations keyed using almost 90 000 plusclicking and linking rules enabling us to manage 800million plus unique identitiesglobally We are well into our journeywith significant productiontraffic coming soon We have an aggressiveplan to growthe company via acquisitionsand organically The partnershipwe ve had with Googlethrough this entirejourney has been great Both our engineeringteams have beenable to collaborateclosely to achievethese capabilities andthe seamless accessto the product engineeringteams at Google have been key We have invested in a long termrelationship with Googleand look forward to asymbiotic partnership Thank you for havingme  and hope youguys found thispresentation useful ANDI GUTMANS  Kumar thank you for sharingyour story of data drivencloud transformation It s truly inspiring We look forward to ourongoing partnershipwith you and Equifax At heart  Googleis a data company When it comes tomanaging data  wehave to solve some of themost difficult problems We re imagineddatabases and inventedSpanner  a databasethat gives youthe best of relational andnonrelational databases Our databases benefit fromdisaggregated computing storageand our privately ownedglobal networkto improve performance scalability  security and availability What all this means is that whenyou adopt our Cloud databases you immediately benefit fromour longstanding cultureof engineering excellenceand innovation There is a lot moreexciting contentwaiting for you inthe upcoming sessions I invite you to join thesesessions to learn moreabout Google Cloud databases And I would like tothank you for joining usat Google Cloud Next We hope to partnerwith you to accelerateyour data drivenbusiness transformation Thank you ALOK JAIN  Hello and welcometo What s New in Anthos In this session  we will goover exciting new additionsto Anthos  which willmake it even more easierfor you to manage and secureyour multi cloud and hybridapplications We have been working hardto add more features expand reach  and simplifythe experiences to make Anthosthe best applicationmodernization platform I m Alok Jain And I lead Product Managementfor Anthos at Google Joining us in this sessionis my colleague Lisa Shen LISA SHEN  Hi  everyone My name is Lisa Shen And I am a GoogleCloud product manager ALOK JAIN  Beforewe dive deep  here sa quick introduction forthose who are new to Anthos Let s go back a few years We created Kubernetesto maximizeproductivity of our owndevelopers at Google And then we open sourced it tohelp others achieve the same To make Kubernetesproduction ready we created GoogleKubernetes Engine  or GKE  as we call it  the best way toconsume Kubernetesas a reliable  secure and fully managed service In 2019  we introduced Anthos a managed platform designedto simplify themanagement of Kubernetescluster or any publicor private cloudby extending a GKE likeexperience along with our bestopen source frameworks Anthos extends Googleengineering practicesto your environment  so that youcan modernize your apps fasterand establish operationalconsistency across themfrom a single pane of glass It helps you minimizeoperational overheadas you scale your infrastructureand helps you reducethe total cost of ownership Containerization offers a commonapplication packaging formatacross differentinfrastructure types And Kubernetes offers consistentAPIs and control mechanisms and thus can be used forapplication deploymentson your existingvirtual machines or bare metal servers  or evenon other public clouds like AWSand Azure  besides Google Cloud Anthos is made up ofa few core components Infrastructuremanagement providesa reliable and efficientway to deploy and runcontainers and manage themacross any environment Next is applicationservice management  it provides managed service meshto connect  manage  and securecontainers We extend thecapabilities of Anthosto run serverlessapplications as well It also has a wide selection ofproduction related developmentstacks and services Operation management isan integrated loggingand monitoring service foryour container  services and applications Policy management andsecurity managementare central to manageconfiguration and securitypolicies across your fleet And it helps you reduceyour operational cost  at the same time  enforcesecurity posture that sright for your applications With that  we are nowready to jump rightin and talk about the new andexciting additions to Anthos Anthos MultiCloud allowscontainer orchestrationstandardization across cloudsvia a new managed service With this newservice  we ve alsoadded support forAnthos and Azure Both of these are inpreview with GA coming rightaround the corner So now you can deployGoogle Kubernetes clusterswith GCP  AWS  and Azuredirectly from Google Cloud You can control all clusterlifecycle operations centrally You can review systemand application logsand monitor clustersacross all cloudsthrough a single pane of glass Cluster lifecycleoperations can be controlledvia G  Cloud  API  or Terraform allowing for a completelyautomated workflow As a result  you getconsistent  unified and secure cluster andcontainer managementon a cloud of your choice In addition to centralizedlogging and monitoring you benefit fromour connect gateway which allows for Google Cloudto be the central accesspoint for each cluster We ll talk moreabout it in a minute After you ve deployed Kubernetesfabric in your environment the next step is to deploycluster level configurationpolicy And finally  application specs  choosing Anthos configurationmanagement service you can send GKEand non GKE clusterto a Git repo thatdefines the guardrailsfor your organization The repo can include configmaps  network policies SRE agents  and so on Policy configurations canalso be located in the repoto enforce best practicesor custom rules setby your organization For example  you canset policies to activelyblock non compliantAPI requests or simplyto audit the configurationof your clustersand report violations such as applicationswithout proper labels Configuration specscan be appliedto all clusters or just a subsetusing Cluster Selector Object Any changes to theconfig in the repowill be observedautomatically by the cluster And besides GKE  Anthos supportsCMC of confirmed Kubernetesclusters like AKS or EKS giving you the optionto keep your application runningon your favorite Kubernetesand still take advantageof Anthos services Next is Anthos on bare metal It lets you deployKubernetes clusters directlyon your own servers  givingyou the best performanceand flexibility options You have direct control overthe application scale  security network latency by runningcontainerized applicationon GKE and benefit from Anthoscomponents running locally We have made a numberof announcementsrecently to Anthoson bare metal such as providing Containerdas a default runtime adding registrybuilding that helpsyou insulate from any serviceoutages over the networkand helps improve the security It will also reduceminimum system requirementfor running Anthos on bare metalby over 50  for edge use cases Now you will only needone of the two PCPUsand only fourgigabytes of memoryto run Anthos on bare metal That means morecompute and more memoryavailable for your applications It reduces the overall TCO And you have many edge locationswith a small footprint Anthos on bare metal bringsGoogle Kubernetes Engineto optimized centers with broadsupport for vSphere platforms including the lateststable release 7 0 We have rolled out a numberof features on Anthos VMwarethis year For example  with the ClusterBackup and Restore feature you can now set upadmin plus 10 backupsto perform automatically beforeand after cluster creationor upgrade for userand admin clusters You can also performan on demand backupwhenever you deem it necessary Another feature is UserCluster Autoscaling which resizes thenumber of nodesin a given node pool basedon demand of your workloads You don t need to manuallyadd or remove nodes or evenoverprovision your node poolsto guarantee availability  and thereby adding to the cost Instead  you can specifya minimum and maximum sizefor the node pole and the restis taken care automatically Last year  we announced supportfor Windows server containerson GCP  which lets you takeadvantage of containerswithout putting inapplications to darknet codeor rewriting them We have now enabled supportfor Windows server containerand Anthos for VMware embeddedin your on prem environment By running Windows and Linuxworkloads side by side you get operationalconsistency and efficiency There is no need to havemultiple teams specializingin differenttooling or platformsto manage different workloads Mixed workloads can beconfigured on a single clusteras well  giving youeven more flexibility The ability to manage policiesfrom a central control planefurther simplifies themanagement experience while bin packing multipleWindows applicationsdrives betterresource utilization leading to infrastructureand license cost savings We have recently made AnthosConnect Gateway generallyavailable for our customers It provides a singleGoogle hosted front load allowing for customers touse their existing toolingto effortlessly accessAnthos connected clustersrunning anywhere It solves key pain points wehave heard from many customers deploying on prem  at theedge  or even on other clouds Accessing Kubernetesclusters can be a challenge And they often resort tousing jump hosts or VPNsto these clusters Connect Gateway simplifiesthe connectivity challengeby using the same infrastructurethat powers existing connectedAnthos UIs  The serviceauthenticates and authorizesGCP users and forwards theirrequest to the Connect agentrunning in the cluster  andultimately  the cluster sKubernetes API server As a result  you get aconsistent and secure wayto use your existingKubernetes toolingto perform common operationsagainst any fleet registeredKubernetes cluster A key building block for Anthosinfrastructure managementis our software definednetworking as the capabilities in Anthosextend the full networkingstack from layer 2 to layer 7 And it s built onbest in class Kubernetesopen source infrastructure We have extendedthe capabilitieswith innovations such asnode network firewallsto protect workload atinfrastructure level Load balancing for Kubernetesworkload is bundledin by taking in solutionslike MetalLB and extending itfor layer 4  layer 7 use cases  as well as integratingBJP support into it We also enableconnectivity workloadsacross hybrid environments witheasy to deploy VPN solutionand provide consistentobservability and affordabilitytools There are two importantfeatures that wehave introduced in Anthosnetworking stack this year Anthos Dataplane v2 is optimizedfor Kubernetes networking It is an opinionateddata plane thatleverages Cilium  anopen source project thatmakes the Linux kernelsKubernetes aware using AVP app It delivers a unifieduser experiencefor configurations deployments  and monitoringof networking features acrossGoogle Cloud  hybrid  and evenother cloud environments It makes parts The first cluster isa networking ecosystemwith better performanceand visibility Another key feature isAnthos networking gateway which provides a numberof advanced networkingfeatures for hybrid andmulticloud environments For example  EgressNAT enables youto have a deterministicIP for egress trafficwith namespace andlevel granularity With multi clusterconnectivity  youget direct part reachabilitybetween environments Well  I hope you re asexcited about these additionsas we are And wait  we have alot more to talk about I ll invite Lisa to sharemore exciting capabilitiesthat we ve added to Anthos LISA SHEN  Thank you  Alok Now let s take a look at AnthosConfiguration Management It enables you to automaticallydeploy shared environmentconfigurations and enforceapproved security policiesacross Kubernetes clusterson premises  on GKE and in your otherpublic cloud platforms As part of AnthosConfiguration Management we have recently announceda new feature availablecalled Config Controller a hosted serviceto provision and orchestrateGoogle Cloud resources The service offersan API endpointthat can provisionand orchestratemore than 120 GoogleCloud resourcesthe same way it managesKubernetes resources with continuous monitoringand self healing As a hosted service you don t haveto install or manage thecomponents of the ConfigController or be an expert inKubernetes resource management because Google Cloudwill manage them for you Config Controllerprovisions infrastructure applications  andcloud services configs them to meetyour desired intent and monitors them forthe configuration Git Config changes are aseasy as a Git push and can be easily integratedwith your developmentworkflows Another exciting new featurewe ve introduced this yearis the ACM Multi Repo Support Enabling Multi Repo modelets you sync configurationsfrom multiple repositoriesto the same set of clusters An example is shownin the diagram here The platform admin managesthe centralized infrastructurefor the organizationand enforces policieson the cluster and on allnamespaces in the org The application developers whoare responsible for managinglive deploymentsapply configurationsto applications in thenamespaces they work on The Multi Repo feature decouplesthe config deployment lifecyclefor different teams It provides you with moreautonomy and flexibility And you get to choose whereyou want to place the repoand how to structure it Microservice architecturespresent numerous benefitsbut also introduce challengeslike added complexityand fragmentation fordifferent workloads Anthos Service Mesh  which isa Google fully managed service simplifies servicedelivery across the board from traffic management andmesh telemetry to securityand communications betweendifferent services We have many exciting newfeatures rolled out in ASMthis year And all of these are Anthosvalue add over the Istioopen source project With the managed ASMcontrol plan on GCP Google is responsible for theavailability  scalability and the security ofthe control plane We also have managedcertification on GCPand managed proxy updatesbased on the release channels Both SRE dashboard withservice level objectivesand the SecurityInsights dashboardare available to giveyou in depth visibilityinto the application services With the applicationSecurity Insights dashboard you will have a 360 degreeview of workload security You get a summary of thesecurity configurationwith the workload drilldownview to troubleshootsecurity issues In addition  you can nowadd Google Compute EngineVMs in managed instancegroups to a service mesh You ll get the sameobservability  telemetry and securitycapabilities as you wouldget from runningservices on Anthos GKEclusters in the mesh Anthos Identity Service is anauthentication proxy for Anthosthat enables customersto authenticatetheir users toAnthos environmentusing their existingidentity solutions AIS simplifiesidentity managementacross hybrid andmulticloud deploymentsby providing a commonauthentication abstractionlayer for all theAnthos environments As a result  you can leverageyour existing identityinvestment with Anthosusing standard identityprotocols such as LDAP orOIDC  or you can consistentlyapply your policiesacross the environments It simplifies yourcross environment workloadmigration Anthos Workload Identity featureenables Kubernetes workloadhosted on any clusterto authenticateto Google Cloud or otherservices in a consistent way We have extended GKEworkload identityconcept to Anthos platformson prem and in multiclouds The Workload IdentityFederation featureeliminates the need forlong lead credentialsand replaces them withshort lead  autorotatedcredentials It enables consistentauthenticationacross all the clusters The feature not only helpsapplication developersto connect applications toessential services easier but also helpsthe platform adminto maintain the governanceacross the clusters On the applicationmanagement side I m very excited that wewill have Anthos HybridCI CD available for publicpreview in November this year Built on top of theexisting Google CloudBuild and the GoogleCloud Deploy solution Anthos Hybrid CI CD will allowcustomers to build and deploycontainerizedapplications seamlesslyto hybrid and multicloudenvironments usinga single control plane Hybrid Cloud on clusterCI CD is ideal for customersthat are prohibited fromconnecting their source controlor artifact repos to GCP dueto organizational  or network or security requirements More specifically  thiswill allow customersto run CI CD workloadswith their private networkwith simplified networkconfigurations that don trely upon network peering In addition  the on clusterCI CD also reduces the latencyfor accessing local resources Another popular tool includedin the Anthos productis Migrate for Anthos and GKE Migrate for Anthosand GKE intelligentlyextracts  migrates and modernizesapplications to run nativelyon containers in GKE and Anthosclusters It makes it easy andfast to modernizetraditional applicationsaway from virtual machinesand into native containers You can migrate VMs runningon VMware  AWS  or Azureinto containers managedby Anthos in real timeand capitalize on increasedresource utilization  unifiedlogging and monitoring  andmodern application lifecyclemanagement tools A recent featurethat we ve introducedis called the MigrateFit AssessmentTool  which helps customersdiscover their inventoryand assess fit forcontainerization It replaces the existingMigrate Discovery Tooland it runs completelydisconnected from the internet In addition  Migratefor Anthos and GKEnow supports enhancedcontainer runtimessuch as Anthos Cloud Run Cloud Run is a managedcompute platformthat enables you to runstateless containers thatare invocable by webrequest or Pub Sub events The enhancements let youdeploy your migrate containerworkloads on Cloud Run You can learn more aboutAnthos via Google Cloud Productwebsite or explore our powerfulmulti cluster Kubernetesmanagement features withthe Anthos sample deploymenttutorial When you are ready  be sureto schedule your applicationassessment workshopwith your Google teamto begin planning yourapplication modernizationjourney Thank you for watching the show And I hope you have a great day ALOK JAIN  Thanks  Lisa And thank you  everyone for being with uson What s New With Anthos We hope you enjoy the restof the Google Next 2021 STEREN GIANNINI  Hi  I amSteren Giannini  Senior ProductManager at Google Cloud Welcome to  What sNew in Serverless  Today  we are mainlygoing to talk about twoof our Serverless products Google Cloud Functions  whichis Serverless functions and Google CloudRun  which is Serverlessapplications and containers First  I want to callout that Serverlessis available in everyGoogle Cloud region You will find Clarin andCloud Functions in allof the Google Cloud regions And you can expect us to open upthose products in future GoogleCloud regions All right Let us dive intoCloud Functions So  first  we ve beenhard at work deliveringa new runtimes for CloudFunctions  new programminglanguages The first one is Ruby then is  NET Core  PHP  andwe ve also been updatingour existing runtimes withPython 3 9 and Node js 14 and even 16 in preview Next  we addedmore customizationto the build process So you might know that when youdeploy your function on CloudFunction  the firststep that happensis that this function isbuilt before it gets deployed So you can nowcustomize where thatbuild happens by targetinga private worker pool This allows you tocustomize the machinetypes on which thefunction will be built or to ensure that the functionis built within your VPC SCperimeter We ve also added theability to definebuild environment variables So these en vars are en varsavailable at build time They  for example  helpyou configure the buildbackbehavior And  finally  we delivered Mininstances to Cloud Functions So we ve been hearing from youthat cold start is a problemwith function as a service So what is a cold start When your functionreceives no traffic its number or functioninstances are scaled to 0 But then when trafficarrives  Cloud Functionswill scale from 0to 1  and the timeit takes for thefunction to startis what we call a cold start Thanks to Min instances  youcan simply define a valuethat Cloud Functionswill scale down to keeping one or moreinstances warm so that when the nextrequest comes in there are already some warminstances to process them So when not in use those warm instancesare charged for memory  butalso for CPU at 10  the price To use Min instances  you cando so via a simple command lineflag or user interface inputs Right now  I wouldlike to call outtwo of the GoogleCloud orchestrationproducts that pair very wellwith Cloud Functions and CloudRun The first one iscalled Eventarc So Eventarc allows you toasynchronously deliver eventsfrom Google Servicessoftware as a service and you own apps toServerless products Eventarc has addeda few new features  the ability to target CloudFunctions  the abilityto receive eventsfrom Cloud Storage and a new standaloneuser interface thatallow you to manageyour Eventarctriggers in a central place Second  I want tocall out Workflows which is Google Cloud productto orchestrate automate GoogleCloud and HTTP based APIservices with a workflowthat you define So on the right  you cansee a screenshot of the userinterface where the workflow isusing a comprehensible syntax but can also be visualizedin a very nice graph So Workflows has addeda few new features First  the ability to useHTTP callbacks  the abilityto connect to manyGoogle Cloud APIs It has increased the memory as well as the numberof concurrent executions And there were many syntaxand library enhancements All right Now let us dive into Cloud Run So  first  I wantto call out that wehave added committed usediscounts to Cloud Run So committed usediscounts allow youto commit to an early spendon Cloud Run for a year and you would   you simplyreceive a 17  discounton this commitment Of course  usage abovethe committed amountis charged at normal rates This is self service From the Cloud Console you go into Billing you click Commitment and this iswhere you can purchasethe commitment for a year I should note that committeduse discounts applyto all Cloud Runservices of your accountsacross all of your projectsfor a given region Let s take an example On the right  we can seean example Cloud Run billover time This  basically  reflects theCloud Run usage over time As you can see  because most ofthe time  the bill is above  1 it makes it very economicalin that situationto commit to a  1 perhour usage  because thatmeans that you would onlypay  0 83 for every hour And of course  if usagegoes above the commitment for example  because youused  1 50 of Cloud Run then you would only paythe discounted on the  1 00and  0 50 above the dollar Now  I want to tell you how wehave been improving the CloudRun developer experiencewith three things followingthe development journey First  we have added localdevelopment to Cloud Run Second  we are making it easierto deploy your source code And third  we improvedthe observabilityof your Cloud Run services Let s take a look Local development   this allowsyou to run your Cloud Runservices in a local emulator This emulator is availablein the gcloud command line in the Cloud Code for VSCode in Cloud Code for IntelliJas well as inCloud Shell Editor the online IDE of Google Cloud If we take anexample  you can seethat you can store your CloudRun config in a file thatwould allow you to definehow the emulator shouldbe configured Then  using a simple command gcloud beta code dev you start a localdevelopment environment thatwill emulate thecharacteristics of Cloud Run with the CPU andmemory allocationthat you defined withthe environment variablesthat you defined And it will watch your localsource code for changes and when they happen will rebuild theminto a container that   andrestart the local server So quite handy fora fast developmentgroup instead ofdeploying to the Cloudfor testing new changes Then  once you havedeveloped your service you want to deploy it We have added supportfor deploying source codeto Cloud Run This is  basically  a command a very simple command in three words that will build push  and deploy your servicefrom local source code This command isgcloud run deploy That s right That s the defaultfor gcloud run deploy It s to build your localsource code using Cloud Build push it to ArtifactRegistry  and then deploy itto Cloud Run This command is also leveragingGoogle Cloud Buildpacks in order to allow you todeploy source code in popularlanguages without anyDockerfile  that includes Go Node js  Python  Java  and  Net But  of course  if a localDockerfile is present then this commandwill simply build itwith the instructions thatare in the Dockerfile and deploy the arbitrarycontainer image to Cloud Run Lastly  we added moreobservability to Cloud Run First  by deliveringa few more metrics notably  the instancecount metric which would show yousimply the numberof active and idle instancesfor a given Cloud Run service Then  we ve walked withthe Cloud AR Reporting teamto make sure AR reportingwas able to capturesystem errors out of the box So without anymore configuration that means that  if your CloudRun container instance is outof memory  or ifCloud Run is notable to scale because it isreaching its max instancelimit  those errors thatare present in your logswill now also be aggregatedin Cloud error reportingand displayed in a veryactionable way in the Cloud Runuser interface And lastly  we have collaboratedwith the Cloud Trace teamto provide out of the boxrequest tracing That means that you can analyzethe latency of your Cloud Runservices using Cloud Tracewithout any instrumentationneeded All right  now  I mgoing to tell youhow we have made it easier torun more workloads on CloudRun with three changes First  giving you more controlaround the CPU allocationof your service Second  by delivering anew execution environment And third  by pushingthe limits thatwere historicallimits of Cloud Run Let s take a look CPU allocation  let s take an example Here  you can see one containerinstance of a Cloud Runservice It is starting Then it has started It receives requests potentially multiple requestsat the same timethanks to concurrency And then at the end of itslife  it gets terminated Today  the CPU forthat container instancewould only be allocated when thecontainer instance is starting or when it is receivingat least one request As a customer  youwould also onlybe charged when theCPU is allocated And  in particular  you arecharged for CPU and memory but also you are chargedevery time a request arrives We are introducing theability to opt out of this CPUthrottling by simply sayingthat you want the CPU to alwaysbe allocated to yourcontroller instancesas long as theinstance is alive So in that case  you wouldbe charged  of course for the entire lifetime ofthat continual instance But the CPU will always beavailable to that containerinstance You can enable thisvia simple command lineflag on an existingor a new serviceas well as a simple checkboxin the user interface So this new option toalways have CPU allocatedallows you to run backgroundtasks or async processingafter you havereturned requests It makes Cloud Run morecompatible with monitoringagents like OpenTelemetry Many programming languagesdo things asynchronously like Goroutines  or async or threads  or coroutines These are now morecompatible with Cloud Run And lastly  you candeploy Spring Boot appsthat are expecting todo background things And as I said  when you optin to this new CPU allocationto always haveCPU allocated  youare charged for adifferent price There is no request fee And the CPU andmemory is 25  lower You can couplethis always on CPUallocation with Mininstances  basicallyallowing you to havea number of instancesthat are always scheduled and which always have CPU and looking at some quiteinteresting use cases Next  you will findthat we ve improvedthe performances ofCloud Run  as wellas we are adding networkfile system support This is thanks to a newexecution environmentthat we call Second Generation So  basically  when you deploya container image to Cloud Run it runs in the executionenvironment  also knownas Sandbox So  today  you can opt intothe Second Generation executionenvironment Again  using a simple commandline flag or user interfacebutton And this new sandboxhas increased networkin CPU performance  hasbetter Linux compatibility and allows you to use networkfile system  like CloudFilestore  for example And  next  I should callout that we are constantlyat work to increase thelimits of Cloud Run So  first  we have addedsupport for more protocols like Websockets  HTTP 2  andgRPC bi directional streaming All of these aregenerally available now You can now pick up to 16gigabytes of memory in preview up to 4 VCPUs Each request can runfor up to 16 minutes And each containerinstance can process upto 1 000 concurrentrequests at the same time This can becomevery handy if youare using Websockets andopening many streams on a giveninstance  for example Lastly  I want to tellyou how we have made CloudRun even easier to secure  by meeting regulatoryframeworks by allowing you to put a secureperimeter around your Cloud Runservices  and by helping yousecure your Cloud Run services So Cloud Run is now covered bythese regulatory frameworks We know this is very importantfor enterprise settings I should call out FedRAMPModerate  PCI DSS  SOC 1  2 and 3 All of these are nowcovered for Cloud Run Next  how can yousecure or definewhat is allowed to reachyour Cloud Run service Well  first  we have addedsupport for ingress control By default  all trafficat a networking layeris allowed to reachyour Cloud Run service But you can now restrictthe ingress to only allowinternal traffic  so that meanstraffic coming from your VPCor from Google Cloud like Cloud Webserve and you can allow internaltraffic plus traffic comingfrom Google CloudLoad Balancing Similarly  you can control wherethe unknown request should go For example  when youdo an outbound requestto the internet  you can forceit to go through your VPC And by using ingressand egress settings you can also enableVPC service control allowing you to put your CloudRun services inside a VPC ACperimeter Thanks to Cloud Run Integrationwith Google Cloud LoadBalancing  you canleverage Cloud Armor  whichis a web application firewall And you can useidentity aware proxyto build internalapplications thatwould only be accessible toyour employees  for example And  lastly  I wantto tell you howwe made it easier to secureyour Cloud Run services First  by providing a nativeintegration with Google CloudSecret Manager So Secret Manager isa Google Cloud productto store your secretsin a secured placewith potentially yourown encryption keys You can now mount thosesecrets as volumes or asen vars inside yourCloud Run services allowing you todecouple your code your container fromits configuration or even its secret Then  we added moresecurity features Of course  you might knowthat Cloud Run services runwith a certainidentity  so that meansyou can provide aservice account thatwould be used as theidentity of the code thatruns for a particular service And the service account can havemore or less IM permissions Notably  you can remove allpermission of a certain CloudRun service if this one isnot expected to call any APIs We are announcing thegeneral availabilityof Binary Authorizationfor Cloud Run Binary Authorization allows youto define a policy regardingwhich containers can or cannotbe deployed to Cloud Run And by using anorganizational policy you can enforceBinary Authorizationto be applied to your Cloud Runservices in a set of projects Lastly  we delivered customermanage encryption keys allowing you to bringyour own encryptionkeys to encrypt the containerimages deployed to Cloud Run And finally  wehaven t stopped here We are proactivelyproducing recommendationsto secure yourCloud Run services The first one thatwe delivered isto recommend you touse a dedicated serviceaccount with a minimal set ofpermissions for each productionCloud Run services So to recap  we have addedmany things to Cloud Run The first is committedused discount allowing you to get adiscount of up to 17 on the amount you commit We ve improved  and weare continuously improvingits developer experience with local development deploying from sourcecode  more metrics We ve increased theworkloads that youcan deploy to CloudRun  with CPU allocationcontrols  a Second Generationexecution environment or just by pushing thelimits of Cloud Run And  lastly  we made it easierto secure your Cloud Runservices To learn moreabout Serverless  Irecommend you thosethree sessions  Dev100  which is agreat introductionto Serverless on GCP Dev201  which will deepdive into some very coolfeatures of Cloud Functions including some thatare coming  and Dev205 which will teach you how tobuild event driven applicationswith Eventarc and Cloud Run Thanks for joining me today This was  What sNew in Serverless  Enjoy the rest of Cloud Next JONATHAN KELLER  Thank youall for joining me today My name is JonathanKeller  and Ihave the privilegeof being the Directorof Product Managementfor the BigQuery teamhere at Google Cloud Very excited tohave the opportunityto walk you through alot of the innovationsthat we re working onto help you unlock valuefrom your data It s no secret whywe re all here The world has becomemuch more data driven and every companyand every industryis on their journey to figureout how to leverage datato drive value Unfortunately though many companiesare still struggling torealize tangible and measurablevalue from theirdata  but they wantto learn how to providebetter customer experiencesor drive operationalexcellence through moreintelligent decision makingand improve processes Happy to be here to share withyou many of the innovationsthat BigQuery is working on tohelp customers around the worldfinally be able to untapthe power of their data If you re not familiarwith BigQuery it is Google s cloud scaleenterprise data warehouse It sits at the center ofour open platform strategythat delivers value from data And of course standard SQL with DML It s truly serverless multitenant  encrypted durable  and ready for thelargest internet scale servicesand also highly secureenterprise workloadsfrom gigabytes to exabytes It is also intelligent  withbuilt in ML  extensibility and support forin memory analytics It has a simplifiedarchitecture thatunifies real timeand batch workloadsand supports virtually unlimitednumber of analytics use cases And it does this all withindustry leading reliabilityand predictable costs Just to drill into alittle bit of how Googleis able to uniquelydeliver this value BigQuery takes advantage ofGoogle s full stack innovationsfrom hardware andstorages  network all the way up tothe software thathas been used to analyze datafor search and advertisingfor over a decade We are not strangersto big data analytics This full stackapproach allows youto access statelesscompute  whichenables virtually unlimitedcapacity to analyze your data There s no dedicatedVMs and no clustersfor you to worry about think about sizing or trying to manage Our unique network andstorage infrastructureprovides incrediblyhigh throughput making it easy for BigQueryto analyze exabytesof customer data everyday and support streamingingestion of petabytesof data every day as well  all while providingindustry leading reliability And now with the recentnatively integrated BI engine we re leveraging thisamazing full stack to powerin memory and real timeanalytics  as well I wish I had more time towalk you through everythingwe are working on to providean innovation pace thatsupports your business But unfortunatelyI just have timeto call out a few things fromthe huge number of capabilitiesthat have been releasedover the last few months So whether it sgovernance capabilitieslike our field levelencryption  our automatic DLPintegration with BigQuery whichhelps you secure your dataor continue to invest inadvancing in data warehouse machine learning with BQML or powerful new SQL workloadmanagement capabilities we re committedto meeting your analytics needs With that quick recap ofwhere we ve been investing I want to pivot to adeep dive in the areasinvestment we re making toreally support your businessbuilding and runningapplications in the Cloud such as interoperabilitysupport that breaks downdata and application silos our migration and managementsupport for helpingyou get to the Cloudand save money whenyou re here  and finally a ton of investments in crackingthat hard problem of reallygetting value fromdata with real timeand predictive analytics As we think aboutBigQuery s interoperability it s really not like anyother data warehouse You get choice of storage cloud  tools  engines and languages  and we makethat possible to leverageall these choices witha consistent securityand governanceframework  so whether itis our recent launch of CloudSpanner Query Federationor the launch ofautoscaling serverless SparkIntegration  which you canlearn more about in the Openand Integrated DataAnalytics session or some of the new capabilitiesI m going to walk youthrough now  you getto make the choice thatis right for your businessand your data analytics needs Speaking of choiceof cloud  we arevery excited to announcethe general availabilityof BigQuery Omni for Amazon WebServices and Microsoft Azure Most companies have dataacross multiple clouds today and that frequentlycreates data silosthat make it difficult if not even impossible for analysts to accessand leverage that data BigQuery Omni is a flexiblecross cloud analytics solutionthat lets you cost effectivelyaccess and securely analyzedata across GCP  AWS  and Azure With BigQuery Omni you can leveragethe familiar BigQueryUI  API  or standard SQLto quickly answerquestions and share resultsfrom a single pane of glass With net new capabilitythat we are announcingcalled Cross Cloud Transfer you can use standard SQLto bring the results ofyour analysis back to GCPwith a simple copy statementand combine that dataand perform advanced analyticsfor something like BQMLor aggregate the data frommultiple clouds for Lookerand BI tools BigQuery Omni is serverlessand connects directlyto your data on AWS or Azure You can securely run analyticson other public cloudwith a fully managedinfrastructure All compute runs onBigQuery Omni clustersin the same AWS or Azureregion as your data storage To learn moreabout BigQuery Omniand see a demo of our new CrossCloud Transfer capability please visit the Data Analyticssession on multi cloud and we encourage youto come give it a try In addition to therecent BigQuery Omni happy to announce that wewere making the secure lakehouse architecture a realityacross all three clouds With the introduction ofauthorized external tables it is now possible to havefine grained governanceand security permissionson lake house data Authorized external tables allowyou to set up a service accountfor accessing your data nativelyin Google Cloud Storage  S3 or Azure storage so you don t haveto grant privilegesto the raw dataor worry aboutphysical file layout You can use BigQuery snative policy capabilitiesto provide table  column or row level securityfor your data and datalakes  and those policiesare consistentlyenforced  regardlessof whether accessing thedata through SQL  data PROC or other engines  like Spark Net  you no longer have to careabout where your data residesor what engine you reusing to access it in orderto provide consistent secure  auditable  and evenpolicy drivenaccess to your data Before today  BigQuerycustomers had the abilityto create user definedfunctions in SQL or JavaScriptand run them entirelywithin BigQuery and while these functions areperformant and fully managedfrom within BigQuery customers expressed an interestand desire toextend BigQuery UVFsto their own external code We had requests fromhealth care providerswho wanted to bringtheir existing securityplatforms to BigQuery financial institutionsthat wanted toenrich their BigQuerydata with real timestock updates and data scientists who wantedto be able to use Vertex AIalongside with BQML To help these customers extendBigQuery into other components we created BigQueryExternal Functions BigQuery External Functionsprovides a direct integrationwith cloud functions GCP s serverless executionenvironment forsingle purpose functions With externalfunctions in BigQuery you ll now be able to write afunction in node  Python  Go Java   NET  Ruby  or evenPHP and execute it on columnspassed in fromBigQuery SQL queries BigQuery external functions thatyou incorporate BigQuery SQLfunctionality with softwareoutside of BigQuery We ve already startedworking with key partnerslike Protegrity onusing external functionsas a mechanism to mergeBigQuery into their securityplatform  whichwill help us helpour mutual customers addresstheir stringent compliancecontrols Also happy to announce thepreview of Analytics Hub  whichwill help organizations publish discover  subscribe  and sharedata assets so they canseek broader insightsand ask bigger questions BigQuery has hadcross organizational datasharing in placecapability since inception and we have thousands oforganizations sharing hundredsof petabytes of data today Our Analytics Hub makesthat experience of discoveryand management much easier Analytics Hub willallow publishersto create exchanges thatcombine unique Google datasets with commercial industrydata sets and public data sets Publishers will be ableto curate data exchangesinternally andexternally  and they llbe able to view aggregatedusage metrics on how populartheir exchanges are With other interoperabilitycapabilitiesI mentioned  it willsoon be possible to sharedata in the Analytics Hubin place from your datato data lake  as well In addition to theunique Google datathat we are working tobring to Analytics Hub such as the recent releaseof Google Trends data which enables discovery of topsearch terms across locations we re also partneringwith Crux Informaticsto accelerate access todata on Analytics Hub Data providers across finance geospatial  retail  and moreare all bringingtheir data to BigQueryand delivering itthrough Analytics Hub We will announcemore partnershipsas we move closer to generalavailability of Analytics Hubnext year Deciding to move to theCloud is a big commitment both in tech  processes and change management And knowing that it is theright choice for your business both for thecapabilities it unlocksbut also for the overallTCO  is equally important Happy to highlightsome capabilitieswe re invested in that makeall aspects of moving your datawarehouse to the Cloud and toBigQuery on GCP a good bet I m excited to introducethe BigQuery migrationservice  a comprehensivesolution for migrating datato BigQuery which will enablefast and low risk migrations The BigQuery migrationservice speeds upTeradata to BigQuery migrationswith predictable toolingfor customers andpartners  whichcover migration planning  datatransfer  automatic SQL scriptconversion  anddata verification Support for additional datawarehouses is also coming soon One of the hardest pieces ofany data warehouse migrationis modernizing legacy businesslogic like SQL queries scripts  and stored procedures The BigQuery migrationservice providesfast  semantically correct andhuman readable translationsof legacy objects withno ongoing dependencies It supports a broad rangeof Teradata artifacts including DML  DDL  andBTEQ  and translationscan be run in batchmode or ad hoc directlyfrom the BigQuery SQL Workspace Customers like Walmartand MercadoLibrehave used it to successfullytranslate millions of queries and we encourage youto come give a try When doing analytics at scale understanding what is happeningand being able to take actionin real time is critical and many of our customers alsodesire capacity managementcapabilities to optimizetheir BQ environments We re happy to launch a newBigQuery Administration Hubexperience with newfeatures such as researchcharts and the slot estimator The capabilitieshelp our admin usersto understand and managetheir BigQuery environmentslike never before Research chartsprovide a native out of the box experiencefor real time monitoringand troubleshooting ofyour BigQuery environments These charts make iteasy to understandyour historical patternsacross slot consumption  jobconcurrency  andjob performance allowing you to takeactions to ensureyour BigQuery environmentcontinues to run smoothly This is now generally availablefor all customers thatpurchase slot reservations Many customers withpre purchase capacityalso want to understand should I buy more Do I have too many slots Will additional slotsaffect my job performance The slot estimator is aninteractive capacity managementtool that helps administratorsestimate and optimizetheir BigQuery capacitybased on jobs performance It looks at historical usageand helps customers makecapacity planning decisions It also provides estimateson price for performancewhen adding or reducing slots This is now available for publicpreview for all reservationcustomers In addition tothe Admin tooling sometimes customers have oneoff or spiking workloads and flex slots canbe a great solution In fact  many customers havebeen leveraging flex slotsto scale up or down theirenvironments via simple SQLstatements This is great forone off batch workloads but does require you to estimateand manage your capacity We re now happy to announcethe preview of the Flex SlotAutoscaler Autoscaling enablescustomers to takefull advantage of BigQuery selastic serverless architecturewhile maintainingcontrols over cost With autoscaling  yousimply set the sizeof the environment you rewilling to scale up toand then BigQuery looks atyour current query load And if more budget isavailable and more capacitywould accelerate yourqueries in flight it scales up yourenvironment on the flyin very small increments By adjusting the size of yourenvironment every few seconds you get maximum performancewithout any wasted capacityup to your budget Customers like Snaphave been usingthis to provide phenomenalperformance for their analysts and we ve had customersscale up tens of thousandsof slots for incredibleperformance on reallylarge workloads In addition to thegreat capabilitieswe ve announced for managingyour query resources we re also addingadditional supportfor managing your storage Snapshots  currentlyin Preview  provideread only  point in timeversions of tableswithout actually havingto copy the base dataand incur duplicate costs They can be really usefulfor logical backups saving the state of somethinglike a financial dashboardto keep points in time and many other use cases Clones  coming soon  providemodifiable versions of tableswithout having to copythe base data  as well This makes it possiblefor you to test thingslike schema evolution  whereyou re modifying columnsfor the rollout of a newversion of your applicationor to set up dev testenvironments that workend to end so you can runcrews on a large amount of datawithout incurring thecost of duplicate datajust for a test environment Once you re in theCloud  it s timeto really leverage thecapabilities that have beenunlocked that will enableyou to drive your businessand really drivevalue from that data Happy to highlightsome of the investmentsacross the full stack  fromhigh throughput investment to BIin tooling and ML that willhelp you unlock value from data For customers with needs forreal time data analytics the new BigQuery Write APIis now generally available This new Write API underscoresour engineering innovationto bring industry leadingperformancefor streaming ingestion With over 1 gigabyteper second of ingestionthroughput enabled bydefault and the abilityto scale to 100 times or morethat  the Write API allowsyou to unify streamingand batch ingestionand offers exact oncedelivery semantics It also supportsstream level transactionsand automatic detectionof target schema changes These features  combinedwith native integrationwith the data flow will allow youto accelerate your real timeanalytics workloads I m also happy totell you that allof this net new value over theprevious Streaming API  yetis priced at 50  cheaperthan the current offerings So we encourage you to moveto the new BigQuery Write API BigQuery is a powerfulanalytics enginethat provides great performanceon calculations  aggregationsfor data at any scale However  the waycustomers use BigQueryhas expanded from traditionaldata warehouses or data lakes and today there are manyapplications built on BigQuerythat may not wantto crunch numbers but simply return ahandful of rows associatedwith a specific data point like a name or an IP address Customers expressed that whenfinding these specific datapoints in terabytesor petabytes of data it was slower  maybeeven more cost expensive than the applicationswere designed for This is because BigQueryneeds to do a full table scanand read all of it just tofind the rows associatedwith the unique element To help customers build theseoperational applicationson top of BigQuery  weare announcing new searchcapabilities in BigQuery Customers can nowcreate Search Indexesfor a table that acceleratesthe speed at which youcan identify the rows thatcontain specific text You can use BigQuery SearchIndexes and Preview todayif you have scenarios thatwould benefit from speed upof point lookups Or another way Search Indexes areuseful when you need tofind a needle in a haystack Search Indexes can be usedacross multiple columnsat once So even if you don t knowexactly where the data isstored  it can still be found It s also fully integratedwith our native JSON data type also in Preview meaning that youcan search for fieldsand values in datathat is unstructured or has aconstantly changing structure These search indexes alsoback our new log analyticsapplication that wasrecently announced Cloud Logging  which helps useBigQuery to better understandyour telemetry and data Like the rest of BigQuery these search indexesare fully managedand serverless Google has all theresource provisioning tokenizes the data  and loadsthe index behind the scenes So once you create aSearch Index on a table you can start streaming datain and know that your searchindexes willautomatically be refreshedwithout any management overhead We ve been workingwith our customerson this feature  whostarted testing itfor a variety ofdifferent use cases For example  amajor retailer foundthat they areimproving dashboardsthat have highlyselective inventory itemsthat they need toslice and dice We ve also worked forseveral companies in the Uwho are hoping use SearchIndexes to dramatically reducecosts for their GDPRprocesses  since they re nowable to touch only the rows thatcontain specific individuals no longer having toquery the full data setfor each GDPR request And of course  we found amyriad of other improvementsfor log analyticsapplications thatcan now use BigQuery forefficient lookups of thingslike IP addresses  errorcodes  emails  and URLs We were one of the firstto bring integratedmachine learning into the datawarehouse with BigQuery ML and we continue to seetremendous adoptionand customer innovationbecause of the ease of use ability to unlock ML withonly a few lines of SQL and no infrastructuremanagement In addition to continuedinvestment in additional modelsand capabilities  likeXG Boost  AutoML Tables and HyperparameterTuning  I wantedto highlight our workaround a couple of areas First off  Explainable AI Explainable AI helps youunderstand the resultsthat your predictivemachine learning modelsgenerate by defining howmuch each feature contributesto the predicted results This is referred to asfeature attribution This information can be usedto verify the model as behavingas expected  to recognizebias in your models and to inform waysto improve your modeland your training data We support both local andglobal explainability We re also investing inintegrating BQML and Vertex AIcloser together so you canleverage the best of bothfor ML ops deployments For example  BQML modelswill be registered in Vertexand they can be used to manageversions and compare there We ll also be addingsupport in Vertex Pipelines So all BQML trainingand predictionwill be possible throughVertex pipeline components So this user can continue touse their favorite notebookand use BQML pipelinecomponents to vote  training and prediction Overall  in partnershipwith the Vertex AI team we re committed to deliveringa powerful and fully integratedML solution withgreat ease of use Finally  happy to announcethe launch of BigQuery sBI Engine  our solution forenabling data and businessanalysts to perform interactiveanalytics in real timeand with high concurrency BI Engine is not a cache It is a fully distributed highly available  in memorydatabase Engine  whichis natively incorporatedinto standard BigQuery APIs It enables subsequentqueries on large datasets with high concurrencyand can eliminate the needto manage BI serversor ETL pipelines and it natively works with SQLand BI tools such as Looker Tableau  Power BI  et cetera For GA  we added support forpinning preferred tables and upto one terabyte ofmemory per reservationand additional joint operationsand support for streaming This means that incombination with the new WriteAPI for highthroughput streaming materialized views which havealso been recently released and BI Engine for accelerationof interactive analytics these technologiesall work togetherto make the vision of alwaysfresh  always fast analyticsa reality at any scale That was quitethe whirlwind tourof what s new with BigQuery Thank you for joiningus today  and Ihope you can see ourcommitment to providepowerful analyticsolutions that willhelp drive your businessesforward and help you unlockdata If you d like tolearn more  hereare some additionalsessions I suggestyou take a look at to seethe power of Omni  ServerlessSpark  Streaming  Log Analytics and Secure Data Classificationwith Google Cloud Thanks again for joining meto learn more about BigQuery and I hope we can help youdrive your business and dataanalytics for yourcompany forward Thank you BRIAN SCHWARZ Welcome to the What sNew and What s Next forStorage session hereat Google Cloud Next  21 My name is Brian Schwarz And I m joined by my colleaguehere  Dave Nettleton We re leaders in theproduct managementteam here forstorage  and excitedto talk to you about theentire storage portfoliothat Dave and I work onalong with our colleagues During the sessiontoday we ve broken it upinto a few different sections First  Dave s going to startwith the strategy for GoogleCloud Storage And then we re going to runthrough the updates and what snew on a number of parts of theproduct line  Cloud Storage file  persistent disk And we ll end with somediscussions about transferin Backup Archive DR  as wellas include a few links to whereto find more information With that  I wantto turn it overto Dave to start a discussionabout the strategy DAVE NETTLETON  Thanks  Brian And so just at avery high level I thought it d behelpful to go through howwe think about our strategy andthe different parts of that So our goal is todeliver easy to use performant  and highly availablestorage for all workloadswith enterprise managementcontrols and insights This really guides how wethink about both the portfolioof products that we have but also the individualfeatures within them And so at a high level three main types of storagethat we provide in the Cloud block  object  and file We ll spend a little bitmore time on each of these These cover both cloud nativeand more traditional enterpriseuse cases Together with those we have capabilitiesfor data transfer anddata protection  whichare very importantservices that come handin hand often with storage And then as well as what wedeliver in terms of first partyproducts from Google we re very bigbelievers in having an openecosystem of partners hereto give customers a range ofchoices for their workloadsas they bring them to the cloud Two big categories there wouldbe in file and data protection For file we partner closelywith NetApp  Dell EMC  and DDN And for Backup and DR Commvault  and Veeamare two of the partners who amongst otherswe work with very  very closely So let me dive in alittle bit on someof the greatenhancements that we vehad in the products recently So starting with our objectstorage product  Cloud Storage So one of the greatfeatures in Cloud Storage our object storageproduct  is the notionof dual regional storage And what this does isit lets you manage dataacross two regions So without thisproduct  you needto put a bucket ineach of two regions Each of those buckets wouldhave a different name You d have to separately dealwith managing and copying datafrom one bucket to the other If you had to dealwith a disasteror you wanted toredirect trafficto read from onebucket or the other you d have to deal withfailover and fail backof those storage buckets With dual regional storage we provide a productthat gives just a single bucketthat spans those two regions So resources in eachof those regions or indeed  in other regions can read data from that bucket And we will manage  behind thescenes  the data placement And we provide asingle namespacethat you can read and write to So it s active  active froma read and write perspective We provide strong consistencyover that metadataof the bucket And then in the background we move the data replicate the databetween the two regions In the event of a failurein one of the regions the application won tnotice it continueto read the metadata that sconsistent across these tworegions  giving immediatefailover to any activity that shappening in the other region One of the big enhancementsthat we are announcingis that we re going to allownot just fixed name pairs  whichis what we have today We currently have apair of regions in Asia a pair of regions in the US and a pair of regions in Europe We re going to allowcustomers to specifythe particular regions thatthey want to create a customdual region across This is very powerful being able to pair togethera couple of regions andprovide a single bucketthat spans those two regions Together with that  we alsowant to deliver a capabilityto give stronger guaranteesaround the recovery pointobjective So we call this turboreplication  wherewe ll give an SLA of the datathat is written on one sidewill be replicated to theother side within 15 minutes And so we ll providean SLA back replicationfor the data in thesedual region buckets Very excited about this inhelping customers bettersupport their businesscontinuity needs and also managing their dataacross multiple regions As well as that feature some other enhancementsfrom Cloud Storage thathave happened recently So multipart uploads  thishas been a very common featureask from customerswho are lookingto make it easy to bringlarge objects to the cloud and construct them in parts and finalize them in the clouds This has been a very popularprogrammability feature And then a category  the nextthree all around security So security is something wetake very  very seriously We work a lot with customersaround their security needs So we want to make sure that wesupport the various workloadsthat they have here through assured workloads through providing tags  bucketlevel tags that let customersreason about their datathrough IAM policies and supportcertifications like FINRAwith capabilitieslike bucket lockthat let customers lock abucket for an extended durationof time And then another enhancementthat we announced earlier inthis year  it s relatively small but actually very powerfulis with Google Cloud Storage weoffer four classes of storage from hot all theway down to archive And we let customerstier between that datafor managing costs It doesn t actually  all storage classeshave the same API and thesame fast access to data So even the archive tier isonline and instantly available But customers want a tierbetween those storage classesfor cost purposes With a custom timestampas part of the metadata customers can nowuse a measure of timeto help them make decisionsabout when to tier data So it s a powerful capability Moving on fromour object storageand cloud storageover to file storage both from Filestore our first party product and also with partners So file storageis a huge use casefor our customers particularly customerscoming from on premises where they have builtrich and sophisticatedworkflows around enterprise fileofferings  whether that sjust as traditional fileshares  or as directorieswhere they re putting sharedexecutables aspart of workflows build pipelines  et cetera So customers are looking as they lift and shiftto the cloud  to have avery powerful set of filecapabilities To date  we ve had afile store product that sboth a basic and a high scale And these have beenzonal products And we re reallypleased to announcethat we ve launchedFilestore Enterprise  whichis a regional service So it will survivea zone failure So we replicate dataacross multiple zones So customers don t needto worry about that They have a highlyavailable regional service backed with a four nines SLA And then we re also makingsure that this is thenwell integrated with all ofthe other enterprise featuresthat customersexpect in the cloud whether that sStackdriver for logging or our security features likeVPC Service Controls  VPN Interconnect to help customersset up the appropriate securityperimeter aroundtheir infrastructure and have that file productto be part of that And we also wantto make sure thisis available through ourconsole  CLI  nice and easyto use And then in particularwith GKE  to helpmake it easy to provisionand get file resources there providing the CSI driversupport to simplify So as well asFilestore Enterprise some other enhancements for filethat we ve announced recentlyis backups for Filestore Basic So Filestore Basicis our zonal product And we ve deliveredbackups for that Filestore High Scale isour high performance scaleof Filestore  which letscustomers scale to many tensto up to 100 terabytes of scaleat much higher performance And then as well as what we vebeen doing our first partywith Filestore lots of enhancementsthrough our partners  inparticular with NetAppand Dell And we re seeing a lotof customers havinga lot of success withsome of the workloadsthat they re bringingto the cloud with thingslike SMB workloads forWindows  or media renderingand genomics And with that  I llhand back over to Brianto tell you all a littlebit about persistent disk BRIAN SCHWARZ  Thanks  Dave So persistent disk is theblock storage offering herein Google Cloud It s often used withthe Compute Engine VMs That s probably themost common use case but it s also used quite a bitwith Kubernetes Engine  GKEfor container workloads And it s also used with CloudSQL for the managed databaseoffering So pretty popular offering  abunch of different use cases One of the new  great featuresthat we introduced recentlyis a new class of performance extreme persistent disk And it s really targeted at thehighest performing workloadsin your data center particularly high enddatabases  SAP HANAbeing an absolutelyclassic example of this In addition togreat performance another great featureit has is the abilityto independently scalecapacity and performance So you can really tune the cost and performance  and capacityto really get the best TCOfor your environment  justin the persistentdisk volumes itself It also includes easyto use snapshots which can be used forlike cloning use cases So you could refresh test anddev environments and workflowssimilar to that And also  you can attachpersistent disk volumesto any size VM So again  it s anothercapability of flexibility So you can really use theGoogle Cloud infrastructureto tune the capabilitiesof the infrastructureto exactly what you need So you don t pay for maybea really large VM justbecause you need reallyhigh performance storage So again  just try and giveyou all that flexibility Dave mentioned security It kind of exists kind ofthroughout our portfolio Another classic example ishere with persistent diskis the ability tohave  of course at rest encryption andencryption in transit and lots of flexibilityin terms of howyou manage thekeys  with customermanaged keys orcustomer supplied keys And another thing we rereally excited aboutis these constant investmentswe make in the resiliencyof our system And extreme persistent diskhas a six nines durability which is a measure of howdurable the storage is It s really importantthat we investin making these thingsavailable and durable for you There s a number ofother enhancementsthat we ve had forpersistent disk thatexpand beyond extreme PD And the first oneis probably a tieoff to what I justtalked about durability We have a nice publicblog posting we ve doneand inclusion in somemore documentationfor PD about the reallyleading durabilityguarantees that we haveincluded in our services Another service that weadded earlier in the yearwas balance PD It s an SSD class storage So it gives you goodperformance  not asgood as extreme PD  but ata much lower price point And it s a really goodoffering for a lot of I ll say general purposeworkloads in the cloud People use them for boot disks for test and dev databases There s lots of reasons whythese things are interesting And it really gives you greatIOPS at a lower price point Also with balanced PDis we have somethingwe refer to as regionalPD  which essentiallyis synchronous replicationacross the zones in a givenregion This is a differentiatedcapabilityfor Google Cloud  super powerfuland important for those of youwho basically need toprotect against zone failuresand essentially you regetting synchronous rightsacross multiple regions Used quite a bit with theCloud SQL offering todayto provide the highestlevels of availabilityfor some of yourimportant databases And  of course along the same lines increasingly investingin APIs to beable to deliver applicationconsistent snapshots which will reallyspeed the recovery timeif you need to re instantiatea database from a snapshot Another small part of theblock storage product linehere is our local SSD offering And you can think aboutthis as a counterpointto persistent disk that s reallytargeted at ephemeral or cacheworkloads So locally attached to the VM very high performance  veryhigh IOPS  very low latency Really targeted atthese caching workloads in memory databases whereyou need the absolute highestlevel of performance  but youdon t need the persistence and durability and the enterprisefeatures that we havein persistent disk So again  a nice additionto the product lineearlier this yearwas the supportfor 9 terabyte local SSDs And again  like I mentionedearlier  the abilityto attach these to manysizes and shapes of VMsso you can really  again  tuneperformance with CPU  memory and storage performance  allbeing targeted exactly what youneed for your workload And with that  I would liketo turn it back over to Daveto talk about data transfer DAVE NETTLETON  Thanks  Brian Yeah  so data transferthere s two major productsin the portfolio that we havehere  the Transfer Applianceand the Transfer Service So starting out withTransfer Appliance one exciting enhancementthat we ve hadis to allow this to be availableto customers in what wecall a connected mode So to date  we vehad an appliancethat ships to acustomer s premises either  for example  tomove large amounts of datafrom a data center thatthey re migrating to the cloud Or increasingly we re seeing forcapture at the edge use cases where for AI ML workloadsthere s a lot of data capturethat might happen in the field And we want to gathera lot of that dataand then bring it to the cloud So we ve had aTransfer Applianceto allow customersto capture that dataand ship it to the cloud Recently we just announcedthat we ll actually allow thisto operate in a connected mode So you can actually dropdata onto the appliance and then over anetwork connectionit would move the data tothe cloud on your behalfvia that connection So excited to seesome of the use casesour customers will put this to As well as the Appliance  a lotof enhancements on the TransferService recently Several here thatI ll highlight First of all  theTransfer Service API So giving programmabilitycontrol over transfers we thinkwill help customers muchbetter integrate thisinto their various workflows Agent pools is acapability we veadded to the agent thatis part of transferservice for on premises So this is somethingthat is deployedinto a compute resourceand manages the transfer With agent rules  we re nowenabling those resourcesto be better managed acrossmultiple transfer jobs and over a shared resourcelike a network link so giving much more flexibilitycontrol over transfer And then more sources andsinks for their service We ve added supportfor Azure Data Lake V2 And for the appliance we readding more locations with London being one that srecently been announced And with that  I llhand back to Brian BRIAN SCHWARZ  Thanks  Dave For the last contentsection  I wantto talk about Backup Archive  and DR And it s a reallyimportant area to make surethat you re protecting your datain all the different places itexists in Google Cloud One of the newofferings that we justannounced in Septemberof  21 is a new offeringfocused on containers Google Cloud GKE Backup And what we veseen is an increasein the number of customersrunning stateful workloadsinside of containers  includingvery traditional databases And as a result ofthat  we ve beenworking on essentially anew service offering that stargeted at protecting thedata inside these statefulcontainers It has a niceorchestration built into itthat essentially allows youto take regular PD snapshotsto protect the data insidethese stateful containers And it gives you prettyflexible recovery optionsto recover individualcontainers as well as full clusters  bothin the normal region whereit was running  as well aspotentially for a DR use caseto reinstantiate itinside of another region There is flexibilityto basically manageyour backup policy  scheduling recurring snapshots as well as setting theretention cycles on how longyou want to keep thesesnapshots around And as GKE andcontainers continuesto be used for moreimportant  very traditional enterprise class workloads it s important to have offeringslike this thatessentially help youmeet compliance requirements So you can createbackups for the datafor inside of these containers And we also have theability to do pre and posthooks so you canalso get applicationconsistent snapshots Across the rest of ourBackup and Archive portfolio there s a number of otherthings that have been going on In particular  lastyear at the end of 2020we acquired Actifio And it s the real centerpieceof our first party backupinvestments going forward Actifio was aGoogle Cloud partnerand had an offering calledActifio Go in the marketplace And we ve been workingon making that offeringmore native insideof Google Cloud to really simplify thedeployment of this service integrated billing And  of course  it leveragesall the great technologyActifio had as itcame into Googlearound incrementalforevers in termsof speeding up the backupsafter the first one It s really targeted again  at these missioncritical databases  SAP HANA Oracle  MySQL  et cetera And it s really important for usto continue to invest in this particularly about making ita first class service that llbe more nativelyintegrated into someof the higher level managedservices as we go on And to round out this  Iwanted to briefly mentionsome of the partners  justlike Dave did for file There s a number ofdata protection partnerswe work with  two ofthem who have donesome recent enhancementsto the offeringsto support Google Cloudare Veeam and Commvault So Veeam V11  whichis often used on prem now supports Google CloudStorage as a backup target So you can really think aboutthe tape replacement use case I like to tell people  oneof the hidden gems of GCSis some of the cheapertiers of storagehave very low latencyin terms of access So you can really use thesecheaper tiers to protectyour data and have compliance But if you everneed to get it back you don t need to wait hoursor days to do restores And also  there is a new Veeamfor Backup for Google  V2that just got releasedon the marketplace And it also supports thearchive class tiering which  again  will reduce thecost for long term retention And lastly  Commvaulthas done a great job They re a memberof our marketplace recently came ontothe marketplace And has some great capabilities Obviously well knownfor VMware backups and have done some niceintegration with Google CloudVMware Engine So again  an excitingnew partnership We re excited to work with thesepartners as well as others again  to provide a wide arrayof backup  archive  and DRsolutions  both from Googleand from our partner ecosystem So to round outthe session  I justwanted to talk alittle bit about whereto learn more information And there s a coupleof other sessionshere at Google CloudNext that I thinkif you re interestedin this section wouldbe useful to look at There s a more specific sessionthat goes a little bit deeperinto Filestore Enterprise  whichis something Dave mentioned There s a wider HA andavailability sectionthat talks about a numberof the capabilities as well as somecustomer sessionssprinkled in throughout So I really hope youget value out of these Included a numberof links to whereto find best practicesand additional technicaldocumentation  as wellas some of the trainingand certification links on howto get more training and getcertified And one of the thingsthat I was excited to seeis that last year the GoogleCloud Professional CloudArchitect was the highestpaying certification So great kind of careeroption for peopleto invest in themselves And we re excited Hopefully this sessionwas a good overviewof all the things that is what snew with Google Cloud Storage Enjoy the rest of the event And we ll see you again soon Thank you SHAILESH SHUKLA  Goodmorning  good afternoon and good evening  everyone Thank you for joining us and welcome to What s Newand What s Next for Networking I m Shailesh Shukla  VicePresident and General Managerfor Networking WENDY CARTEE  And I mWendy Cartee  Directorof Outbound Product Management We had a very busyyear  and we reso excited to share whatwe ve been working on SHAILESH SHUKLA  Wehave a packed session and we ll cover our Cloudnetworking portfolio  what snew in networking  networksecurity  and monitoring But first  thewhy of networking In addition to being a criticalpart of the global CloudInfrastructure  networking alsodrives real business outcomesfor our customers As workloads and workforcesbecome increasingly globaland increasinglydistributed  the networkis at the front and centerof connecting  securing and enabling everything while deliveringa seamless experience toall applications  all users and all devices For example  ourretail customerstell us that a high performancenetwork is directlytied to superior userexperience  which translatesto faster revenue conversion That s a top line I don t want to go througheach of these items here but what you willsee is our customerstell us  in all verticals across the world that networking is a key enablerfor their business outcomes Our Cloud Networking visionis centered on our customers and the vision is this  planet scale networking fora smart and connected world Google has built a massiveplanet scale networkinfrastructure servingbillions of users today And we are leveragingadvanced AI  automation and programmability to builda self driving network thatmakes it simple  smart  trusted and open  so our customers canfocus on running andenabling their business Our global networkis at your service with 27 regions  82 zones 146 edge PoPs  thousandsof edge nodes  16 subseacable investments connecting more than 200countries and territoriesworldwide We are proud to servebillions of users while staying 100  carbonneutral since 2007 And we continueto invest heavilyin our infrastructure our network And just this year we have alreadyadded four new Cloud regions  Warsaw  Delhi Melbourne  and Toronto In addition  we haveannounced five subsea cables  Echo  Firmina  Blue Raman  and Apricot which connect all parts ofthe world from Asia  Europe Africa  Middle East to the Americas On top of this massiveglobal network we offer a broad portfolio ofnetwork and security servicesthat connect  secure scale  optimize and modernize your networkand your infrastructure To do that  we havethis full stackof networkingservices from layer 1to layer 7  that providehigh performance networkingfor all your workloadsrunning in VMs in containers  or on Bare Metal 2021 has been an incrediblyexciting year for us We added several newproducts this year including the Google DistributedCloud  Cloud IDS Service and enhanced our servicenetworking portfolio with Network ConnectivityCenter  Private ServiceConnect  Network IntelligenceCenter enhancements  et cetera Wendy will tell youa lot more about it Let s get started Wendy  over to you WENDY CARTEE  All right Thank you  Shailesh We have a lot ofexciting announcements so let s take a look atwhat s new in networking I would like to introduceour Google Distributed Cloudstrategy  which provides a fullymanaged  integrated hardwareand software solution thatextends Google Cloud sinfrastructure and services toedge locations and customers data center Our Distributed Cloudsolution is enabled by Anthosthrough a centralizedcontrol plane so customers can build andrun modern applicationsanywhere with low latencyand operational efficiency Google DistributedCloud is idealfor local data processing low latency edgecompute workloads  modernizingon prem environments running sensitive workloads tomeet sovereignty requirements and offering private 5G LTEsolutions for enterprisecustomers It can be deployed at GoogleEdge  operator edge  customeredge  and or customerdata centers We announced NetworkConnectivity Center  or NCC a few months ago  and itis now generally available NCC provides a simple andconsistent management modelto connect your on premand Cloud networkingneeds seamlessly With NCC  you can connectVPNs  partner and dedicatedinterconnects  SD WANs  andthird party router appliancesin a hub and spoke model And we ve partneredwith SD WAN leaderslike Cisco  Fortinet  Palo AltoNetworks  Versa  and VMware enabling you tosimplify connectivityfrom branch to Cloud In addition tonetwork layer connectivity we also offerservice layer connectivity with Private ServiceConnect  or PSC PSC provides a service orientednetworking modelby abstracting the underlyingnetwork  such as pairing IP address configuration  et cetera PSC allows you to createprivate and secure connectionsfrom your Cloud networksto producer services such as Cloud Storage Cloud Bigtable or third party services fromElastic  MongoDB  or Snowflake PSC is now GA in allregions  and we introducedthree new enhancements PSC with Consumer ServiceControls is now in preview With HTTPS ServiceControls  PSC allowscustomers to have finer grainedcontrol of their own policies and support data regionalizationfor sovereign clouds We added HTTPS internal loadbalancer as a PSC targetto help scale services For example  you canconfigure the load balancerto log all requeststo Cloud logging or choose which services areavailable using a URL map PSC is integratedwith Service Directoryso you can auto registeravailable services to makeservice consumptioneven faster and simpler This is so cool that companieslike Bloomberg  Elastic Cloud and MongoDB shared why they areusing Private Service Connect And we hope you willalso test drive PSCand let us know how you like it Now  moving upthe service stack let s talk about GKEand Anthos networking We released a lotof features in GKEand Anthos networkingrecently  and we are onlygoing to cover four today First  GKE gateway controlleris Google Cloud s implementationof the KubernetesGateway API  and itmanages internaland external HTTPS load balancing for a GKE clusteror a fleet of GKE clusters It is currently in previewand will GA this quarter Second  Cloud DNS for GKEsupports the full KubernetesDNS specification  andit will GA this quarter It provides pod andservice level resolution with a fully managed DNSthat does not requirea cluster hosted DNS provider So it removes the overhead ofmanaging cluster hosted DNS For Bare Metal fans out there a BGP based load balancerfor Anthos Bare Metalis now in preview supporting N wayactive active redundancyfor faster failover andefficient utilizationof bandwidth And lastly  withthe increasing useof container networkfunctions for CNFs we are introducingmulti NIC podson Anthos BareMetal  which allowspods to attach to multiplenetwork interfaces for usecases such as data andcontrol planes operations multi tenancy  managementfunctions  et cetera Now  we covered edge hybrid connectivity service layer networking GKE  Anthos networking What s next We re going to talk aboutApplication Delivery Our ApplicationDelivery portfoliooffers layer 7 load balancingand layer 4 network loadbalancing In the layer 7 loadbalancing family we are introducing regionalexternal load balancer which supportsdistribution of HTTPand HTTPS traffic to backendshosted on Compute Engineand GKE Our layer 7 loadbalancers are nowconverged on Envoy  aCNCF open source project as the common data plane Leveraging Envoy Proxy  theregional external load balancerprovides advanced trafficmanagement capabilitiessuch as routing andtraffic policies so you can steer trafficwhere you want it In addition  we added supportfor hybrid load balancing Hybrid load balancing supportsworkloads running on premin Google Cloudor in other cloudsto help customers withmigration  performanceoptimization  andhigh availability You can use externalor internal HTTPS loadbalancing  in additionto TCP SSL proxy Together  they re not onlyload balance endpointson GCP  on prem or other clouds they can extend CloudArmor to provideDDoS protectionto your services and leverage Cloud CDN forlow latency performance Next  let s take a quicklook at DNS enhancements Now today  thousands ofcustomers use our Cloud DNS And we ve added alot of enhancements One to highlight isCloud DNS Policy Manager which is in preview It enables you to steerprivate and internet trafficusing dynamic responsesfrom Cloud DNS Policies may be set upusing weighted round robinand or geolocationrules  and includeshealth checks  which can beused for automatic failovers You can use thisto manage trafficin a multi vendorenvironment  direct traffic and set up failovertraffic patterns Let s take a look at what snew in network security We announced many new securityenhancements this year such as hierarchical firewall Cloud Armor rate limiting and Cloud IDS We are excited to announcethe preview of Cloud Armorbot and fraud management  whichleverages reCAPTCHA Enterpriseto detect and stopunwanted bot activityat the edge of the network This integrated DDoS  WAF and bot detection solutionuses signals collectedin the browser plus threat data to automatethe detection of fraud While customers can use theseproducts together already we are simplifyingthe deployment processand providing a moreintegrated experienceso customers can acceleratetheir time to value For more information please attend the sessionon DDoS  WAF  firewalls  andnetwork based threat detection Next  we announcedCloud IDS in Julywith our partnerPalo Alto Networks Cloud IDS provides a fullymanaged network threatdetection service  withhigh levels of securityefficacy and low noise We worked closely withPalo Alto Networksto embed their industry leadingthreat detection technologies and deeply integrated theminto our Cloud  beyond havinga uniform usage model acrossthe UI  APIs  and command line This means that customers canpick from GCP native targets including anything from aVPC to a specific workload and even leverageour flexible taggingcapabilities to identifywhat they want to monitor We surface the alert informationin the Google Cloud UI  APIs and Cloud Logging systems plus feed the datato our Security Command Center Chronicle  and third party SIEMand SOAR products It s quick and easy to deploy as it is managed  scaled and operated by Google with high performanceand availability all built in Now  with all thegreat products what about day 2 apps Let s take a look at monitoring Our flagship monitoring product Network intelligence Center has been expanded withseveral new modules recently Network Topology lets you viewactual flows  traffic patternchanges  ingress egress bytesover VPN and Interconnectto and from the internetand GCP managed services Support for VPNs andInterconnect is now in preview and support for Googlemanaged services is now GA Global PerformanceDashboard gives youreal time metrics  suchas latency and packetloss for the GoogleCloud network as well as visibility into theperformance of your projects resources With connectivitytests  you can checkthe connectivitybetween a VM instanceand GCP managed services such as GKE and Cloud SQL and troubleshootissues at any time Well  I hope youenjoyed the What sNew in Networking session To learn more please take the timeto view the deeper divesessions on these topics and visit our websitefor more information Thank you for joining us NIRAV MEHTA  Thankyou for joining me My name is Nirav Mehta I am Director of ProductManagement for Google CloudCompute Engine I d like only 20 minutesof your time today The first half  Iwant to share with yousome of our recent innovations but also the core principlesthat drive them And for the second half I llbe talking with Omer Hasan  whowill join me from AppLovin He s the VP of Operations I m very excited to have himhere for a short conversation So let s get rightto it and startwith some of the investmentthemes and principles thatdrive us First  Google CloudCompute Engineis all about offering youthoughtful and intuitivechoices What this means isit s easy for youto select the right virtualmachine type  the right storagetype simply by looking at howwe ve organized our choicesby workloads or by use case And I ll talk about thatin a couple of minutes Second  packagesolutions that allowyou to very easily  in anopinionated way  deploy for example  SAP HANA  orVMware  Microsoft SQL Server We ve thought throughreference architecturesand common deploymenttemplates to helpyou get to outcomes quickly Third  and perhapsthe most central tenetthat we have behind theplatform engineering is simplicity at scale Many customers Italk with tell methat Google Cloud is intuitive And as they use it more they find it simpler and that we make complexthings seem simple This is very important to us We take it to heart and wantto ensure we maintain thisas we grow  add more features So it s a commitment tomaintain this advantage and I ll talkabout how we do it Security is paramount Not only making sure that weoffer all the right securitycontrols andcompliance controls but also that we put privacyin the hands of the customer or in the control ofyou who owns the data A simple example of thisis our recent featurefor encryptingin memory contentswithin a virtual machine The encryption keys arecontrolled by the customer and turning thefeature on is a matterof simply checking one box So simple security that scomprehensive with the customerremaining in control And lastly  cost optimizationin all its forms not just offering the rightprice points  the right priceperformance points  but alsohelping you reduce waste making machine learning basedrecommendations that tell youwhat types you shouldbe using  how youshould be adjusting your spend All of that comes togetherto make this a highly costoptimized platform So let s talk abouteach of these starting with VM families With VM families  atthe very highest level there are two categories  workload optimizedand general purpose For workload optimized thereare three categories   computeoptimized for highperformance computing and these are heavier on computepower  or memory optimized or the M series of VMs This is for the high enddatabases like SAP HANA with very highconfiguration of RAM And lastly  there isaccelerator optimized which is GPU enabledVMs for machinelearning training inference I ll talk about acouple of these soon And looking atgeneral purpose  wehave  again  threedifferent categories ranging from price optimizedor cost optimized to price performance optimizedfor scale out Our cost optimized familyis called the E series or the Efficient series If price per VMor price per coreis the most importantconsideration  that s the oneyou would pick If you want abalance and a featureset that works for the longtail of general purposeapplications  you wouldgo for the M series And the newest additionto the VM familyis the T Series  or the Tau VMs These are optimizedwith a lean feature set eliminating waste  and offeringonly the right featuresfor scale out digital nativeapplications at a veryattractive price point Specifically  we tested our TauVMs against competitive optionsfrom leading clouds And as you see here  we veachieved a price pointthat s 42  betterin price performancethan anything elsefrom a leading cloud This includes anything with anIntel  AMD  or ARM processor We are committed toall these CPU types But what s interestingis with Tau VMswe ve achieved the leading priceperformance point with AMD VMs That means yourdevelopers don t haveto go port froman x86 applicationto ARM or anothertype  and stillachieve this price performance Tau VMs are in preview and weare open for registration now An example of recent innovationfor accelerator optimized VMsis our A2 VMs These are the first VMs tointroduce the NVIDIA A100 GPU And I m very proud that wehave delivered the largestsingle instance VM with up to 16NVIDIA A100 GPUs among leadingclouds  so that you can bringyour machine learning trainingworkloads  achieve the scaleand performance you need and grow with Google Cloud Another thing that s newwith virtual machinesis our spot VMs Spot VMs are a newand enhanced versionof our pre emptibleVMs  which allowyou to use ourinfrastructure when it is notbeing used by us so that youcan get very attractive pricepoints What s interesting here is weguarantee 60  minimum savings which we believe is one ofthe most compelling pricepoints in the market Second  and more importantly this pricing is predictable Quite often for this type ofVM you will see in the marketthe prices varyevery five minutes In our case  thesevirtual machinesare much morepredictable in pricing The variation is not morefrequent than once a month Third  Kubernetes Enginealso supports smart VMs so you can take advantageof the price pointfrom the Kubernetes clusters And in doing so we have made surethat the preemption  when itdoes happen  is very graceful And lastly  you cancombine this price benefitwith custom machine types Custom machine types allowyou to pick the exact pointfor the number ofcores and memory so that you don t have any wastethat is often caused by havingpredefined machine sizes Moving on to block storage again  a simple set of choices At the very left  youare optimized for cost We call it the persistentdisk efficient category The efficient categorywill be released next year In the middle  asyou try to seeka balance of priceand performance we have PD balanced And then  as yougo to the right you have the moreperformance intensive storageoptions  PD performanceand PD extreme For example  PD extreme wouldbe used for the highest end SAPHANA databases  deliveringup to 1 200k IOPS So we talked aboutcompute and storage Let me addresssimplicity at scalewith two features thatwere released recently And we are continuouslyenhancing those featureswith new capabilities First is VM Manager This is a suite of tools thatallow you to do three things  patch management config management and inventory managementfor operating systems Combined together  thisallows very easy visibilityinto the posture of youroperating system compliance Quickly identify whereyou are out of compliance and then launchfrom the dashboard or in an automatablemanner  the remediationto bring it back to compliance This is very important formanaging VM fleets at scale Having this sortof burden and toilremoved by having itmanaged by the platformsaves a lot of timeand costly labor Similar to VM Management we have this featurecalled managed instance groups But in this case  insteadof managing the VMs you re actuallymanaging a group of VMsas a composite application which is much more intuitive If this is whatyou want to do  youcan achieve thatwith Google Cloud For example  as shown here we can auto heal applications So for example  some of theVMs in a group stop working The rest of the VMswill take over the load And when the VMs are healed they re put back into the groupand they start managing theload with the rest of the group We can also achieve autoscalingand talk about that and things like auto updating It s very important that themanaged instance groups arestateful  so you can deliver astateful application  not juststateless applications We support that So let s talk about autoscaling With managed instancegroups you can scale upand scale down as needed Most often customersin the past have beenusing scheduled autoscaling So if you re expectinga peak at 9 00 AM  thena little bit in advance ofthat  schedule autoscaling And then later in the dayhave an autoscale down With Google Cloud you cando another thing in additionto schedule auto scale You can use machine learningto have the platform recommendor automatically scale up andscale down the managed instancegroups based on pastpatterns of traffic This is very useful whenyou re in a business whereyou don t quite know how yourtraffic might evolve over time And so using machinelearning at scale we are able to drive autoscalingwith managed instance groups So in summary  I ve covered afew of these core principlesthat drive us Hopefully you see how ourinvestments are reallydriven by all the principlesthat I ve shared here Over time I hope to come backto you and share much more And now I am very excitedto have with me Omer Hasan VP of Operations at AppLovin AppLovin is a leadingmarketing platformthat helps mobile app developersgrow their applications Omer  welcome  and thankyou for being with us OMER HASAN  Gladto be here  Nirav NIRAV MEHTA  So Omer  let sstart with your migrationto Google Cloud You achieved one ofthe faster migrations closing down severaldata centers and coming over to GoogleCloud in a very short time I d like to have you tellus a little bit about howyou went about it What were some of thekey tools  practicesthat you used in achievingthe speed of migrationthat you did OMER HASAN  Yeah  sure Our journey did start overon on prem data centers And we found  as ourbusiness continued to grow they no longercould scale with us And we looked atGoogle Cloud for that and we leveraged tools likeGoogle Cloud Interconnectto help with ourmigration initially and then tools likeTerraform and Chefto build up our infrastructure We found that Terraformwas really simplewith Google Cloud  and leverageda lot of the APIs underneath And we were able to buildour data centers reallyfast and efficiently NIRAV MEHTA  What wasthe rough timeframe And I think you did quite a fewdata centers within a few days OMER HASAN  Yeah  sure We built out our infrastructurefirst on Google Cloud And when it cameto migration day we were confident enoughto do five data centermigrations in a single day NIRAV MEHTA  Wow  very good I m sure a lot of planning but also a lot of skills thoughtful trainingthat went into it How did you go aboutguiding your teamsto embrace a new conceptbecause they were workingwith on premise data centers What were some of theobservations there OMER HASAN  Yeah  sure Part of that was workingwith Google s PSO team along with just readinga lot of documentationand understanding thebest practices of it From there  we startedto automate and build outour Terraformscripts and modules And it was easy from there NIRAV MEHTA  So thesupport for Terraform having the right librarieswas important to you You found the Google supportto be complete enough that youcould just be productive OMER HASAN  Absolutely The Google team wasvery responsive And if we needed a new feature it was built out within a week NIRAV MEHTA  Good Good to know With respect to once youarrived on Google Cloudand how you scaled up what benefits you saw can you start withmaybe  what weresome of the first observationsyou had in  hey  thisis working a lotbetter than it used to OMER HASAN  Yeah  theimpact was immediate We saw that in ourKPIs internally We saw latency drop by 25  We were just able tohandle a lot more We saw  in addition tothat  a material impactto the business NIRAV MEHTA  What about SLAs How have your SLAs tothe business changedas a result of this move OMER HASAN  We were ableto tighten up our SLAs Given that things wereperforming faster we were scaled outmore  and doing thisall in the midst ofa growing business And I m pretty proud to saythat we were able to justdo more  and cheaper NIRAV MEHTA  That s great Yeah  that s whatwe like to hear Let s talk about thenetwork resilience the overall resilienceof the platform Where did you see advantagesin terms of eliminatingsingle points of failure OMER HASAN  Yeah  sure When we were on prem we wouldhave NAT based load balancers And we found with moving intoGoogle Cloud we didn t quiteneed that anymore And by eliminating that singlepoint of failure  and reallya choke point  wefelt more confidentin our infrastructure And particularly infailover scenarioswhere traffic increased bya lot  or significantly we had an ease of mind there NIRAV MEHTA  Great Yeah  good to know I think the collection ofdifferent availability featurescome together to supportyour applications And knowing how you ve usedthose features to deliver scaleto your business isvery useful to us It realizes the promise ofwhat we like to deliver I want to also talkabout things that you redoing that are above theraw virtual machine layer Tell us about anyservices you re using higher order services in GoogleCloud beyond virtual machines OMER HASAN  Yeah  sure We re using Dataproc today NIRAV MEHTA  What areyou using that for OMER HASAN  We re usingthat for our machinelearning algorithms  and beingable to do that at scale In fact  it was actuallyone of the first serviceswe used at Google And as we were growingand scaling our businessand doing more ML  we foundthat doing that on premwould just be a CapEx feat And it would really takea long time to build And so when we saw Dataprocit felt like a natural fit And able to scaleout  and not onlyscale out  but alsolower our SLAs and do that in acost efficient manner NIRAV MEHTA  Yeah  and alot more elastic  I imagine Because the nature ofthese workloads is theygo up and down unpredictably OMER HASAN  Correct NIRAV MEHTA  So that swhat you meant by CapEx and not having tospend on those peaks  OMER HASAN  Exactly NIRAV MEHTA  Makesa lot of sense So as I reflectback  you migratedalmost the entire platformover to Google Cloud achieved a larger regionalfootprint  better SLAs better performance cost savings We couldn t ask for more Thank you so much  Omer That is an impressive journey The speed with which youmigrated over to Google Cloud how you scaled  how you tookadvantage of all our principlesto improve your resilience offer better SLAs better performance these are all the thingswe love to see in our customer So thank you for takingthe time to join meand sharing your experienceswith the audience To all of you out there  it sa real pleasure to be with you Thank you for the time you took Hopefully you get agood glimpse of whatdrives our roadmaps  whatwe ve delivered recently I hope to come back and sharemuch more with you over time and also bring moreconversations with customers Thank you BOB KILLEN  Hello  everyone And welcome to What sNext in Kubernetes I m Bob Killen  a programmanager within Google s OpenSource Programs Office also a longtime contributorto the Open SourceKubernetes project And I m here totalk a little bitabout some of the overarchingfuture paths for Kubernetes But before I can diginto what s next I need to talk a littleabout where we ve been what it s taken forKubernetes to becomethe foundational platformthat powers so much today From a high level  you can sortof break down Kubernetes phasesinto three broad layers thathighlight a focus or themein the project s lifecycle The first and thelongest was four years from June 2014 to March 2018 This is a period ofrapid growth  whereall the variousresource types arebeing defined and solidified These are the basiccomponents thatare used by everything else Think of the core workloads such as deployments stateful sets  andall the others These core workloads graduatedto GA in the 1 9 releaseand was shortlythereafter recognizedby the Cloud Native ComputingFoundation s TechnicalOversight Committee  approvingthe project for graduation Kubernetes was the firstproject to achieve the status It was recognizedfor being stable used by a number of companies having a diverse contributorbase and solidproject governance This is the foundational stage And every future thingis built on top of it The second phase  from March2018 to September 2019 is marked by the graduation ofcustom resource definitions or CRDs  a unified pattern toextend Kubernetes It created an entire ecosystemallowing people  projects and organizationsto create and managetheir own applications inany other sort of Kubernetesresource Instead of managingthese deployments you can now directlymanage things like a databaseor a game server in a Kubernetes native way It was huge for thecloud native landscape And if you look at the CNCF with its 100 or so projects you ll find that the majorityof them are using CRDs This last phase  fromlate 2019 to August 2020 isn t noted by a new feature but a shift in the mentalityof the project We built our coreworkload types We ve made the projecteasy to extend And now with more usersthan ever using the project we shifted to supportmobility and stability Policies and processeswere put in placeto help ensure everyfeature being developedwas observable  scalable could be rolled back and a slew of other importantthings for general operations And in doing so  wewere more comfortableexpanding our support window with new releases beingsupported for a full year We built our core  we made iteasy to extend and integratewith  and thencommitted ourselvesto a stronger support policy So that takes us to nowand where we re going And I ll keep the theme forthis one sort of a secretto the end But I want to giveyou an idea of someof the features and areasof where we re focusing And for that  I pickedtwo big things to follow The first is multi cluster So why multi cluster Kubernetes has done a lot tohelp people rethink and reshapetheir architectureand build moreresilient and scalable systems But for many usecases  you can t justbuild a larger cluster Say you want global availabilityand to minimize the latencyto your customers That s somethingyou can t easilydo with a single cluster Or you might want to scaleand burst your workloads especially in something like ahybrid environment where you veoutgrown your on prem hardwareand want to burst to the cloud You may also be working withprivileged or sensitive data and want to maximize yoursecurity by using clustersas a security boundary Or lastly  you may just wantto make your application moreresilient by running multipleclusters in HA and to segmentyour failure domains The list goes on And it s often oneof the first placesorganizations go to aftergetting up and goingwith a single cluster Now with that  I m going togo out a little on a limb But in a multi cluster world clusters are the new pod As management of Kubernetesclusters becomes easier it allows you to focus onthe important parts  namelyyour application Clusters are becoming a tightlycoupled grouping of these appcomponents that you mightwant to scale and secureindependently So just like a deploymentmanages the pods and help scalethem up and down andmanages their life cycle you should be able to dothat with clusters too So why has this been so hard Well  to be frank multi cluster configurationsin their current form areincredibly complex with a lotof moving parts If you have adatabase in cluster Athat should be accessible tosome component in cluster B it requires a lot ofadditional plumbingof services between clusters  often  additional loadbalancers or a mix of themwith a service mesh It s not fun andby no means easy Also  to be frank Kubernetes was honestlynot really designedfor multi clusterin mind initially But this has beenlong recognizedby the upstream community And there have been severalinitiatives targetingand improving this use case The first is the introductionof multi cluster services a new CRD that standardizesthe way in which youcan share Kubernetesservices across clusters No more additional provisioningof load balancers  no morehodgepodge custom DNS rulesthat require manual updating You define a cluster set or agroup of two or more clustersand decide whichservices in each clusteryou want to expose And a new controller willjust take care of the rest For your applications all you have to dois them to a new cluster setservice DNS entry insteadof the regular cluster localone that we allare sort of used to And with that  your trafficwill be routed appropriately Now  this is avery simple model And it was specificallydesigned to be that wayto address the complexityof cross cluster networkingand service discoveryto make it easy to useas a drop in replacementfor current services Now  there s a lot thatgoes on under the hood But you as a user do not needto manage those aspects of it So now that we have thesemulti cluster services thatcan help you manage yourcross cluster networking how about Ingress How do you managegetting trafficinto your clusterat a global level Well  there s somethingfor that too  the Gateway API Think of it asIngress version 2 0 It was designed withall the lessons learnedfrom the originalKubernetes Ingressimplementation withmany of the pain points such as excessive use ofannotations being addressed And for the purposesof multi cluster it can use the exports fromthe multi cluster servicesAPI as a validback end  allowingyou to manage yourIngresses at a global level Now  for GKE these two resourceshave come together to offera very flexible and powerfulsystem Within GKE  there are twofundamental different typesof clusters  standard  where youretain full control and autopilot  where muchof the management aspectsare abstracted awayand managed by Google Autopilot is a greatoption for many workloads It comes with advantages likebetter security and per podbilling But there are some workloadsthat don t necessarilylend itself to it well  namelylarge stateful applicationsor things where you may need torun in a pod in privilege mode But that s wheremulti cluster comes in What if we can interconnectand deploy appswhere they fitbest without havingto worry about the plumbing You d be able to take advantageof the greater security and perpod billing for mostresources in autopilotand retain the greatercontrol over just the partsof your stack that need itin a standard cluster while honestly  managing them ina dramatically easier way And that s part of thefuture we re working towards So now that we vecovered multi cluster let s take a look atthe next focus area And that s in the area ofAI ML and batch computing Now  this is anarea that s actuallynear and dear to my heart Before I started at Google I was heavily involvedin research computingand exploringKubernetes for highperformance computing needs And while therehave been advancesin what I ll say broadlyas research computingon top of Kubernetes there has generallybeen a much slowerrate of adoption of itversus  I ll say  moretraditional applications And that s because it s hardfor a number of reasons First and foremost  theKubernetes scheduleris built for bin packing or trying to cram as muchon a single nodeas possible  whichtends to go against mostreally resource intensive jobs It also doesn t support theconcept of co scheduling  whereyou block schedulinguntil there senough resources availableto run your entire job The other thing is most ofthese research applications alsorequire a lot of tuning  the application  the underlyingsystem or host OS  and alsothings like the API server If you re spinning up hundredsof thousands of tasks you need everything to beworking at peak performanceand to be able to handlethat sort of scale Now  there are a fewthings coming in the pipesto help improve this But I want to focuson one thing thathas been an open requestfor a very long timeand will help reduce thecomplexity of work queues Works queues are afairly simple in concept They are a list of tasks ordata intended to be processedby one or more workers They are by far themost common work patternseen in AI ML and batch You re providing a dataset and iterating over itin parallel fortraining or processing And Kubernetes has not reallysupported this workflownatively It s always required someother external system such as Pub Sub to keep trackof what s being processed This also meansthat you must havebake in more sort ofmiddleware logic into your job just to be able to process it It works Organizations have beenusing this pattern since jobswere a thing in Kubernetes But do you reallywant to do that Or would you ratherrely on somethingthat could be done natively I d pick the latter So in Kubernetes 1 21 a new alpha featurewas added to jobs  a completion mode  where ifyou specify it as indexed each pod is given anumbered identifierwith the index starting at 0 This index will beused in the generationof the pods hostnames  givingit something predictable akinto stateful sets That index is also passedinto the pod as an environmentvariable And what this does is itgives you the ability  a built in native way tostatically assign datato the processes of each pod It will require a little bitof planning ahead of time But you won t have torun an external system You won t have to bake somethinginto your code for the job justto pull items from the queue It reduces the overallcomplexity of work queuessignificantly And this seemslike a small thing But it solves a problemfor a lot of people In addition to this  thereare also several other thingsin the works thatwill enable or AI MLand batch types of workloads  things like thescheduler framework where instead of trying tomake the committee scheduler beeverything to everyone many more hooks and methodsof integrating withit have been created It s now significantly easierto extend and customizehow scheduling is doneby assigning workloadsto differentscheduling profiles Jobs can also now be suspended And what this means is thatif you say a higher priorityjob is to be scheduled the lower priority jobcan be suspended untilthe higher priorityjob is completed It doesn t haveto be rescheduled Work can pick back up when thehigher priority job is done  something we ve seen ina lot of classic batchsystems for years Lastly  there s been somechanges to the Kubernetes APIserver so it can be tunedto support the exceedinglyhigh throughput of requests thatoften come in with batch joboriented systems And all thesethings combined willmake running AI ML and batchworkloads significantly easieron top of Kubernetes And at this point withthat sort of short previewof the improvements tothings like multi clusterand AI ML andbatch  you may havestarted to pick up on a theme What is Kubernetes Kubernetes is foundational We spent four yearsbuilding a strong base All the core workloadsand design patternsthat you depend ontoday started here It is extensible A year and a half were spentmaking it easy for peopleto adopt  build  andextend on that foundation creating an ecosystem that isjust absolutely massive today Kubernetes is mature We put a lot ofeffort into makingsure you can depend on everyfeature and every release And we demonstrated this byextending our support window So what is Kubernetes now Is it easy Well  we re not there yet We re making iteasier for you to use We re reducing muchof the complexity thathas forced peopleinto workaroundsand building our large more complex systems A lot of these featuresand improvementsmay seem minor at first look But all of themare there to helpimprove the variousworkflows that peoplehave built around andon top of Kubernetes And as we make Kuberneteseasier for people to use it will also enable awhole new set of use casesthat we haven t imagined yet So Kubernetes isn t easy yet But we ll get there And with that  if you areinterested in learning moreabout some of the stuffI talked about today here are someadditional resourcesthat you might find useful In particular  I want tohighlight the Learn Kuberneteswith Google Video series You ll find videosgoing over howyou can get going withmulti cluster index jobsand quite a bit more Thank you And I look forward to buildingthe future of Kuberneteswith you CHELSIE CZOP  Welcome Thanks for joining usto review what s newand what s next withinfrastructure for AI and ML I m Chelsie Czop  ProductManager for Compute Enginewithin Google Cloud And I m joined today byOmkar Pathak  Product Managerfor Cloud TPUs within Google First  we ll review ouraccelerator mission  AI and MLwithin Google  how we reable to provide youwith a differentiatedcompute platform and the ways to take advantageof our accelerator hardware Then we ll focus on ouraccelerator hardware platform Cloud GPUs  and Cloud TPUs We ll cover what s new andwhat s next in the portfolioand sprinkle in waysthat our customersare creating innovative productsand services within GoogleCloud At Google Cloud  we believe thatcutting edge machine learninginfrastructure should excelat performance  scalability and usability to allowyou to be able to createinnovative and differentiatedservices for your users all at a low totalcost of ownership Our leadership in AI throughboth Google research  DeepMind and also in the practicalapplications of AIwithin our Googleproducts  allowus to be able to provide youwith an innovative platformfor your AI and ML use cases We bring our experienceof deploying AI and MLmodels in productionacross all of Googleand provide you with thesame infrastructure and toolsthat we use This means we have acceleratorsfor any of your use cases from high performancetraining to serving modelscost effectively You re able to achieve theoptimal price per performancefor your use case through ourselection of Cloud GPUs  CloudTPUs  as well ascompute optimized VMson Compute Engine This gives you theability to choosethe latest andgreatest infrastructureto fit your use case needs Last  and arguablymost important for you is that it s easy toget started and to scaleusing a wide rangeof tools and servicesoffered through ComputeEngine and Vertex AI platform allowing you to createbusiness value faster It really helps to partnerwith companies that have fullydeployed AI and ML in theirown production environmentsto help you overcomeroadblocks faster We have a long historyof hardware development And we re always dedicatedto being an open cloud allowing you adiversity of options from frameworks toapplication portability We re at a really exciting timein the world for AI and ML Every industry willsoon be transformed The organizations thatembrace the AI and MLin this transformation willhave a competitive advantage To unlock the fullvalue of AI and ML we must first focus onyour business challenges We re able to do thatin two progressive ways First  you can focus oncommon business challenges Shown here are the commonbusiness challengesby industry  along with thesolutions that we ve createdto help you easily get started Then  moving on  you re ableto solve challenges thatare unique to your business The following solutions services  APIs  and modelsall help with AI needsat every single levelof expertise and readiness You Really don t need to be anML expert to get started We have easy to useAPIs or AutoML modelsthat allow you tocreate a customizedsolution to your uniquebusiness challenges You can plug and playthe APIs and modelsto create exactly what you need Underneath this entire portfolioare our hardware accelerators Cloud TPUs  Cloud GPUs and Compute Engine There s three main ways totake advantage of acceleratorson Google Cloud First we have Vertex AI offering end to end AIand ML tooling andscheduling via GKEusing containers  enablingGPUs and TPUs as a service removing the burden from you tomanage the hardware or the VMsfor your AI workload Simply give yourcontainers accessto GPUs via the KubernetesAPI  and then you reable to take advantageof autoscaling see the GPU utilization and scale these workloads upand down based on the demand Last  also viaCompute Engine  weoffer custom VM shapes  tooling and workflow support for youto be able to scale from asingle GPU under your deskto supercomputer computingscale available on demandwith preemptible instances as well as committeduse discounts basedon reservations to get you the best price Our virtual machinesare availablein many configurations including predefined sizes thatalign to Numa topology or as we ve mentioned prior the ability to createa custom machine typesize for your specific needs Then you aren t havingto manage a single VM You re also able to use managedinstance groups in a wayto control an entire fleetof your infrastructure You re able todefine a set of VMsand scale up anddown as you needor as your application requires Now let s dive into what snew with Cloud GPUs Within our CloudGPU portfolio  wehave an acceleratorfor your use casesfrom ML training and inference GPU enabled HPC  CUDA Compute and GPU acceleratedvisualization Our high end offerings includehigh performance GPUs  A100sand B100s  that are greatfor training  inference  HPC and CUDA Compute On the other endof the spectrum we have T4s  which arein more global locations providing you low latencyat a very low cost A very common use case thatwe see with our customersis training and doinginference on A100s or B100sand then serving in thatglobal capacity on T4s Let s take a closer lookat our A100 offering They re available in ouraccelerator optimized VMfamily  A2s  on Compute Engine The A100s have 40gigabits of memory And within the VM you can provision upto 96 Cascade Lake vCPUsand 1 3 terabytes of memory They re in fixed VM shapes toalign with the new transparentNuma topology And they can scale up to 16GPUs for scale up and scale outworkloads  such as naturallanguage processingand computer vision But don t worry ifyou don t need 16 You can go all theway down to two GPUsto be able toeasily get started NVLINK fabric is theall to all connectivitywith up to 9 6 terabytesper second peak bandwidth You also have the accessto advanced networkingas well as theoption for local SSD With all these features you re able to achieve upto 20 petaops of int8 allwithin a single VM  which iswhat I call mega performance As I mentioned earlier T4s on Google Cloudare able to provide a lowlatency at a low cost They re currently in 18 regions with more along the way What s new is that we dohave flexible T4s in previewcurrently You can provision exactlywhat you need for T4sfrom vCPU and VM memory We re also launching automaticCUD renewals  Committed UseDiscounts  for GPUs This helps reducecost and accidentalloss of your reservations Now  here s what s really goingto make a difference for youfor being able to easily findcapacity in your selectedregion for batch workloads We have distribution shapeswithin managed instance groupsset to Any The way you re able totake advantage of thisis in three verysimple  easy steps First  create amanaged instance group Second  set thedistribution shape to Any And three  specify yourtarget audience and region Optionally  you can alsorun preemptible GPUs VMsto save money You can take noteof any constraintsthat you have  suchas supported hardwarein their zones or resourcequotas  to get started We also have the optionfor deep learningand HPC VMs thatallows you to easilyget started on ourinfrastructure within minutes A simple one clickinstall  and your machineis provisioned and alreadyperformance optimizedon Google Cloud It also hascompatibility tested So you have no need to worryabout the software stack Simply select the CPU  GPU or TPU to add on to the VM We also provide thechoice and flexibilitybetween different ML models or you have the abilityto install your own Schrodinger s team was lookingfor a cloud provider thatwas just as committedto advancing lifesciences and drugdiscovery as they were That s why theychose Google Cloud They also choseGoogle because we reknown for our strength ofour network and our securityposture The partnershipdrastically changed the waythey were doing drug discovery And as Dr  Farid said this could potentiallytransform all of the way thatpharma is doing drug discovery As with any largeresource intensive models cost does start to pile up The price per performanceworked really well for themto create and discoverlife saving drugsto help combat COVID If they re able to do thiscost efficiently usingcustom types ofVMs with GPUs  thenthink about what you d beable to do with your workload With the change in consumerbehavior due to COVID Pagani was able tocreate a virtual showroomto allow you tocustomize your own carfrom the comfort of your couch This virtual showroom is servedon NVIDIA T4s with the NVIDIARTX virtual workstation image  MUSIC PLAYING Next  let s pass it up toOmkar to cover Cloud TPUs what s new  and what s next Take it away  Omkar OMKAR PATHAK  Thanks  Chelsie We have seentremendous innovationin the field of machinelearning in the past few years And we expect thisrate of innovationto grow in the future However  this fast pacedinnovation requestsever increasing amountsof computing power As machine learningmodels get more accurate they tend to getlarger  more complex and require larger data sets This is exactly whyGoogle built TPUs to accelerate the rate ofmachine learning innovationby delivering high performanceand by training and servingat a very low cost TPUs are used in manyproducts at Google And it s likely thatyou have interactedwith these when you have usedsome of Google s products These accelerators areavailable to you in Google Cloudright now Our goal with TPUs is toadvance machine learningat Google and across the world So TPUs are built for thisend to end user journeyof an app developeror a researcher starting from fast pacediterative developmentin the beginning all the way tolarge scale production trainingand serving across a range ofmachine learning use cases  vision  language recommendations  and more It s also easy to getstarted with CloudTPUs with TensorFlow PyTorch  or JAXin the Google Cloudproject or equivalent You can also bringyour own modelsor use one of our referencemodels to get started What s even moreinteresting is that once youget started withCloud TPUs  it seasy to scale your machinelearning models on Cloud TPUpods  which means thatyou can use petaflopsof computing power withminimal code changes This image here showsa Cloud TPU v3 pod which has 2 048cores that are builtfrom the ground upfor machine learning All of these coresare connected togetherwith a custom built  super fastinterconnect  which is reallywhat delivers thathigh performance whenyou scale your machine learningmodels on a Cloud TPU pod Combined with thelow cost of rentingCloud TPUs on Google Cloud asopposed to owning your own datacenters  thishigh performance machinecan unlock completely newopportunities for innovationwithout any upfront capitalinvestment on your part To understand the powerof easy scalability just look at our recentresults in MLPerf which is the industry standardbenchmark for ML performance Cloud TPUs demonstratedleading performance numbersacross a range of machinelearning use cases And as you can seein the chart here Cloud TPUs can train some ofthe common machine learningreference models injust a few seconds  for example  TLRM in 38seconds or BERT in 17 seconds We believe that thisis incredibly valuable especially since large scaleML training has unlocked someof the recentbreakthroughs in AI  for example  lambdain natural languageprocessing in multimodal models Cloud TPUs are particularlywell suited for these kindsof compute intensive large scale machine learningworkloads Over the last few years  wehave made relentless progresson TPU hardware and software And this year  we are excitedto announce our latestgeneration  Cloud TPU v4 On a chip to chipbasis  Cloud TPU v4is more than twice as fastas its previous generation And at a pod scale Cloud TPU v4 podstake Google Cloud s machinelearning infrastructureto a whole new level with more than one exaflopof computing powerequivalent to more than 10million laptops combined We have already deployedmany of these podsin Google s datacenters and will soonhave more  many ofwhich will be operatingat or near 90 carbon free energy Cloud TPU v4 pods will beavailable to our Google Cloudcustomers inpreview in Q4  2021 Now  it s not justthe sheer computingpower that pushes machinelearning innovation forward We believe that makingthe system easy to useis just as important as well And so we recently launcheda completely new softwarearchitecture that helps youinteract much more easilyand much more seamlesslywith Cloud TPUs With Cloud TPU VMs  you cannow drag full Cloud TPU partsdirectly from your desksfor the first time It s like having a machinelearning supercomputerunder your desk With Cloud TPU VMs  we aim tomake interactive supercomputingattainable to our users And it s been incrediblyencouraging to seepositive feedback fromour early customers For example  Hugging Face which is a leading open sourceprovider of NLP technologies recently integratedJAX  along withTensorFlow and PyTorch in their popularTransformers library And they were able topre train a base sized BERTmodel in less than a day withjust eight cores of v3  whichis the smallestreservable unit of a pod Similarly  ourusers at Gridspace which provides contactcenter automationtools  such as low latencyspeech analytics were able to train bothspeech and languagemodels with an immediate2x performance improvement They were also able to scalethese models on Cloud TPU partswith no code changes These usability andperformance benefitsmake it much easier thanever before to get startedwith Cloud TPUs and scaleyour machine learning modelson Cloud TPU pods Along with this powerfuluser experience Cloud TPUs also supportall the major machinelearning frameworks  includingTensorFlow  PyTorch  and JAX We encourage you tocheck out these linksand try out our referencemodels and tutorials In summary  at GoogleCloud  we believethat cutting edge machinelearning infrastructureshould excel at performance usability  and scalability So we ll continue to investin making our systemsmore easy to use enterprise class and open and pervasive Hopefully  with all of theseupdates on Cloud GPUs and CloudTPUs  our cutting edge machinelearning infrastructureis much more accessible and muchmore useful to you than everbefore Thank you 